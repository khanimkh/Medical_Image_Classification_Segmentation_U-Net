{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning Framework (Implement U-net)**\n",
        "\n",
        "McMedHacks"
      ],
      "metadata": {
        "id": "f9vw1LGgiuWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding U-Net Architecture**  [Original paper](https://arxiv.org/pdf/1505.04597.pdf)"
      ],
      "metadata": {
        "id": "rF_77V5vjBeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "jPB8JbQDi_g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   There are an encoder and decoder parts.\n",
        "*   Max poolign 2*2 reduce size of image by half"
      ],
      "metadata": {
        "id": "FURJJ_gDjf1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write one block"
      ],
      "metadata": {
        "id": "Y7U6m13Bj12B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "jEBhb5OfeJMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MedPy\n",
        "import numpy as np\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "def dsc(y_pred, y_true, lcc=True):\n",
        "    if lcc and np.any(y_pred):\n",
        "        y_pred = np.round(y_pred).astype(int)\n",
        "        y_true = np.round(y_true).astype(int)\n",
        "        y_pred = largest_connected_component(y_pred)\n",
        "    return np.sum(y_pred[y_true == 1]) * 2.0 / (np.sum(y_pred) + np.sum(y_true))\n",
        "\n",
        "\n",
        "def crop_sample(x):\n",
        "    volume, mask = x\n",
        "    volume[volume < np.max(volume) * 0.1] = 0\n",
        "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
        "    z_nonzero = np.nonzero(z_projection)\n",
        "    z_min = np.min(z_nonzero)\n",
        "    z_max = np.max(z_nonzero) + 1\n",
        "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
        "    y_nonzero = np.nonzero(y_projection)\n",
        "    y_min = np.min(y_nonzero)\n",
        "    y_max = np.max(y_nonzero) + 1\n",
        "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
        "    x_nonzero = np.nonzero(x_projection)\n",
        "    x_min = np.min(x_nonzero)\n",
        "    x_max = np.max(x_nonzero) + 1\n",
        "    return (\n",
        "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "    )\n",
        "\n",
        "\n",
        "def pad_sample(x):\n",
        "    volume, mask = x\n",
        "    a = volume.shape[1]\n",
        "    b = volume.shape[2]\n",
        "    if a == b:\n",
        "        return volume, mask\n",
        "    diff = (max(a, b) - min(a, b)) / 2.0\n",
        "    if a > b:\n",
        "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
        "    else:\n",
        "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
        "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "    padding = padding + ((0, 0),)\n",
        "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
        "    return volume, mask\n",
        "\n",
        "\n",
        "def resize_sample(x, size=256):\n",
        "    volume, mask = x\n",
        "    v_shape = volume.shape\n",
        "    out_shape = (v_shape[0], size, size)\n",
        "    mask = resize(\n",
        "        mask,\n",
        "        output_shape=out_shape,\n",
        "        order=0,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    )\n",
        "    out_shape = out_shape + (v_shape[3],)\n",
        "    volume = resize(\n",
        "        volume,\n",
        "        output_shape=out_shape,\n",
        "        order=2,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    )\n",
        "    return volume, mask\n",
        "\n",
        "\n",
        "def normalize_volume(volume):\n",
        "    p10 = np.percentile(volume, 10)\n",
        "    p99 = np.percentile(volume, 99)\n",
        "    volume = rescale_intensity(volume, in_range=(p10, p99))\n",
        "    m = np.mean(volume, axis=(0, 1, 2))\n",
        "    s = np.std(volume, axis=(0, 1, 2))\n",
        "    volume = (volume - m) / s\n",
        "    return volume\n",
        "\n",
        "\n",
        "def log_images(x, y_true, y_pred, channel=1):\n",
        "    images = []\n",
        "    x_np = x[:, channel].cpu().numpy()\n",
        "    y_true_np = y_true[:, 0].cpu().numpy()\n",
        "    y_pred_np = y_pred[:, 0].cpu().numpy()\n",
        "    for i in range(x_np.shape[0]):\n",
        "        image = gray2rgb(np.squeeze(x_np[i]))\n",
        "        image = outline(image, y_pred_np[i], color=[255, 0, 0])\n",
        "        image = outline(image, y_true_np[i], color=[0, 255, 0])\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "\n",
        "def gray2rgb(image):\n",
        "    w, h = image.shape\n",
        "    image += np.abs(np.min(image))\n",
        "    image_max = np.abs(np.max(image))\n",
        "    if image_max > 0:\n",
        "        image /= image_max\n",
        "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
        "    ret[:, :, 2] = ret[:, :, 1] = ret[:, :, 0] = image * 255\n",
        "    return ret\n",
        "\n",
        "\n",
        "def outline(image, mask, color):\n",
        "    mask = np.round(mask)\n",
        "    yy, xx = np.nonzero(mask)\n",
        "    for y, x in zip(yy, xx):\n",
        "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
        "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
        "    return image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Twc2f2rZb2o",
        "outputId": "8e1dc742-8d71-4776-cb6f-171fcc57f003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting MedPy\n",
            "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 33.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from MedPy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from MedPy) (1.21.6)\n",
            "Collecting SimpleITK>=1.1.0\n",
            "  Downloading SimpleITK-2.1.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 1.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: MedPy\n",
            "  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MedPy: filename=MedPy-0.4.0-cp37-cp37m-linux_x86_64.whl size=754488 sha256=5c7a53672374f725520882d2b84d3a35f4cf52c4a435f8a480cc299d8d3984e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/57/3a/da1183f22a6afb42e11138daa6a759de233fd977a984333602\n",
            "Successfully built MedPy\n",
            "Installing collected packages: SimpleITK, MedPy\n",
            "Successfully installed MedPy-0.4.0 SimpleITK-2.1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.transform import rescale, rotate\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "\n",
        "def transforms(scale=None, angle=None, flip_prob=None):\n",
        "    transform_list = []\n",
        "\n",
        "    if scale is not None:\n",
        "        transform_list.append(Scale(scale))\n",
        "    if angle is not None:\n",
        "        transform_list.append(Rotate(angle))\n",
        "    if flip_prob is not None:\n",
        "        transform_list.append(HorizontalFlip(flip_prob))\n",
        "\n",
        "    return Compose(transform_list)\n",
        "\n",
        "\n",
        "class Scale(object):\n",
        "\n",
        "    def __init__(self, scale):\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        img_size = image.shape[0]\n",
        "\n",
        "        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n",
        "\n",
        "        image = rescale(\n",
        "            image,\n",
        "            (scale, scale),\n",
        "            multichannel=True,\n",
        "            preserve_range=True,\n",
        "            mode=\"constant\",\n",
        "            anti_aliasing=False,\n",
        "        )\n",
        "        mask = rescale(\n",
        "            mask,\n",
        "            (scale, scale),\n",
        "            order=0,\n",
        "            multichannel=True,\n",
        "            preserve_range=True,\n",
        "            mode=\"constant\",\n",
        "            anti_aliasing=False,\n",
        "        )\n",
        "\n",
        "        if scale < 1.0:\n",
        "            diff = (img_size - image.shape[0]) / 2.0\n",
        "            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n",
        "            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n",
        "            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "        else:\n",
        "            x_min = (image.shape[0] - img_size) // 2\n",
        "            x_max = x_min + img_size\n",
        "            image = image[x_min:x_max, x_min:x_max, ...]\n",
        "            mask = mask[x_min:x_max, x_min:x_max, ...]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        angle = np.random.uniform(low=-self.angle, high=self.angle)\n",
        "        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n",
        "        mask = rotate(\n",
        "            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n",
        "        )\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class HorizontalFlip(object):\n",
        "\n",
        "    def __init__(self, flip_prob):\n",
        "        self.flip_prob = flip_prob\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        if np.random.rand() > self.flip_prob:\n",
        "            return image, mask\n",
        "\n",
        "        image = np.fliplr(image).copy()\n",
        "        mask = np.fliplr(mask).copy()\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "mCcoJ2WzbE2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class BrainSegmentationDataset(Dataset):\n",
        "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
        "\n",
        "    in_channels = 3\n",
        "    out_channels = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir,\n",
        "        transform=None,\n",
        "        image_size=256,\n",
        "        subset=\"train\",\n",
        "        random_sampling=True,\n",
        "        validation_cases=10,\n",
        "        seed=42,\n",
        "    ):\n",
        "        assert subset in [\"all\", \"train\", \"validation\"]\n",
        "\n",
        "        # read images\n",
        "        volumes = {}\n",
        "        masks = {}\n",
        "        print(\"reading {} images...\".format(subset))\n",
        "        for (dirpath, dirnames, filenames) in os.walk(images_dir):\n",
        "            image_slices = []\n",
        "            mask_slices = []\n",
        "            for filename in sorted(\n",
        "                filter(lambda f: \".tif\" in f, filenames),\n",
        "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
        "            ):\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                if \"mask\" in filename:\n",
        "                    mask_slices.append(imread(filepath, as_gray=True))\n",
        "                else:\n",
        "                    image_slices.append(imread(filepath))\n",
        "            if len(image_slices) > 0:\n",
        "                patient_id = dirpath.split(\"/\")[-1]\n",
        "                volumes[patient_id] = np.array(image_slices[1:-1])\n",
        "                masks[patient_id] = np.array(mask_slices[1:-1])\n",
        "                print(f\"{len(image_slices)} images read!\")\n",
        "\n",
        "        self.patients = sorted(volumes)\n",
        "\n",
        "        # select cases to subset\n",
        "        if not subset == \"all\":\n",
        "            random.seed(seed)\n",
        "            validation_patients = random.sample(self.patients, k=validation_cases)\n",
        "            if subset == \"validation\":\n",
        "                self.patients = validation_patients\n",
        "            else:\n",
        "                self.patients = sorted(\n",
        "                    list(set(self.patients).difference(validation_patients))\n",
        "                )\n",
        "\n",
        "        print(\"preprocessing {} volumes...\".format(subset))\n",
        "        # create list of tuples (volume, mask)\n",
        "        self.volumes = [(volumes[k], masks[k]) for k in self.patients]\n",
        "\n",
        "        print(\"cropping {} volumes...\".format(subset))\n",
        "        # crop to smallest enclosing volume\n",
        "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"padding {} volumes...\".format(subset))\n",
        "        # pad to square\n",
        "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"resizing {} volumes...\".format(subset))\n",
        "        # resize\n",
        "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
        "\n",
        "        print(\"normalizing {} volumes...\".format(subset))\n",
        "        # normalize channel-wise\n",
        "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]\n",
        "\n",
        "        # probabilities for sampling slices based on masks\n",
        "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes]\n",
        "        self.slice_weights = [\n",
        "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights\n",
        "        ]\n",
        "\n",
        "        # add channel dimension to masks\n",
        "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes]\n",
        "\n",
        "        print(\"done creating {} dataset\".format(subset))\n",
        "\n",
        "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
        "        num_slices = [v.shape[0] for v, m in self.volumes]\n",
        "        self.patient_slice_index = list(\n",
        "            zip(\n",
        "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []),\n",
        "                sum([list(range(x)) for x in num_slices], []),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.random_sampling = random_sampling\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_slice_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.patient_slice_index[idx][0]\n",
        "        slice_n = self.patient_slice_index[idx][1]\n",
        "\n",
        "        if self.random_sampling:\n",
        "            patient = np.random.randint(len(self.volumes))\n",
        "            slice_n = np.random.choice(\n",
        "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
        "            )\n",
        "\n",
        "        v, m = self.volumes[patient]\n",
        "        image = v[slice_n]\n",
        "        mask = m[slice_n]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform((image, mask))\n",
        "\n",
        "        # fix dimensions (C, H, W)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        mask = mask.transpose(2, 0, 1)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
        "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
        "\n",
        "        # return tensors\n",
        "        return image_tensor, mask_tensor"
      ],
      "metadata": {
        "id": "uAAZblZhZU8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please copy the data and the notebook to your drive and change the path in the datasets function below accordingly."
      ],
      "metadata": {
        "id": "vr5wmoRRBQQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bl4h_ghTBPE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=2\n",
        "\n",
        "def datasets():\n",
        "    aug_scale = 0.05\n",
        "    aug_angle =15\n",
        "    image_size = 256\n",
        "    # images= \"/content/drive/MyDrive/Data_segmentation/kaggle_3m\"\n",
        "    ### Here write the name of the path you copied the data to\n",
        "    images = \"/content/drive/MyDrive/McMedHacks Workshop 2022 for participants/Week 6-1_Implement_U-Net/kaggle_3m\"\n",
        "    train = BrainSegmentationDataset(\n",
        "        images_dir=images,\n",
        "        subset=\"train\",\n",
        "        image_size=image_size,\n",
        "        transform=transforms(scale=aug_scale, angle=aug_angle, flip_prob=0.5),\n",
        "    )\n",
        "    valid = BrainSegmentationDataset(\n",
        "        images_dir=images,\n",
        "        subset=\"validation\",\n",
        "        image_size=image_size,\n",
        "        random_sampling=False,\n",
        "    )\n",
        "    return train, valid\n",
        "def data_loaders():\n",
        "    dataset_train, dataset_valid = datasets()\n",
        "\n",
        "    def worker_init(worker_id):\n",
        "        np.random.seed(42 + worker_id)\n",
        "\n",
        "    loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=1,\n",
        "        worker_init_fn=worker_init,\n",
        "    )\n",
        "    loader_valid = DataLoader(\n",
        "        dataset_valid,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=False,\n",
        "        num_workers=1,\n",
        "        worker_init_fn=worker_init,\n",
        "    )\n",
        "\n",
        "    return loader_train, loader_valid"
      ],
      "metadata": {
        "id": "W23qa7uDbTMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 1.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        assert y_pred.size() == y_true.size()\n",
        "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
        "        y_true = y_true[:, 0].contiguous().view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dsc = (2. * intersection + self.smooth) / (\n",
        "            y_pred.sum() + y_true.sum() + self.smooth\n",
        "        )\n",
        "        return 1. - dsc"
      ],
      "metadata": {
        "id": "jcGKYlPbbnNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
        "    dsc_list = []\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
        "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
        "        dsc_list.append(dsc(y_pred, y_true))\n",
        "        index += num_slices[p]\n",
        "    return dsc_list"
      ],
      "metadata": {
        "id": "toFU81tmclkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "epochs= 100\n",
        "vis_freq = 10\n",
        "vis_images = 2\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "loader_train, loader_valid = data_loaders()\n",
        "loaders = {\"train\": loader_train, \"valid\": loader_valid}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "htQhlMt2Z9gT",
        "outputId": "f596ff22-887d-4ea9-e088-f3673d1c04d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading train images...\n",
            "26 image read!\n",
            "24 image read!\n",
            "25 image read!\n",
            "22 image read!\n",
            "24 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "24 image read!\n",
            "20 image read!\n",
            "23 image read!\n",
            "36 image read!\n",
            "36 image read!\n",
            "40 image read!\n",
            "26 image read!\n",
            "38 image read!\n",
            "36 image read!\n",
            "22 image read!\n",
            "20 image read!\n",
            "28 image read!\n",
            "36 image read!\n",
            "71 image read!\n",
            "49 image read!\n",
            "52 image read!\n",
            "56 image read!\n",
            "58 image read!\n",
            "60 image read!\n",
            "53 image read!\n",
            "51 image read!\n",
            "57 image read!\n",
            "53 image read!\n",
            "38 image read!\n",
            "60 image read!\n",
            "58 image read!\n",
            "preprocessing train volumes...\n",
            "cropping train volumes...\n",
            "padding train volumes...\n",
            "resizing train volumes...\n",
            "normalizing train volumes...\n",
            "done creating train dataset\n",
            "reading validation images...\n",
            "26 image read!\n",
            "24 image read!\n",
            "25 image read!\n",
            "22 image read!\n",
            "24 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "20 image read!\n",
            "24 image read!\n",
            "20 image read!\n",
            "23 image read!\n",
            "36 image read!\n",
            "36 image read!\n",
            "40 image read!\n",
            "26 image read!\n",
            "38 image read!\n",
            "36 image read!\n",
            "22 image read!\n",
            "20 image read!\n",
            "28 image read!\n",
            "36 image read!\n",
            "71 image read!\n",
            "49 image read!\n",
            "52 image read!\n",
            "56 image read!\n",
            "58 image read!\n",
            "60 image read!\n",
            "53 image read!\n",
            "51 image read!\n",
            "57 image read!\n",
            "53 image read!\n",
            "38 image read!\n",
            "60 image read!\n",
            "58 image read!\n",
            "preprocessing validation volumes...\n",
            "cropping validation volumes...\n",
            "padding validation volumes...\n",
            "resizing validation volumes...\n",
            "normalizing validation volumes...\n",
            "done creating validation dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5f603fa1d3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloader_valid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'in_channels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "unet.to(device)\n",
        "\n",
        "dsc_loss = DiceLoss()\n",
        "best_validation_dsc = 0.0\n",
        "\n",
        "optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
        "\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in tqdm(range(epochs), total=epochs):\n",
        "    for phase in [\"train\", \"valid\"]:\n",
        "        if phase == \"train\":\n",
        "            unet.train()\n",
        "        else:\n",
        "            unet.eval()\n",
        "\n",
        "        validation_pred = []\n",
        "        validation_true = []\n",
        "\n",
        "        for i, data in enumerate(loaders[phase]):\n",
        "            if phase == \"train\":\n",
        "                step += 1\n",
        "\n",
        "            x, y_true = data\n",
        "            x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                y_pred = unet(x)\n",
        "\n",
        "                loss = dsc_loss(y_pred, y_true)\n",
        "\n",
        "                if phase == \"valid\":\n",
        "                    loss_valid.append(loss.item())\n",
        "                    y_pred_np = y_pred.detach().cpu().numpy()\n",
        "                    validation_pred.extend(\n",
        "                        [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
        "                    )\n",
        "                    y_true_np = y_true.detach().cpu().numpy()\n",
        "                    validation_true.extend(\n",
        "                        [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
        "                    )\n",
        "                    if (epoch % vis_freq == 0) or (epoch == epochs - 1):\n",
        "                        if i * batch_size < vis_images:\n",
        "                            tag = \"image/{}\".format(i)\n",
        "                            num_images = vis_images - i * batch_size\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    loss_train.append(loss.item())\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            if phase == \"train\" and (step + 1) % 10 == 0:\n",
        "                print(f\"step : {step}  ---  loss train : {loss_train}\")\n",
        "                loss_train = []\n",
        "\n",
        "        if phase == \"valid\":\n",
        "            print(f\"step : {step}  ---  loss train : {loss_valid}\")\n",
        "            mean_dsc = np.mean(\n",
        "                dsc_per_volume(\n",
        "                    validation_pred,\n",
        "                    validation_true,\n",
        "                    loader_valid.dataset.patient_slice_index,\n",
        "                )\n",
        "            )\n",
        "            print(f\"step : {step}  ---  mean_dsc : {mean_dsc}\")\n",
        "            if mean_dsc > best_validation_dsc:\n",
        "                best_validation_dsc = mean_dsc\n",
        "                torch.save(unet.state_dict(), \"unet.pt\")\n",
        "            loss_valid = []\n",
        "\n",
        "print(\"Best validation mean DSC: {:4f}\".format(best_validation_dsc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jawk49Fek0I4",
        "outputId": "550b453b-e29d-4031-eaa2-d02dba180e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 9  ---  loss train : [0.9485752582550049, 0.8822183609008789, 0.986813485622406, 0.9499821662902832, 0.9409418106079102, 0.8278796672821045, 0.8765856623649597, 0.8768494725227356, 0.8687744140625]\n",
            "step : 19  ---  loss train : [0.8040692806243896, 0.9276217818260193, 0.8360174298286438, 0.8611055612564087, 0.9169609546661377, 0.8626937866210938, 0.9304288625717163, 0.7517061829566956, 0.8392825126647949, 0.8657299280166626]\n",
            "step : 29  ---  loss train : [0.844277024269104, 0.7898597717285156, 0.7870556116104126, 0.9478707313537598, 0.8007240891456604, 0.8635917901992798, 0.9404151439666748, 0.8180797696113586, 0.9752345085144043, 0.8887588381767273]\n",
            "step : 39  ---  loss train : [0.9512389898300171, 0.7373946905136108, 0.7916602492332458, 0.8063149452209473, 0.7851880788803101, 0.748427152633667, 0.8323068618774414, 0.8993020057678223, 0.7513787746429443, 0.7673826217651367]\n",
            "step : 49  ---  loss train : [0.7874736785888672, 0.8388473391532898, 0.9490716457366943, 0.8257683515548706, 0.7866491079330444, 0.8925980925559998, 0.8901062607765198, 0.862768292427063, 0.9081695079803467, 0.9391151666641235]\n",
            "step : 59  ---  loss train : [0.9307332634925842, 0.9351263046264648, 0.836089015007019, 0.8521355390548706, 0.8004101514816284, 0.8653051853179932, 0.8957732319831848, 0.9087252616882324, 0.8849485516548157, 0.850996732711792]\n",
            "step : 69  ---  loss train : [0.8687208294868469, 0.9137090444564819, 0.650719404220581, 0.7467071413993835, 0.8400112390518188, 0.8347015380859375, 0.7464162111282349, 0.8928787112236023, 0.5705599784851074, 0.7339913845062256]\n",
            "step : 79  ---  loss train : [0.7664727568626404, 0.7365264296531677, 0.9325698614120483, 0.9036556482315063, 0.8193231225013733, 0.7996949553489685, 0.90986168384552, 0.6978113651275635, 0.8957213163375854, 0.7672451138496399]\n",
            "step : 89  ---  loss train : [0.6878495216369629, 0.6596329212188721, 0.8058620095252991, 0.7512152791023254, 0.6730680465698242, 0.6884924173355103, 0.8024209141731262, 0.7759948372840881, 0.703563928604126, 0.6659160852432251]\n",
            "step : 99  ---  loss train : [0.7839852571487427, 0.7220199704170227, 0.7356443405151367, 0.8629461526870728, 0.9120072722434998, 0.9016198515892029, 0.6309093236923218, 0.7542207837104797, 0.6327387094497681, 0.6497516632080078]\n",
            "step : 109  ---  loss train : [0.805005669593811, 0.7821470499038696, 0.8099774122238159, 0.7412906885147095, 0.5805032253265381, 0.5945760011672974, 0.6266367435455322, 0.7547969222068787, 0.914217472076416, 0.7316871285438538]\n",
            "step : 119  ---  loss train : [0.794009268283844, 0.5660872459411621, 0.6910557150840759, 0.6019588708877563, 0.869409441947937, 0.6577640771865845, 0.8581451177597046, 0.8012954592704773, 0.607970118522644, 0.8167814016342163]\n",
            "step : 129  ---  loss train : [0.738606333732605, 0.8707855939865112, 0.751995861530304, 0.575809121131897, 0.7451585531234741, 0.9076133966445923, 0.5768848061561584, 0.6905310153961182, 0.5407298803329468, 0.5346622467041016]\n",
            "step : 139  ---  loss train : [0.5542584657669067, 0.5317680239677429, 0.6932563781738281, 0.8798232674598694, 0.992297887802124, 0.4456455707550049, 0.47399675846099854, 0.568676233291626, 0.7150055170059204, 0.41715705394744873]\n",
            "step : 149  ---  loss train : [0.4958828091621399, 0.4796030521392822, 0.5451115965843201, 0.4598029851913452, 0.35966747999191284, 0.5367223024368286, 0.4138149619102478, 0.3095443844795227, 0.4038335680961609, 0.48803001642227173]\n",
            "step : 159  ---  loss train : [0.6979889869689941, 0.36669766902923584, 0.7615975737571716, 0.6572592258453369, 0.9999263286590576, 0.367836058139801, 0.2694709300994873, 0.418520450592041, 0.44803810119628906, 0.6725702881813049]\n",
            "step : 169  ---  loss train : [0.47983819246292114, 0.40354007482528687, 0.40538984537124634, 0.303339421749115, 0.45903563499450684, 0.35007619857788086, 0.5033252835273743, 0.2940259575843811, 0.23320835828781128, 0.2815350294113159]\n",
            "step : 179  ---  loss train : [0.4236149191856384, 0.7400436401367188, 0.2833811044692993, 0.48487454652786255, 0.4441055655479431, 0.35445356369018555, 0.7816850543022156, 0.6231006383895874, 0.4643474221229553, 0.341381311416626]\n",
            "step : 189  ---  loss train : [0.28060925006866455, 0.6459941864013672, 0.4894171357154846, 0.6367908120155334, 0.7062335014343262, 0.26864898204803467, 0.42306357622146606, 0.5179899334907532, 0.5035896897315979, 0.45767658948898315]\n",
            "step : 199  ---  loss train : [0.45909953117370605, 0.4503949284553528, 0.3732854127883911, 0.29431474208831787, 0.5473005771636963, 0.19582831859588623, 0.5202454328536987, 0.35250765085220337, 0.6921623945236206, 0.5602293014526367]\n",
            "step : 209  ---  loss train : [0.8432482481002808, 0.28963810205459595, 0.235484778881073, 0.2501373291015625, 0.5497748851776123, 0.21485096216201782, 0.47200560569763184, 0.322107195854187, 0.21433347463607788, 0.20623165369033813]\n",
            "step : 219  ---  loss train : [0.5552641153335571, 0.23854970932006836, 0.3836420774459839, 0.29467880725860596, 0.2681350111961365, 0.356139600276947, 0.19736361503601074, 0.37621045112609863, 0.9962919354438782, 0.47526127099990845]\n",
            "step : 229  ---  loss train : [0.4498298168182373, 0.16587305068969727, 0.5112934112548828, 0.3563494086265564, 0.3682937026023865, 0.4033408761024475, 0.6716650724411011, 0.38326966762542725, 0.2827147841453552, 0.3193681240081787]\n",
            "step : 239  ---  loss train : [0.24660247564315796, 0.8250409960746765, 0.39626073837280273, 0.39596468210220337, 0.26763057708740234, 0.4193950295448303, 0.30761510133743286, 0.27948325872421265, 0.2567824721336365, 0.338276743888855]\n",
            "step : 249  ---  loss train : [0.2652515769004822, 0.2683783173561096, 0.2728801965713501, 0.5314639806747437, 0.35866791009902954, 0.2652775049209595, 0.4417268633842468, 0.17712849378585815, 0.4075140357017517, 0.3028712868690491]\n",
            "step : 259  ---  loss train : [0.3313937783241272, 0.17618638277053833, 0.1568710207939148, 0.21479833126068115, 0.11712366342544556, 0.8398606777191162, 0.18665212392807007, 0.18572592735290527, 0.7570651769638062, 0.2583409547805786]\n",
            "step : 269  ---  loss train : [0.19243937730789185, 0.2240116000175476, 0.2532619833946228, 0.27579808235168457, 0.322205126285553, 0.12958991527557373, 0.09452313184738159, 0.5162912607192993, 0.20194917917251587, 0.7266064882278442]\n",
            "step : 279  ---  loss train : [0.8117243647575378, 0.2546764612197876, 0.6059609651565552, 0.18728435039520264, 0.5923546552658081, 0.37230348587036133, 0.33428776264190674, 0.2138800024986267, 0.25479793548583984, 0.16221898794174194]\n",
            "step : 289  ---  loss train : [0.23913997411727905, 0.37839406728744507, 0.12367099523544312, 0.21168136596679688, 0.23583418130874634, 0.8319205045700073, 0.6094361543655396, 0.27244728803634644, 0.27570074796676636, 0.42945462465286255]\n",
            "step : 299  ---  loss train : [0.20334601402282715, 0.22698062658309937, 0.22190439701080322, 0.10101163387298584, 0.12114429473876953, 0.42737507820129395, 0.21883952617645264, 0.22042518854141235, 0.4829757809638977, 0.253601610660553]\n",
            "step : 309  ---  loss train : [0.24349147081375122, 0.3023945093154907, 0.361580491065979, 0.1755596399307251, 0.2356630563735962, 0.1623743176460266, 0.23929542303085327, 0.12573421001434326, 0.2596966624259949, 0.30958789587020874]\n",
            "step : 319  ---  loss train : [0.611201286315918, 0.24395376443862915, 0.2736457586288452, 0.6513301730155945, 0.3936859369277954, 0.11601805686950684, 0.599469780921936, 0.16390836238861084, 0.3560146689414978, 0.2212185263633728]\n",
            "step : 329  ---  loss train : [0.87596195936203, 0.09513580799102783, 0.16954457759857178, 0.14488261938095093, 0.5156181454658508, 0.19586235284805298, 0.2853021025657654, 0.22301393747329712, 0.23748266696929932, 0.2539849877357483]\n",
            "step : 339  ---  loss train : [0.23487406969070435, 0.18145936727523804, 0.3543684482574463, 0.2483454942703247, 0.6342878937721252, 0.698378324508667, 0.2198083996772766, 0.08627182245254517, 0.08999335765838623, 0.1621125340461731]\n",
            "step : 349  ---  loss train : [0.20972305536270142, 0.697743833065033, 0.3847143054008484, 0.15295416116714478, 0.4601615071296692, 0.2317819595336914, 0.2266237735748291, 0.19550812244415283, 0.19903111457824707, 0.18084406852722168]\n",
            "step : 359  ---  loss train : [0.0930861234664917, 0.1343335509300232, 0.39923804998397827, 0.09205257892608643, 0.2069137692451477, 0.20154953002929688, 0.15637218952178955, 0.201585054397583, 0.48265743255615234, 0.15990155935287476]\n",
            "step : 369  ---  loss train : [0.17994588613510132, 0.25448811054229736, 0.18838614225387573, 0.21495479345321655, 0.15978169441223145, 0.11302530765533447, 0.18844342231750488, 0.22735822200775146, 0.13838374614715576, 0.5081804990768433]\n",
            "step : 379  ---  loss train : [0.2949826717376709, 0.20538246631622314, 0.17971402406692505, 0.1718774437904358, 0.23699545860290527, 0.19505149126052856, 0.297568678855896, 0.16888052225112915, 0.24754977226257324, 0.787631630897522]\n",
            "step : 389  ---  loss train : [0.17548245191574097, 0.553259551525116, 0.40052932500839233, 0.3493598699569702, 0.1976969838142395, 0.16858446598052979, 0.2718225121498108, 0.20154553651809692, 0.09221017360687256, 0.2647692561149597]\n",
            "step : 399  ---  loss train : [0.3280177116394043, 0.196658194065094, 0.12131094932556152, 0.3088260293006897, 0.07400590181350708, 0.10059988498687744, 0.151971697807312, 0.32355546951293945, 0.1704878807067871, 0.1631954312324524]\n",
            "step : 409  ---  loss train : [0.1432335376739502, 0.17438513040542603, 0.1324356198310852, 0.1756029725074768, 0.22239363193511963, 0.3143102526664734, 0.18688905239105225, 0.15842729806900024, 0.2286224365234375, 0.2599636912345886]\n",
            "step : 419  ---  loss train : [0.15302503108978271, 0.12882006168365479, 0.19329488277435303, 0.09119129180908203, 0.7797516584396362, 0.131358802318573, 0.16513139009475708, 0.08357465267181396, 0.28855568170547485, 0.21941429376602173]\n",
            "step : 429  ---  loss train : [0.06939089298248291, 0.19888460636138916, 0.2222934365272522, 0.29925012588500977, 0.08812582492828369, 0.300926148891449, 0.13541293144226074, 0.16760140657424927, 0.4258447289466858, 0.13789212703704834]\n",
            "step : 439  ---  loss train : [0.36987441778182983, 0.37548530101776123, 0.15994209051132202, 0.08968478441238403, 0.13356131315231323, 0.1447058916091919, 0.244867205619812, 0.14416944980621338, 0.17722159624099731, 0.11066460609436035]\n",
            "step : 449  ---  loss train : [0.211439847946167, 0.09281402826309204, 0.10643565654754639, 0.274938702583313, 0.18096810579299927, 0.5796427726745605, 0.5347086787223816, 0.12358951568603516, 0.27374327182769775, 0.16432225704193115]\n",
            "step : 459  ---  loss train : [0.30433452129364014, 0.5055184364318848, 0.2985990643501282, 0.2153925895690918, 0.12030917406082153, 0.25888192653656006, 0.7143402099609375, 0.1487782597541809, 0.14275139570236206, 0.14067018032073975]\n",
            "step : 469  ---  loss train : [0.18618696928024292, 0.14492285251617432, 0.19727814197540283, 0.1526891589164734, 0.3001909852027893, 0.22638684511184692, 0.09363245964050293, 0.16318202018737793, 0.0969894528388977, 0.12452483177185059]\n",
            "step : 469  ---  loss train : [0.9998975396156311, 0.9998296499252319, 0.9154226779937744, 0.4192820191383362, 0.33026641607284546, 0.7937047481536865, 0.9995279908180237, 0.9987024664878845, 0.9934141039848328, 0.9984145760536194, 0.9995715618133545, 0.9997845888137817, 0.9996715188026428, 0.3208605647087097, 0.1329159140586853, 0.13633185625076294, 0.16811615228652954, 0.9865472316741943, 0.9850460290908813, 0.9997822642326355, 0.9997628927230835, 0.9991753101348877, 0.9995689988136292, 0.9996803402900696, 0.6649503707885742, 0.46141761541366577, 0.4050365686416626, 0.3540799021720886, 0.30491161346435547, 0.3497759699821472, 0.9913208484649658, 0.9839144945144653, 0.9837743043899536, 0.9974038004875183, 0.9995737671852112, 0.9995874166488647, 0.9999127388000488, 0.9996089339256287, 0.9991742968559265, 0.9979546070098877, 0.39690524339675903, 0.14346426725387573, 0.13392764329910278, 0.15354663133621216, 0.24226051568984985, 0.31148386001586914, 0.999942421913147, 0.999971330165863, 0.9999707937240601, 0.9999690651893616, 0.999961793422699, 0.9999529123306274, 0.9999567270278931, 0.9444509744644165, 0.9545018672943115, 0.9837831854820251, 0.999947190284729, 0.9999617338180542, 0.9999595284461975, 0.9998674392700195, 0.9998091459274292, 0.6001617908477783, 0.22608262300491333, 0.30825525522232056, 0.44721120595932007, 0.9998859763145447, 0.9996802806854248, 0.9884311556816101, 0.9999119639396667, 0.9999606013298035, 0.9999545812606812, 0.9999437928199768, 0.9998941421508789, 0.9998149871826172, 0.9996999502182007, 0.9997054934501648, 0.9982001185417175, 0.9984479546546936, 0.9991274476051331, 0.9920744299888611, 0.9974073767662048, 0.9979418516159058, 0.9868778586387634, 0.7521401643753052, 0.29755550622940063, 0.6844910383224487, 0.5694059133529663, 0.4961012601852417, 0.49331825971603394, 0.4520711898803711, 0.33406591415405273, 0.19210773706436157, 0.20561349391937256, 0.2473433017730713, 0.9841678738594055, 0.9842171669006348, 0.9835467338562012, 0.9973538517951965, 0.9963094592094421, 0.9815904498100281, 0.998650312423706, 0.9800814390182495, 0.9994581341743469, 0.9990595579147339, 0.40521007776260376, 0.19746160507202148, 0.22388941049575806, 0.350158154964447, 0.6074848175048828, 0.9889523386955261, 0.9878257513046265, 0.9987101554870605, 0.9998189806938171, 0.9998167753219604, 0.9998818635940552, 0.999702513217926, 0.9140774607658386, 0.3610767126083374, 0.15571367740631104, 0.18408864736557007, 0.27995842695236206, 0.4951353073120117, 0.9872943758964539, 0.9999285340309143, 0.999932587146759, 0.9998629689216614, 0.9998577237129211, 0.9998568296432495, 0.999746561050415, 0.9987443685531616, 0.9984527826309204, 0.9992033839225769, 0.9992915987968445, 0.9992036819458008, 0.9867030382156372, 0.9871543049812317, 0.9867953062057495, 0.9898344278335571, 0.999126136302948, 0.9992098808288574, 0.9858261942863464, 0.9858887791633606, 0.9861350059509277, 0.9895939230918884, 0.9931783676147461, 0.9500154256820679, 0.6870015859603882, 0.9985073804855347, 0.9995026588439941, 0.9986476302146912, 0.9817019701004028, 0.9635692834854126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/100 [00:35<57:45, 35.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 469  ---  mean_dsc : 0.4314098778493177\n",
            "step : 479  ---  loss train : [0.2098659873008728, 0.1604621410369873, 0.7295231819152832, 0.16507524251937866, 0.405208945274353, 0.074582040309906, 0.11482274532318115, 0.21259182691574097, 0.44608384370803833, 0.20681673288345337]\n",
            "step : 489  ---  loss train : [0.16065454483032227, 0.1690846085548401, 0.26829302310943604, 0.27989304065704346, 0.18842220306396484, 0.10672390460968018, 0.17615514993667603, 0.23823708295822144, 0.1001744270324707, 0.07148343324661255]\n",
            "step : 499  ---  loss train : [0.1483413577079773, 0.38811928033828735, 0.47864198684692383, 0.11284422874450684, 0.15878069400787354, 0.28474634885787964, 0.07494205236434937, 0.5937433242797852, 0.0851169228553772, 0.33934420347213745]\n",
            "step : 509  ---  loss train : [0.136601984500885, 0.211586594581604, 0.1673126220703125, 0.3566826581954956, 0.22775566577911377, 0.3156664967536926, 0.15606552362442017, 0.595569372177124, 0.26477599143981934, 0.18749642372131348]\n",
            "step : 519  ---  loss train : [0.06075090169906616, 0.5395767688751221, 0.2603539228439331, 0.09186965227127075, 0.5672594308853149, 0.23287701606750488, 0.12174445390701294, 0.4808104634284973, 0.5388436317443848, 0.7063551545143127]\n",
            "step : 529  ---  loss train : [0.39602452516555786, 0.2755761742591858, 0.05537688732147217, 0.09249383211135864, 0.12365555763244629, 0.09485965967178345, 0.17098790407180786, 0.14620280265808105, 0.7563313245773315, 0.2821245789527893]\n",
            "step : 539  ---  loss train : [0.15955716371536255, 0.27688294649124146, 0.0885305404663086, 0.1932203769683838, 0.08346635103225708, 0.09612035751342773, 0.382617712020874, 0.2961975932121277, 0.2310703992843628, 0.10635226964950562]\n",
            "step : 549  ---  loss train : [0.15011680126190186, 0.5557979345321655, 0.32251298427581787, 0.16709637641906738, 0.16823965311050415, 0.49542587995529175, 0.07273167371749878, 0.43164658546447754, 0.24915671348571777, 0.21750962734222412]\n",
            "step : 559  ---  loss train : [0.15852153301239014, 0.07393443584442139, 0.07987385988235474, 0.48685067892074585, 0.5559911727905273, 0.3260801434516907, 0.23004382848739624, 0.1257597804069519, 0.09862518310546875, 0.2275557518005371]\n",
            "step : 569  ---  loss train : [0.2110811471939087, 0.08919346332550049, 0.2256302833557129, 0.690292239189148, 0.9991065263748169, 0.1235303282737732, 0.12972033023834229, 0.10387039184570312, 0.20242196321487427, 0.25146883726119995]\n",
            "step : 579  ---  loss train : [0.2844548225402832, 0.21665942668914795, 0.26770681142807007, 0.16812223196029663, 0.19741004705429077, 0.09732586145401001, 0.30918294191360474, 0.3567354083061218, 0.21998322010040283, 0.22428792715072632]\n",
            "step : 589  ---  loss train : [0.09715908765792847, 0.17306530475616455, 0.07527881860733032, 0.4375317692756653, 0.21845847368240356, 0.47830086946487427, 0.30863577127456665, 0.2122367024421692, 0.3390883207321167, 0.15547144412994385]\n",
            "step : 599  ---  loss train : [0.36109215021133423, 0.4125019907951355, 0.24498426914215088, 0.12062257528305054, 0.6746652126312256, 0.17984068393707275, 0.4733576774597168, 0.1429726481437683, 0.6073596477508545, 0.12721049785614014]\n",
            "step : 609  ---  loss train : [0.24883008003234863, 0.246604323387146, 0.6040029525756836, 0.9994862675666809, 0.09970945119857788, 0.08102786540985107, 0.1532875895500183, 0.16670697927474976, 0.162073016166687, 0.10747748613357544]\n",
            "step : 619  ---  loss train : [0.2599238157272339, 0.0869017243385315, 0.16720634698867798, 0.07746946811676025, 0.10074520111083984, 0.16380852460861206, 0.07151138782501221, 0.12879717350006104, 0.17435461282730103, 0.38350266218185425]\n",
            "step : 629  ---  loss train : [0.17076778411865234, 0.42603689432144165, 0.2935848832130432, 0.9997575879096985, 0.10328477621078491, 0.10423511266708374, 0.17412441968917847, 0.13485980033874512, 0.2837982177734375, 0.3768707513809204]\n",
            "step : 639  ---  loss train : [0.08483105897903442, 0.09798461198806763, 0.14530903100967407, 0.33066993951797485, 0.10995441675186157, 0.236463725566864, 0.0890892744064331, 0.14922845363616943, 0.21067744493484497, 0.11261439323425293]\n",
            "step : 649  ---  loss train : [0.13648688793182373, 0.11107927560806274, 0.1798950433731079, 0.17699360847473145, 0.2705584168434143, 0.43810218572616577, 0.12134671211242676, 0.17760056257247925, 0.290127694606781, 0.10044074058532715]\n",
            "step : 659  ---  loss train : [0.13909143209457397, 0.19366061687469482, 0.2642010450363159, 0.5099315643310547, 0.10703027248382568, 0.18811273574829102, 0.44174838066101074, 0.16776037216186523, 0.2358681559562683, 0.170881450176239]\n",
            "step : 669  ---  loss train : [0.24951034784317017, 0.21839451789855957, 0.2763964533805847, 0.22719216346740723, 0.19896316528320312, 0.2855914235115051, 0.18274670839309692, 0.44062280654907227, 0.3538805842399597, 0.6981074213981628]\n",
            "step : 679  ---  loss train : [0.18526065349578857, 0.0785796046257019, 0.10904908180236816, 0.28546881675720215, 0.0960381031036377, 0.28177309036254883, 0.1756567358970642, 0.08417993783950806, 0.10315710306167603, 0.4175999164581299]\n",
            "step : 689  ---  loss train : [0.12521082162857056, 0.2955934405326843, 0.10527855157852173, 0.0956883430480957, 0.25763124227523804, 0.1411304473876953, 0.27531325817108154, 0.9995942711830139, 0.23402470350265503, 0.3095698356628418]\n",
            "step : 699  ---  loss train : [0.10908502340316772, 0.2838154435157776, 0.15389132499694824, 0.24122720956802368, 0.3481602072715759, 0.45697611570358276, 0.34630876779556274, 0.16518360376358032, 0.42238688468933105, 0.1940617561340332]\n",
            "step : 709  ---  loss train : [0.5131008625030518, 0.37680482864379883, 0.20295929908752441, 0.2123207449913025, 0.21277755498886108, 0.2918049097061157, 0.15706384181976318, 0.14401018619537354, 0.3699260354042053, 0.29962730407714844]\n",
            "step : 719  ---  loss train : [0.17832255363464355, 0.2071954607963562, 0.21797031164169312, 0.1265518069267273, 0.23741942644119263, 0.4445691704750061, 0.1362791657447815, 0.17639541625976562, 0.22943568229675293, 0.2929770350456238]\n",
            "step : 729  ---  loss train : [0.10617434978485107, 0.08876544237136841, 0.18647533655166626, 0.08622294664382935, 0.8603901267051697, 0.16050171852111816, 0.10456180572509766, 0.6419017314910889, 0.19367045164108276, 0.10916489362716675]\n",
            "step : 739  ---  loss train : [0.11434745788574219, 0.16640043258666992, 0.18668746948242188, 0.27359652519226074, 0.07708483934402466, 0.06448417901992798, 0.30096644163131714, 0.12763607501983643, 0.5601677894592285, 0.7716309428215027]\n",
            "step : 749  ---  loss train : [0.22674620151519775, 0.4571847915649414, 0.15043210983276367, 0.38203543424606323, 0.40782833099365234, 0.21895766258239746, 0.1896536946296692, 0.17814314365386963, 0.16968125104904175, 0.16152900457382202]\n",
            "step : 759  ---  loss train : [0.4170936346054077, 0.09424835443496704, 0.20219814777374268, 0.22891300916671753, 0.7384262084960938, 0.42962831258773804, 0.14320439100265503, 0.28110969066619873, 0.3726479411125183, 0.10441839694976807]\n",
            "step : 769  ---  loss train : [0.1197887659072876, 0.1082046627998352, 0.05328500270843506, 0.08494424819946289, 0.37628060579299927, 0.15265387296676636, 0.17677640914916992, 0.5285338759422302, 0.1848408579826355, 0.19706612825393677]\n",
            "step : 779  ---  loss train : [0.25730711221694946, 0.2608882188796997, 0.15777480602264404, 0.18729835748672485, 0.09247887134552002, 0.20271170139312744, 0.08861786127090454, 0.14972662925720215, 0.27827709913253784, 0.5262100100517273]\n",
            "step : 789  ---  loss train : [0.2009180188179016, 0.3104701638221741, 0.6269432306289673, 0.36639368534088135, 0.0920027494430542, 0.5582467317581177, 0.12149369716644287, 0.2713749408721924, 0.236380934715271, 0.6826474666595459]\n",
            "step : 799  ---  loss train : [0.04329633712768555, 0.17095595598220825, 0.12470579147338867, 0.411964476108551, 0.19857770204544067, 0.13711458444595337, 0.3216847777366638, 0.2311251163482666, 0.15987414121627808, 0.1715283989906311]\n",
            "step : 809  ---  loss train : [0.15352624654769897, 0.25727975368499756, 0.23127257823944092, 0.5733129382133484, 0.6367666721343994, 0.17464929819107056, 0.05692315101623535, 0.06987237930297852, 0.10148543119430542, 0.15993636846542358]\n",
            "step : 819  ---  loss train : [0.6016795635223389, 0.36568236351013184, 0.12606090307235718, 0.4316127896308899, 0.19033247232437134, 0.19381952285766602, 0.1402050256729126, 0.15933191776275635, 0.15066289901733398, 0.06446784734725952]\n",
            "step : 829  ---  loss train : [0.11190551519393921, 0.2957152724266052, 0.0631028413772583, 0.18683427572250366, 0.19721561670303345, 0.13420164585113525, 0.18880832195281982, 0.4066530466079712, 0.1395583152770996, 0.13580405712127686]\n",
            "step : 839  ---  loss train : [0.23076194524765015, 0.16638809442520142, 0.1867450475692749, 0.09955155849456787, 0.06130087375640869, 0.1646309494972229, 0.08626765012741089, 0.1168854832649231, 0.4449911117553711, 0.2186492681503296]\n",
            "step : 849  ---  loss train : [0.18077152967453003, 0.17737746238708496, 0.15620368719100952, 0.25085848569869995, 0.23158305883407593, 0.4180877208709717, 0.1670103669166565, 0.23055237531661987, 0.611122727394104, 0.1624162197113037]\n",
            "step : 859  ---  loss train : [0.5195409059524536, 0.3521909713745117, 0.3616548776626587, 0.1496220827102661, 0.153242826461792, 0.2533645033836365, 0.16452836990356445, 0.06897640228271484, 0.17281728982925415, 0.33901268243789673]\n",
            "step : 869  ---  loss train : [0.10680866241455078, 0.09916001558303833, 0.24256712198257446, 0.06976330280303955, 0.08578741550445557, 0.13354063034057617, 0.3039877414703369, 0.15830975770950317, 0.14668786525726318, 0.13372278213500977]\n",
            "step : 879  ---  loss train : [0.11333054304122925, 0.13187313079833984, 0.15306514501571655, 0.22690832614898682, 0.27619028091430664, 0.1793348789215088, 0.13857781887054443, 0.20195132493972778, 0.2041441798210144, 0.1419227123260498]\n",
            "step : 889  ---  loss train : [0.11865764856338501, 0.19851404428482056, 0.06615602970123291, 0.6943939328193665, 0.1005173921585083, 0.1565263867378235, 0.2055743932723999, 0.37489116191864014, 0.2492486834526062, 0.06775021553039551]\n",
            "step : 899  ---  loss train : [0.18740087747573853, 0.230737566947937, 0.26874035596847534, 0.07604265213012695, 0.2712472677230835, 0.12564831972122192, 0.1412932276725769, 0.44015002250671387, 0.12570428848266602, 0.4474210739135742]\n",
            "step : 909  ---  loss train : [0.3954869508743286, 0.14215677976608276, 0.06897878646850586, 0.10512399673461914, 0.1293431520462036, 0.18048155307769775, 0.10966181755065918, 0.14455878734588623, 0.08837348222732544, 0.17174196243286133]\n",
            "step : 919  ---  loss train : [0.08061301708221436, 0.07888752222061157, 0.3164317011833191, 0.16530007123947144, 0.6298282742500305, 0.5654100179672241, 0.0850115418434143, 0.30731701850891113, 0.16491460800170898, 0.2285894751548767]\n",
            "step : 929  ---  loss train : [0.20185327529907227, 0.30721724033355713, 0.3197373151779175, 0.06804627180099487, 0.1328769326210022, 0.6064532399177551, 0.13792043924331665, 0.1270490288734436, 0.1358509659767151, 0.19011658430099487]\n",
            "step : 938  ---  loss train : [0.9998225569725037, 0.999771237373352, 0.9847450852394104, 0.3330579400062561, 0.3013876676559448, 0.6671837568283081, 0.9992778301239014, 0.9513612389564514, 0.9347696304321289, 0.99764484167099, 0.9650340676307678, 0.9994755387306213, 0.9993549585342407, 0.16776621341705322, 0.07446324825286865, 0.08491897583007812, 0.1394190788269043, 0.9408630132675171, 0.9335447549819946, 0.9990953803062439, 0.9994488954544067, 0.9977975487709045, 0.998971164226532, 0.999424934387207, 0.47301042079925537, 0.287145733833313, 0.2535102367401123, 0.22468852996826172, 0.1609100103378296, 0.21711385250091553, 0.9865349531173706, 0.9400696754455566, 0.9483948945999146, 0.9937448501586914, 0.9986815452575684, 0.9994667172431946, 0.999868631362915, 0.99895179271698, 0.9977549314498901, 0.9988387823104858, 0.2376437783241272, 0.11820155382156372, 0.1164061427116394, 0.14102411270141602, 0.23173683881759644, 0.37051624059677124, 0.9998950958251953, 0.9999412298202515, 0.9999379515647888, 0.9999282956123352, 0.9999207258224487, 0.9998851418495178, 0.9998887181282043, 0.9023893475532532, 0.9983503818511963, 0.999345064163208, 0.9998630881309509, 0.9999169111251831, 0.999934196472168, 0.9998381733894348, 0.9996589422225952, 0.47613525390625, 0.14797914028167725, 0.23687922954559326, 0.3854832649230957, 0.9998602867126465, 0.9996939301490784, 0.9607654213905334, 0.99984210729599, 0.9999269843101501, 0.9998945593833923, 0.9998670220375061, 0.9997639656066895, 0.9995637536048889, 0.9993903636932373, 0.9988354444503784, 0.9949080944061279, 0.9886966943740845, 0.9943391680717468, 0.9755600094795227, 0.9914678931236267, 0.9934005737304688, 0.9542507529258728, 0.9975214600563049, 0.3825191259384155, 0.760151207447052, 0.6679781675338745, 0.5433527827262878, 0.5464843511581421, 0.48769211769104004, 0.37949448823928833, 0.25415700674057007, 0.11432647705078125, 0.14886128902435303, 0.9543153047561646, 0.9648515582084656, 0.9720456600189209, 0.9796744585037231, 0.9832866191864014, 0.958977222442627, 0.9958508610725403, 0.9246898889541626, 0.9989959001541138, 0.9981231093406677, 0.3881800174713135, 0.15187829732894897, 0.15856999158859253, 0.3075985312461853, 0.5906816124916077, 0.9509201049804688, 0.9765617251396179, 0.997882604598999, 0.9996704459190369, 0.9996129870414734, 0.9997438192367554, 0.9990227222442627, 0.7855813503265381, 0.22610938549041748, 0.0842316746711731, 0.12072539329528809, 0.20443099737167358, 0.41587895154953003, 0.9806130528450012, 0.9977520704269409, 0.9996480345726013, 0.9994015097618103, 0.9996377825737, 0.9995421767234802, 0.9984327554702759, 0.9790746569633484, 0.9918292760848999, 0.99444580078125, 0.9986559152603149, 0.9972580671310425, 0.9460436105728149, 0.9512926340103149, 0.9439793229103088, 0.9437097311019897, 0.941381573677063, 0.9974672794342041, 0.9411830902099609, 0.9402333498001099, 0.9431085586547852, 0.9967690706253052, 0.9979115724563599, 0.9993366003036499, 0.998991847038269, 0.9961698651313782, 0.945715606212616, 0.9260228276252747, 0.9241690635681152, 0.8573312163352966]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [01:12<59:17, 36.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 938  ---  mean_dsc : 0.48075319869814626\n",
            "step : 939  ---  loss train : [0.11765921115875244, 0.22340816259384155, 0.2092563509941101, 0.3373778462409973, 0.19648075103759766, 0.08531689643859863, 0.1327853798866272, 0.08301186561584473, 0.1013537049293518, 0.1905841827392578]\n",
            "step : 949  ---  loss train : [0.17062002420425415, 0.6886535882949829, 0.23510092496871948, 0.4442558288574219, 0.07993459701538086, 0.10016852617263794, 0.14330673217773438, 0.43585968017578125, 0.15321427583694458, 0.16449850797653198]\n",
            "step : 959  ---  loss train : [0.08440285921096802, 0.2476251721382141, 0.2610575556755066, 0.15093928575515747, 0.06852912902832031, 0.14278936386108398, 0.2274254560470581, 0.10196691751480103, 0.0641283392906189, 0.14507299661636353]\n",
            "step : 969  ---  loss train : [0.3922090530395508, 0.20794856548309326, 0.09896445274353027, 0.14898091554641724, 0.2853044867515564, 0.0637514591217041, 0.5452953577041626, 0.07135295867919922, 0.17967694997787476, 0.11814826726913452]\n",
            "step : 979  ---  loss train : [0.17320191860198975, 0.070401132106781, 0.3059403896331787, 0.11592262983322144, 0.28761225938796997, 0.12340676784515381, 0.611007809638977, 0.2568669319152832, 0.19055217504501343, 0.0649336576461792]\n",
            "step : 989  ---  loss train : [0.49038904905319214, 0.37216389179229736, 0.09732991456985474, 0.48780715465545654, 0.1898820400238037, 0.10454684495925903, 0.45181089639663696, 0.3490018844604492, 0.6850041747093201, 0.33418697118759155]\n",
            "step : 999  ---  loss train : [0.2708825469017029, 0.04511362314224243, 0.07507109642028809, 0.10684973001480103, 0.09764081239700317, 0.19439876079559326, 0.15505605936050415, 0.6286770105361938, 0.2647913098335266, 0.12750840187072754]\n",
            "step : 1009  ---  loss train : [0.2515733242034912, 0.06283533573150635, 0.23215126991271973, 0.08053922653198242, 0.0645492672920227, 0.3131936192512512, 0.26737308502197266, 0.22067320346832275, 0.09496355056762695, 0.14743083715438843]\n",
            "step : 1019  ---  loss train : [0.5250959396362305, 0.2415776252746582, 0.1626986861228943, 0.17497897148132324, 0.48737722635269165, 0.06179642677307129, 0.4166058301925659, 0.24914544820785522, 0.19504612684249878, 0.16348814964294434]\n",
            "step : 1029  ---  loss train : [0.061378657817840576, 0.06782609224319458, 0.48718661069869995, 0.5502069592475891, 0.3397331237792969, 0.24029475450515747, 0.11159217357635498, 0.0942566990852356, 0.21145039796829224, 0.27670818567276]\n",
            "step : 1039  ---  loss train : [0.08462762832641602, 0.1996517777442932, 0.5701625347137451, 0.9186185002326965, 0.13751566410064697, 0.11611342430114746, 0.10384076833724976, 0.22202998399734497, 0.26992279291152954, 0.28324300050735474]\n",
            "step : 1049  ---  loss train : [0.2055983543395996, 0.2569073438644409, 0.17700964212417603, 0.2238348126411438, 0.10009729862213135, 0.3084947466850281, 0.39666956663131714, 0.2138679027557373, 0.1856619119644165, 0.09512370824813843]\n",
            "step : 1059  ---  loss train : [0.15915334224700928, 0.06288063526153564, 0.4459831118583679, 0.20370125770568848, 0.5553381443023682, 0.3397083282470703, 0.16921353340148926, 0.27244699001312256, 0.114449143409729, 0.22257941961288452]\n",
            "step : 1069  ---  loss train : [0.38253772258758545, 0.21311938762664795, 0.09429013729095459, 0.551943838596344, 0.09095770120620728, 0.4625011682510376, 0.14472585916519165, 0.37637072801589966, 0.10432535409927368, 0.35361093282699585]\n",
            "step : 1079  ---  loss train : [0.37676823139190674, 0.5439244508743286, 0.9995328783988953, 0.11216980218887329, 0.10960948467254639, 0.13756293058395386, 0.12053942680358887, 0.15820270776748657, 0.08542966842651367, 0.27476966381073]\n",
            "step : 1089  ---  loss train : [0.12187337875366211, 0.17989259958267212, 0.06904125213623047, 0.13912934064865112, 0.16708165407180786, 0.0712541937828064, 0.12570816278457642, 0.16621166467666626, 0.3744392991065979, 0.1752426028251648]\n",
            "step : 1099  ---  loss train : [0.43131494522094727, 0.29888588190078735, 0.9997643828392029, 0.08829492330551147, 0.09284466505050659, 0.16776782274246216, 0.12553471326828003, 0.32767796516418457, 0.40207040309906006, 0.08673763275146484]\n",
            "step : 1109  ---  loss train : [0.12512940168380737, 0.12601202726364136, 0.2912229299545288, 0.08648395538330078, 0.22333377599716187, 0.06905722618103027, 0.13204282522201538, 0.20812994241714478, 0.10968524217605591, 0.07544094324111938]\n",
            "step : 1119  ---  loss train : [0.10713094472885132, 0.16030406951904297, 0.16219580173492432, 0.2448420524597168, 0.41778600215911865, 0.10972201824188232, 0.14204132556915283, 0.2850247025489807, 0.09148716926574707, 0.08382195234298706]\n",
            "step : 1129  ---  loss train : [0.17422354221343994, 0.21497154235839844, 0.5006652474403381, 0.08553451299667358, 0.18597131967544556, 0.436359703540802, 0.1555633544921875, 0.22184914350509644, 0.16125404834747314, 0.256316602230072]\n",
            "step : 1139  ---  loss train : [0.2102428674697876, 0.266768217086792, 0.20275312662124634, 0.16852790117263794, 0.2762947082519531, 0.18560302257537842, 0.43330031633377075, 0.35253840684890747, 0.7016726732254028, 0.17662972211837769]\n",
            "step : 1149  ---  loss train : [0.0735059380531311, 0.1003982424736023, 0.28168487548828125, 0.09101462364196777, 0.2758514881134033, 0.15840160846710205, 0.0809052586555481, 0.08867013454437256, 0.41302746534347534, 0.1194196343421936]\n",
            "step : 1159  ---  loss train : [0.3388105034828186, 0.09583801031112671, 0.08669066429138184, 0.2585715055465698, 0.13929659128189087, 0.2577504515647888, 0.9997091889381409, 0.16467636823654175, 0.197329580783844, 0.10167181491851807]\n",
            "step : 1169  ---  loss train : [0.24796169996261597, 0.1207815408706665, 0.2350282073020935, 0.33358579874038696, 0.4428379535675049, 0.3349485993385315, 0.18895959854125977, 0.4169756770133972, 0.15318256616592407, 0.45218461751937866]\n",
            "step : 1179  ---  loss train : [0.24718302488327026, 0.16513442993164062, 0.19502472877502441, 0.18328136205673218, 0.2773815989494324, 0.14495182037353516, 0.1318468451499939, 0.3533428907394409, 0.2824551463127136, 0.18238943815231323]\n",
            "step : 1189  ---  loss train : [0.21602004766464233, 0.23517471551895142, 0.12493181228637695, 0.2617432475090027, 0.41473883390426636, 0.10169142484664917, 0.147413432598114, 0.2383405566215515, 0.31376147270202637, 0.0977371335029602]\n",
            "step : 1199  ---  loss train : [0.08537435531616211, 0.19616448879241943, 0.07787489891052246, 0.8460654616355896, 0.13330930471420288, 0.10259836912155151, 0.5527575016021729, 0.18631649017333984, 0.10147136449813843, 0.10429471731185913]\n",
            "step : 1209  ---  loss train : [0.16256463527679443, 0.1839839220046997, 0.32478803396224976, 0.08180004358291626, 0.0604708194732666, 0.2940836548805237, 0.12391424179077148, 0.4328600764274597, 0.646548867225647, 0.2225790023803711]\n",
            "step : 1219  ---  loss train : [0.4384177327156067, 0.12917488813400269, 0.30918800830841064, 0.4072089195251465, 0.21150285005569458, 0.1857210397720337, 0.16077810525894165, 0.17673468589782715, 0.1485554575920105, 0.3754599094390869]\n",
            "step : 1229  ---  loss train : [0.08075052499771118, 0.1736655831336975, 0.16144514083862305, 0.742525577545166, 0.4378887414932251, 0.14012330770492554, 0.29693979024887085, 0.3560004234313965, 0.154513418674469, 0.10564595460891724]\n",
            "step : 1239  ---  loss train : [0.09607833623886108, 0.05456036329269409, 0.07986050844192505, 0.37342017889022827, 0.1523461937904358, 0.12511146068572998, 0.43373674154281616, 0.15235179662704468, 0.1719973087310791, 0.23097407817840576]\n",
            "step : 1249  ---  loss train : [0.28778064250946045, 0.13497555255889893, 0.17890119552612305, 0.0752415657043457, 0.18651628494262695, 0.08525896072387695, 0.16645556688308716, 0.2628364562988281, 0.5150758028030396, 0.19429230690002441]\n",
            "step : 1259  ---  loss train : [0.33018529415130615, 0.5247257351875305, 0.3652472496032715, 0.07037705183029175, 0.541650652885437, 0.07658153772354126, 0.28145164251327515, 0.22572952508926392, 0.653627872467041, 0.0507044792175293]\n",
            "step : 1269  ---  loss train : [0.13502788543701172, 0.10536885261535645, 0.44364118576049805, 0.17632925510406494, 0.14543956518173218, 0.2932395339012146, 0.2126256823539734, 0.16212087869644165, 0.16203689575195312, 0.16468453407287598]\n",
            "step : 1279  ---  loss train : [0.26962655782699585, 0.24236750602722168, 0.49693524837493896, 0.5865961313247681, 0.16936755180358887, 0.05029541254043579, 0.06667298078536987, 0.09208559989929199, 0.157382071018219, 0.4335072636604309]\n",
            "step : 1289  ---  loss train : [0.3573675751686096, 0.12785524129867554, 0.4743715524673462, 0.21133488416671753, 0.19852906465530396, 0.0851706862449646, 0.1761898398399353, 0.1481020450592041, 0.09734451770782471, 0.10462874174118042]\n",
            "step : 1299  ---  loss train : [0.22559499740600586, 0.05897301435470581, 0.18541806936264038, 0.20349162817001343, 0.13305073976516724, 0.19720208644866943, 0.1792547106742859, 0.14301514625549316, 0.13491272926330566, 0.22316056489944458]\n",
            "step : 1309  ---  loss train : [0.18407094478607178, 0.18239355087280273, 0.09319275617599487, 0.06032824516296387, 0.15715277194976807, 0.09984660148620605, 0.08687621355056763, 0.531680703163147, 0.2164219617843628, 0.16013383865356445]\n",
            "step : 1319  ---  loss train : [0.12220227718353271, 0.14882999658584595, 0.18397927284240723, 0.1913743019104004, 0.3237522840499878, 0.12438559532165527, 0.2065998911857605, 0.6674661636352539, 0.16050684452056885, 0.5433895587921143]\n",
            "step : 1329  ---  loss train : [0.3448094129562378, 0.3286539912223816, 0.14021414518356323, 0.15293854475021362, 0.23656058311462402, 0.16909635066986084, 0.057956814765930176, 0.17141109704971313, 0.34405070543289185, 0.09533721208572388]\n",
            "step : 1339  ---  loss train : [0.09449046850204468, 0.2423805594444275, 0.06180143356323242, 0.07823920249938965, 0.1341332197189331, 0.2847081422805786, 0.1456853747367859, 0.1342850923538208, 0.12795573472976685, 0.12179994583129883]\n",
            "step : 1349  ---  loss train : [0.12849706411361694, 0.14324450492858887, 0.20694643259048462, 0.276919960975647, 0.16698390245437622, 0.11433994770050049, 0.20213234424591064, 0.1923586130142212, 0.13464754819869995, 0.11657959222793579]\n",
            "step : 1359  ---  loss train : [0.19063735008239746, 0.06109023094177246, 0.6526364684104919, 0.09348702430725098, 0.14516198635101318, 0.14710891246795654, 0.36107826232910156, 0.24733608961105347, 0.06010359525680542, 0.18735229969024658]\n",
            "step : 1369  ---  loss train : [0.24141639471054077, 0.2675516605377197, 0.08186215162277222, 0.25437837839126587, 0.10519003868103027, 0.1326259970664978, 0.3952169418334961, 0.133459210395813, 0.3661600947380066, 0.3743036389350891]\n",
            "step : 1379  ---  loss train : [0.1335207223892212, 0.06305742263793945, 0.10289406776428223, 0.1263822317123413, 0.14412760734558105, 0.10978925228118896, 0.13567644357681274, 0.1087425947189331, 0.15421777963638306, 0.08262383937835693]\n",
            "step : 1389  ---  loss train : [0.08350622653961182, 0.35903114080429077, 0.15297859907150269, 0.5363035798072815, 0.5606018304824829, 0.0833175778388977, 0.17163687944412231, 0.1716262698173523, 0.2139430046081543, 0.14437443017959595]\n",
            "step : 1399  ---  loss train : [0.2305179238319397, 0.16809433698654175, 0.053218066692352295, 0.12462037801742554, 0.6881511211395264, 0.13400572538375854, 0.12041103839874268, 0.1217184066772461, 0.1708114743232727, 0.10537725687026978]\n",
            "step : 1407  ---  loss train : [0.9997536540031433, 0.9997248649597168, 0.9887334108352661, 0.2804661989212036, 0.8180844783782959, 0.9976832270622253, 0.9957799911499023, 0.9350670576095581, 0.8553297519683838, 0.9829928278923035, 0.9989566206932068, 0.999683141708374, 0.9993509650230408, 0.23344898223876953, 0.05323624610900879, 0.06829965114593506, 0.20535558462142944, 0.8689131736755371, 0.8339483141899109, 0.9967228770256042, 0.998035192489624, 0.9962574243545532, 0.998721718788147, 0.999262273311615, 0.34117305278778076, 0.19265073537826538, 0.17358136177062988, 0.16694855690002441, 0.11073529720306396, 0.16284972429275513, 0.9845267534255981, 0.8157031536102295, 0.8247803449630737, 0.9782761931419373, 0.9970518350601196, 0.9981291890144348, 0.9997376799583435, 0.9349018335342407, 0.9483361840248108, 0.9964644908905029, 0.2660031318664551, 0.12145107984542847, 0.11549615859985352, 0.16484063863754272, 0.23438072204589844, 0.28410905599594116, 0.9998862147331238, 0.999927282333374, 0.9999266266822815, 0.9999206066131592, 0.9999139904975891, 0.9998799562454224, 0.9998853802680969, 0.9792265892028809, 0.9991174936294556, 0.9995542168617249, 0.9998870491981506, 0.9999262094497681, 0.9999365210533142, 0.9998445510864258, 0.9995765686035156, 0.3827601671218872, 0.10794126987457275, 0.18437176942825317, 0.3185211420059204, 0.9998137354850769, 0.9989943504333496, 0.854627251625061, 0.9992193579673767, 0.999606192111969, 0.9997150897979736, 0.999755322933197, 0.999661386013031, 0.99957275390625, 0.9994325041770935, 0.9989432096481323, 0.9944479465484619, 0.9853964447975159, 0.9937663078308105, 0.9615399837493896, 0.991221010684967, 0.9943926334381104, 0.8932923078536987, 0.9973715543746948, 0.442423939704895, 0.8285212516784668, 0.7138030529022217, 0.5910065174102783, 0.5915852785110474, 0.5553187727928162, 0.44048088788986206, 0.3245691657066345, 0.11108875274658203, 0.14506053924560547, 0.8583712577819824, 0.8868054151535034, 0.9474645256996155, 0.9713358283042908, 0.9350693821907043, 0.8917465209960938, 0.8940389752388, 0.7950010299682617, 0.9991666674613953, 0.998603880405426, 0.35885846614837646, 0.12196528911590576, 0.11359316110610962, 0.2292318344116211, 0.24607282876968384, 0.869867205619812, 0.8480386734008789, 0.9916060566902161, 0.9990674257278442, 0.9995105266571045, 0.9951213598251343, 0.9713148474693298, 0.586387038230896, 0.15597397089004517, 0.06231284141540527, 0.09008312225341797, 0.12954050302505493, 0.28826022148132324, 0.9766109585762024, 0.9969304203987122, 0.9987863302230835, 0.9993144869804382, 0.9996362924575806, 0.9995056986808777, 0.9972012639045715, 0.9822622537612915, 0.9924562573432922, 0.9941685795783997, 0.9966622591018677, 0.9952621459960938, 0.8781583905220032, 0.9173761010169983, 0.8680927753448486, 0.8870645761489868, 0.9982705116271973, 0.9989443421363831, 0.8469448089599609, 0.8466881513595581, 0.8580739498138428, 0.9968821406364441, 0.9991282224655151, 0.9993205666542053, 0.9988648891448975, 0.9959679245948792, 0.8663956522941589, 0.8135296106338501, 0.7961772680282593, 0.658696711063385]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [01:50<59:54, 37.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 1407  ---  mean_dsc : 0.5556239338719078\n",
            "step : 1409  ---  loss train : [0.20147693157196045, 0.1798531413078308, 0.2910512089729309, 0.19014298915863037, 0.08162069320678711, 0.14092206954956055, 0.07956689596176147, 0.10551518201828003, 0.2120816707611084, 0.16628903150558472]\n",
            "step : 1419  ---  loss train : [0.6732090711593628, 0.36184847354888916, 0.44252198934555054, 0.0788125991821289, 0.0938795804977417, 0.14664989709854126, 0.4122741222381592, 0.14803409576416016, 0.12838846445083618, 0.11132383346557617]\n",
            "step : 1429  ---  loss train : [0.24753481149673462, 0.25076162815093994, 0.15251779556274414, 0.06042438745498657, 0.14441484212875366, 0.22168833017349243, 0.09996944665908813, 0.05855870246887207, 0.142974853515625, 0.3906858563423157]\n",
            "step : 1439  ---  loss train : [0.19493108987808228, 0.09771382808685303, 0.12809687852859497, 0.19617575407028198, 0.06381040811538696, 0.36103183031082153, 0.06932574510574341, 0.11149686574935913, 0.08884501457214355, 0.14333659410476685]\n",
            "step : 1449  ---  loss train : [0.06582236289978027, 0.24169456958770752, 0.062368810176849365, 0.1371622085571289, 0.10012614727020264, 0.4294412136077881, 0.160797119140625, 0.14593756198883057, 0.0778423547744751, 0.5936976671218872]\n",
            "step : 1459  ---  loss train : [0.07298636436462402, 0.11008656024932861, 0.40614742040634155, 0.19519531726837158, 0.10454338788986206, 0.497255802154541, 0.41344064474105835, 0.6657857894897461, 0.3438512682914734, 0.1975821852684021]\n",
            "step : 1469  ---  loss train : [0.05631905794143677, 0.08864951133728027, 0.0916968584060669, 0.08240616321563721, 0.10102534294128418, 0.13133937120437622, 0.647635817527771, 0.27248698472976685, 0.09510296583175659, 0.23834198713302612]\n",
            "step : 1479  ---  loss train : [0.06397426128387451, 0.20177912712097168, 0.07609015703201294, 0.055679261684417725, 0.31629830598831177, 0.2688435912132263, 0.2164040207862854, 0.0771259069442749, 0.10629194974899292, 0.33339107036590576]\n",
            "step : 1489  ---  loss train : [0.2026265263557434, 0.15424126386642456, 0.20701968669891357, 0.36003929376602173, 0.12745994329452515, 0.42445141077041626, 0.29793494939804077, 0.18376386165618896, 0.14363902807235718, 0.056498587131500244]\n",
            "step : 1499  ---  loss train : [0.062296509742736816, 0.3481740951538086, 0.4222401976585388, 0.3019409775733948, 0.22247326374053955, 0.11441051959991455, 0.07915568351745605, 0.19073253870010376, 0.1636054515838623, 0.07149213552474976]\n",
            "step : 1509  ---  loss train : [0.2172614336013794, 0.5821921825408936, 0.9997116923332214, 0.14547944068908691, 0.13028675317764282, 0.09552401304244995, 0.19469588994979858, 0.2396431565284729, 0.27851778268814087, 0.17194658517837524]\n",
            "step : 1519  ---  loss train : [0.2619709372520447, 0.13755571842193604, 0.13436424732208252, 0.055840253829956055, 0.32395362854003906, 0.35962390899658203, 0.21357733011245728, 0.250000536441803, 0.07447266578674316, 0.1972569227218628]\n",
            "step : 1529  ---  loss train : [0.0945085883140564, 0.3963611125946045, 0.19547909498214722, 0.417147159576416, 0.29200804233551025, 0.18257445096969604, 0.2815093398094177, 0.12800884246826172, 0.4108577370643616, 0.39041805267333984]\n",
            "step : 1539  ---  loss train : [0.16397088766098022, 0.12437272071838379, 0.5161536931991577, 0.08620482683181763, 0.45005977153778076, 0.11314600706100464, 0.39971399307250977, 0.09359431266784668, 0.22029811143875122, 0.1281772255897522]\n",
            "step : 1549  ---  loss train : [0.6816303133964539, 0.9997616410255432, 0.09066224098205566, 0.08038783073425293, 0.14330554008483887, 0.13910561800003052, 0.14585024118423462, 0.07613992691040039, 0.2606624960899353, 0.06475144624710083]\n",
            "step : 1559  ---  loss train : [0.1638491153717041, 0.07008683681488037, 0.0786401629447937, 0.1730254888534546, 0.07141894102096558, 0.0933113694190979, 0.1592079997062683, 0.36378395557403564, 0.16277992725372314, 0.28682422637939453]\n",
            "step : 1569  ---  loss train : [0.22407066822052002, 0.9997502565383911, 0.11672806739807129, 0.0872037410736084, 0.17050689458847046, 0.11443805694580078, 0.16370588541030884, 0.3730989098548889, 0.07847976684570312, 0.11031025648117065]\n",
            "step : 1579  ---  loss train : [0.12583869695663452, 0.2926671504974365, 0.09425032138824463, 0.2563803791999817, 0.06015932559967041, 0.10852247476577759, 0.1783452033996582, 0.1177874207496643, 0.1946089267730713, 0.10596400499343872]\n",
            "step : 1589  ---  loss train : [0.16209793090820312, 0.17519015073776245, 0.22940492630004883, 0.47446727752685547, 0.11151868104934692, 0.12213289737701416, 0.2760302424430847, 0.08697241544723511, 0.07793855667114258, 0.16969364881515503]\n",
            "step : 1599  ---  loss train : [0.21985191106796265, 0.49917590618133545, 0.09321326017379761, 0.1636652946472168, 0.41967809200286865, 0.1592535376548767, 0.21917396783828735, 0.1601628065109253, 0.23166030645370483, 0.20317596197128296]\n",
            "step : 1609  ---  loss train : [0.2592334747314453, 0.20800751447677612, 0.14720839262008667, 0.2751728892326355, 0.19775784015655518, 0.42738664150238037, 0.3397302031517029, 0.6699665784835815, 0.17504668235778809, 0.07010602951049805]\n",
            "step : 1619  ---  loss train : [0.10039365291595459, 0.31985002756118774, 0.09858417510986328, 0.29625511169433594, 0.14817112684249878, 0.07551777362823486, 0.08416545391082764, 0.40281403064727783, 0.11082547903060913, 0.32840174436569214]\n",
            "step : 1629  ---  loss train : [0.08734208345413208, 0.08177274465560913, 0.2529885172843933, 0.12130671739578247, 0.26805347204208374, 0.9997258186340332, 0.17806553840637207, 0.2522030472755432, 0.09372580051422119, 0.22099268436431885]\n",
            "step : 1639  ---  loss train : [0.11072373390197754, 0.22997140884399414, 0.33385545015335083, 0.3366190791130066, 0.33257365226745605, 0.19316798448562622, 0.4006577134132385, 0.14209973812103271, 0.4201430082321167, 0.2290518879890442]\n",
            "step : 1649  ---  loss train : [0.15947723388671875, 0.19294977188110352, 0.18181604146957397, 0.27082037925720215, 0.14643847942352295, 0.1289542317390442, 0.33283495903015137, 0.2662716507911682, 0.16346007585525513, 0.20940625667572021]\n",
            "step : 1659  ---  loss train : [0.20072275400161743, 0.11791110038757324, 0.2220812439918518, 0.3977486491203308, 0.08344972133636475, 0.1479027271270752, 0.2298051118850708, 0.30634135007858276, 0.09343111515045166, 0.07500529289245605]\n",
            "step : 1669  ---  loss train : [0.174344003200531, 0.07642495632171631, 0.841218113899231, 0.12748515605926514, 0.1001666784286499, 0.48815810680389404, 0.1830245852470398, 0.09040951728820801, 0.10004919767379761, 0.14316117763519287]\n",
            "step : 1679  ---  loss train : [0.16064494848251343, 0.2944876551628113, 0.07661932706832886, 0.05910217761993408, 0.27268147468566895, 0.11937743425369263, 0.3886604905128479, 0.6456947326660156, 0.21290040016174316, 0.41464024782180786]\n",
            "step : 1689  ---  loss train : [0.11419230699539185, 0.31206148862838745, 0.43250948190689087, 0.20649391412734985, 0.17332369089126587, 0.15537869930267334, 0.18642258644104004, 0.14683973789215088, 0.44455045461654663, 0.07917100191116333]\n",
            "step : 1699  ---  loss train : [0.18500953912734985, 0.15214920043945312, 0.7236876487731934, 0.40365540981292725, 0.12007540464401245, 0.29805266857147217, 0.36299264430999756, 0.11517876386642456, 0.13233345746994019, 0.09589594602584839]\n",
            "step : 1709  ---  loss train : [0.05453723669052124, 0.08254045248031616, 0.37143588066101074, 0.13125962018966675, 0.12733423709869385, 0.5057787299156189, 0.15943557024002075, 0.18715912103652954, 0.20089560747146606, 0.23998695611953735]\n",
            "step : 1719  ---  loss train : [0.1497277021408081, 0.18922096490859985, 0.07016563415527344, 0.16917860507965088, 0.08559316396713257, 0.12624478340148926, 0.2583797574043274, 0.5203045606613159, 0.18476295471191406, 0.35150396823883057]\n",
            "step : 1729  ---  loss train : [0.5516046285629272, 0.36178553104400635, 0.07294416427612305, 0.5042440891265869, 0.07935500144958496, 0.2689747214317322, 0.22672104835510254, 0.6448184847831726, 0.038008272647857666, 0.1568077802658081]\n",
            "step : 1739  ---  loss train : [0.10200941562652588, 0.423664391040802, 0.17001307010650635, 0.13165301084518433, 0.30377042293548584, 0.21186482906341553, 0.15446776151657104, 0.16752606630325317, 0.145399808883667, 0.2954310178756714]\n",
            "step : 1749  ---  loss train : [0.2289089560508728, 0.5303072333335876, 0.5663049817085266, 0.16879868507385254, 0.052776038646698, 0.06735420227050781, 0.09456515312194824, 0.14339882135391235, 0.4072619676589966, 0.3156704902648926]\n",
            "step : 1759  ---  loss train : [0.11131995916366577, 0.40663641691207886, 0.1607905626296997, 0.16592979431152344, 0.11762887239456177, 0.14297610521316528, 0.14382988214492798, 0.056426167488098145, 0.11162257194519043, 0.2496039867401123]\n",
            "step : 1769  ---  loss train : [0.06384462118148804, 0.1818770170211792, 0.20093142986297607, 0.1301540732383728, 0.17241597175598145, 0.23995625972747803, 0.1156240701675415, 0.12159609794616699, 0.23452317714691162, 0.17635512351989746]\n",
            "step : 1779  ---  loss train : [0.1913602352142334, 0.08430379629135132, 0.04535776376724243, 0.14344066381454468, 0.08861654996871948, 0.09100621938705444, 0.4393885135650635, 0.248923659324646, 0.15591734647750854, 0.13868802785873413]\n",
            "step : 1789  ---  loss train : [0.14398115873336792, 0.19678378105163574, 0.18556839227676392, 0.39902615547180176, 0.1261875033378601, 0.2165868878364563, 0.6043208241462708, 0.15874618291854858, 0.5056584477424622, 0.34246593713760376]\n",
            "step : 1799  ---  loss train : [0.39307349920272827, 0.13121098279953003, 0.14716750383377075, 0.24334603548049927, 0.16493314504623413, 0.059052467346191406, 0.17710912227630615, 0.3416016101837158, 0.10002464056015015, 0.09336215257644653]\n",
            "step : 1809  ---  loss train : [0.2272152304649353, 0.061080217361450195, 0.07468754053115845, 0.13079595565795898, 0.30552220344543457, 0.14241892099380493, 0.13124758005142212, 0.1286216378211975, 0.11804372072219849, 0.11697423458099365]\n",
            "step : 1819  ---  loss train : [0.13965821266174316, 0.2112836241722107, 0.26707547903060913, 0.16235309839248657, 0.10454541444778442, 0.1942043900489807, 0.1767054796218872, 0.13214713335037231, 0.1185539960861206, 0.18793261051177979]\n",
            "step : 1829  ---  loss train : [0.06050705909729004, 0.6072880625724792, 0.09371858835220337, 0.15179985761642456, 0.2339422106742859, 0.43237966299057007, 0.31109052896499634, 0.06258910894393921, 0.18933647871017456, 0.22148913145065308]\n",
            "step : 1839  ---  loss train : [0.26810407638549805, 0.07526475191116333, 0.2091485857963562, 0.09238594770431519, 0.11648058891296387, 0.3220462203025818, 0.14086437225341797, 0.4031710624694824, 0.4212467074394226, 0.12361174821853638]\n",
            "step : 1849  ---  loss train : [0.06370800733566284, 0.10643750429153442, 0.12444335222244263, 0.17922896146774292, 0.1032288670539856, 0.13035106658935547, 0.10138261318206787, 0.14567548036575317, 0.08584076166152954, 0.10012072324752808]\n",
            "step : 1859  ---  loss train : [0.42269378900527954, 0.14041048288345337, 0.5585627555847168, 0.6009814739227295, 0.11393600702285767, 0.2512732148170471, 0.19613492488861084, 0.13369768857955933, 0.10962116718292236, 0.17140531539916992]\n",
            "step : 1869  ---  loss train : [0.13255131244659424, 0.05615508556365967, 0.11931490898132324, 0.5712734460830688, 0.13380444049835205, 0.12040144205093384, 0.13559871912002563, 0.1805219054222107, 0.09880191087722778, 0.22003328800201416]\n",
            "step : 1876  ---  loss train : [0.9997208118438721, 0.9993196129798889, 0.9993062019348145, 0.4053497910499573, 0.7555500864982605, 0.9978558421134949, 0.7607930302619934, 0.7785530686378479, 0.7640590071678162, 0.7512482404708862, 0.7707538604736328, 0.9992254376411438, 0.999182403087616, 0.2749476432800293, 0.044996559619903564, 0.06639707088470459, 0.23732727766036987, 0.7557555437088013, 0.7351100444793701, 0.9571293592453003, 0.9528009295463562, 0.9128918647766113, 0.9983169436454773, 0.9990531802177429, 0.27068233489990234, 0.18049103021621704, 0.1669473648071289, 0.16140210628509521, 0.0992441177368164, 0.1631406545639038, 0.9841699004173279, 0.7791500687599182, 0.7832303047180176, 0.8003303408622742, 0.8310704827308655, 0.9984001517295837, 0.9997255206108093, 0.7352394461631775, 0.7460982799530029, 0.7422575354576111, 0.5803166031837463, 0.14051681756973267, 0.14002126455307007, 0.21125715970993042, 0.27061599493026733, 0.26605165004730225, 0.9998587965965271, 0.9999207854270935, 0.9999224543571472, 0.9998984932899475, 0.999881386756897, 0.9998193383216858, 0.9998123049736023, 0.9313293099403381, 0.9998505115509033, 0.9997987747192383, 0.999809205532074, 0.9998853206634521, 0.9999216198921204, 0.9998190999031067, 0.9994286894798279, 0.24385470151901245, 0.09308862686157227, 0.1619463562965393, 0.3014274835586548, 0.999796450138092, 0.9986668825149536, 0.7553334832191467, 0.9995502829551697, 0.9995744228363037, 0.9994854927062988, 0.9996727108955383, 0.9995787143707275, 0.9991914629936218, 0.9986741542816162, 0.9832242131233215, 0.8820343017578125, 0.7914544343948364, 0.9772540330886841, 0.8025470972061157, 0.8637259006500244, 0.9565610885620117, 0.7991585731506348, 0.9981062412261963, 0.4758189916610718, 0.945329487323761, 0.7475636601448059, 0.6015459299087524, 0.597069263458252, 0.5621856451034546, 0.45436882972717285, 0.32571524381637573, 0.1034318208694458, 0.1421576738357544, 0.7830327153205872, 0.7792968153953552, 0.7846409678459167, 0.7845332622528076, 0.7648523449897766, 0.7669627070426941, 0.766614556312561, 0.7747714519500732, 0.997821569442749, 0.7818670272827148, 0.2574443817138672, 0.09680682420730591, 0.08201354742050171, 0.18963050842285156, 0.19189000129699707, 0.7635017037391663, 0.7450770139694214, 0.9425578117370605, 0.9988466501235962, 0.9992257952690125, 0.7830312848091125, 0.7730971574783325, 0.5914672613143921, 0.15129566192626953, 0.05431795120239258, 0.07815808057785034, 0.10665374994277954, 0.24723124504089355, 0.9761740565299988, 0.8738081455230713, 0.9836276769638062, 0.9984883666038513, 0.9994131326675415, 0.9987534284591675, 0.9571505188941956, 0.7875795364379883, 0.9448401927947998, 0.9509792327880859, 0.9953101873397827, 0.9174318313598633, 0.779867947101593, 0.7800813913345337, 0.7815476059913635, 0.7820315361022949, 0.7782821655273438, 0.7718501091003418, 0.7768009901046753, 0.7775139808654785, 0.7807741761207581, 0.9979217052459717, 0.9993788599967957, 0.9995054602622986, 0.9992547035217285, 0.9960890412330627, 0.744831919670105, 0.748085618019104, 0.755454421043396, 0.606417715549469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [02:26<58:38, 36.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 1876  ---  mean_dsc : 0.557095640411165\n",
            "step : 1879  ---  loss train : [0.16360175609588623, 0.3710933327674866, 0.15476852655410767, 0.08447545766830444, 0.14041835069656372, 0.07026427984237671, 0.05676257610321045, 0.15932798385620117, 0.140530526638031, 0.554228663444519]\n",
            "step : 1889  ---  loss train : [0.08999359607696533, 0.33228516578674316, 0.07256442308425903, 0.10262399911880493, 0.15911966562271118, 0.43669265508651733, 0.14402973651885986, 0.1072232723236084, 0.07227945327758789, 0.2336968183517456]\n",
            "step : 1899  ---  loss train : [0.24661898612976074, 0.1420118808746338, 0.0673016905784607, 0.11663448810577393, 0.21018940210342407, 0.07119178771972656, 0.06306195259094238, 0.13732314109802246, 0.3868824243545532, 0.1567232608795166]\n",
            "step : 1909  ---  loss train : [0.09390133619308472, 0.13173407316207886, 0.2205195426940918, 0.06009566783905029, 0.36883753538131714, 0.06839621067047119, 0.10494530200958252, 0.07993191480636597, 0.13316959142684937, 0.06567376852035522]\n",
            "step : 1919  ---  loss train : [0.23168832063674927, 0.05418497323989868, 0.10979068279266357, 0.10519993305206299, 0.267558217048645, 0.11594521999359131, 0.14379817247390747, 0.071408212184906, 0.563463568687439, 0.0721932053565979]\n",
            "step : 1929  ---  loss train : [0.08303475379943848, 0.3379892110824585, 0.18405139446258545, 0.10533732175827026, 0.5125836730003357, 0.372186541557312, 0.6756678223609924, 0.3335006833076477, 0.19924861192703247, 0.05413699150085449]\n",
            "step : 1939  ---  loss train : [0.08853983879089355, 0.09232890605926514, 0.07635343074798584, 0.10255962610244751, 0.12749814987182617, 0.6370916366577148, 0.27228599786758423, 0.09267669916152954, 0.2299688458442688, 0.061156272888183594]\n",
            "step : 1949  ---  loss train : [0.2176605463027954, 0.07689261436462402, 0.050350725650787354, 0.3021012544631958, 0.25366127490997314, 0.20731133222579956, 0.06935274600982666, 0.09622174501419067, 0.33485960960388184, 0.24252140522003174]\n",
            "step : 1959  ---  loss train : [0.15343379974365234, 0.17033320665359497, 0.31252723932266235, 0.18707138299942017, 0.4291056990623474, 0.26258784532546997, 0.16550129652023315, 0.1459546685218811, 0.05368077754974365, 0.056811630725860596]\n",
            "step : 1969  ---  loss train : [0.3599581718444824, 0.39881807565689087, 0.3177914619445801, 0.22511547803878784, 0.10907196998596191, 0.07734531164169312, 0.20799601078033447, 0.18553000688552856, 0.07463103532791138, 0.2064473032951355]\n",
            "step : 1979  ---  loss train : [0.4847118854522705, 0.6367760300636292, 0.11332601308822632, 0.12285536527633667, 0.0941697359085083, 0.2021108865737915, 0.25046294927597046, 0.28147274255752563, 0.20477217435836792, 0.2592013478279114]\n",
            "step : 1989  ---  loss train : [0.14509910345077515, 0.16500306129455566, 0.05908697843551636, 0.3118102550506592, 0.31972014904022217, 0.21078550815582275, 0.16733217239379883, 0.07887983322143555, 0.15357708930969238, 0.06277638673782349]\n",
            "step : 1999  ---  loss train : [0.30944007635116577, 0.20479905605316162, 0.42848682403564453, 0.2974421977996826, 0.16207146644592285, 0.32337480783462524, 0.11198747158050537, 0.4562113881111145, 0.39481890201568604, 0.14341843128204346]\n",
            "step : 2009  ---  loss train : [0.11469149589538574, 0.49867480993270874, 0.08050030469894409, 0.46062207221984863, 0.11647653579711914, 0.34842801094055176, 0.10106134414672852, 0.2841796875, 0.3308163285255432, 0.3619447350502014]\n",
            "step : 2019  ---  loss train : [0.9995033144950867, 0.08401572704315186, 0.10335445404052734, 0.12196582555770874, 0.10849791765213013, 0.1186266541481018, 0.08064842224121094, 0.27457302808761597, 0.11449569463729858, 0.17750626802444458]\n",
            "step : 2029  ---  loss train : [0.05912208557128906, 0.2940405607223511, 0.14550215005874634, 0.06093806028366089, 0.1051144003868103, 0.15918618440628052, 0.3500606417655945, 0.13304483890533447, 0.17851442098617554, 0.20322293043136597]\n",
            "step : 2039  ---  loss train : [0.999595046043396, 0.10063391923904419, 0.09568196535110474, 0.1594904661178589, 0.1319146752357483, 0.1902504563331604, 0.35754770040512085, 0.07505589723587036, 0.1054430603981018, 0.12442123889923096]\n",
            "step : 2049  ---  loss train : [0.26699644327163696, 0.07345670461654663, 0.22376364469528198, 0.06415987014770508, 0.11325854063034058, 0.19545817375183105, 0.11207658052444458, 0.13685190677642822, 0.10747706890106201, 0.15921658277511597]\n",
            "step : 2059  ---  loss train : [0.159565269947052, 0.24233269691467285, 0.2871776819229126, 0.1051221489906311, 0.13515007495880127, 0.2881849408149719, 0.09036481380462646, 0.07391685247421265, 0.1516684889793396, 0.20204806327819824]\n",
            "step : 2069  ---  loss train : [0.5019403696060181, 0.0892220139503479, 0.16427123546600342, 0.4363349676132202, 0.15168601274490356, 0.22576433420181274, 0.157112717628479, 0.21298950910568237, 0.20506376028060913, 0.2400531768798828]\n",
            "step : 2079  ---  loss train : [0.20153242349624634, 0.15133148431777954, 0.27454662322998047, 0.18448209762573242, 0.41490256786346436, 0.34437090158462524, 0.640994131565094, 0.17172092199325562, 0.07224047183990479, 0.09777402877807617]\n",
            "step : 2089  ---  loss train : [0.2524387836456299, 0.09111922979354858, 0.293255090713501, 0.14326125383377075, 0.07617628574371338, 0.07836586236953735, 0.3206024169921875, 0.1103748083114624, 0.3226441740989685, 0.08006888628005981]\n",
            "step : 2099  ---  loss train : [0.07449829578399658, 0.24470627307891846, 0.12521207332611084, 0.25506889820098877, 0.9997375011444092, 0.14583057165145874, 0.22045183181762695, 0.08496075868606567, 0.22024023532867432, 0.11131304502487183]\n",
            "step : 2109  ---  loss train : [0.2292797565460205, 0.3252338767051697, 0.32205474376678467, 0.3379177451133728, 0.19174349308013916, 0.41926664113998413, 0.1328752636909485, 0.2663501501083374, 0.20975303649902344, 0.15451353788375854]\n",
            "step : 2119  ---  loss train : [0.19126689434051514, 0.18343192338943481, 0.2539004683494568, 0.14608418941497803, 0.10991650819778442, 0.3284127712249756, 0.2483891248703003, 0.15847033262252808, 0.18769878149032593, 0.2123497724533081]\n",
            "step : 2129  ---  loss train : [0.13265424966812134, 0.17702531814575195, 0.26681381464004517, 0.07238882780075073, 0.17420148849487305, 0.19101107120513916, 0.29907387495040894, 0.10509949922561646, 0.07334131002426147, 0.18703323602676392]\n",
            "step : 2139  ---  loss train : [0.0794527530670166, 0.8456137180328369, 0.13701099157333374, 0.09012347459793091, 0.44480079412460327, 0.18023991584777832, 0.0868036150932312, 0.10284936428070068, 0.1640416979789734, 0.15784227848052979]\n",
            "step : 2149  ---  loss train : [0.29526567459106445, 0.0802263617515564, 0.05466383695602417, 0.28115028142929077, 0.115509033203125, 0.3455114960670471, 0.6446502804756165, 0.20278751850128174, 0.4473101496696472, 0.09080469608306885]\n",
            "step : 2159  ---  loss train : [0.6246892809867859, 0.2805502414703369, 0.18423360586166382, 0.16053146123886108, 0.17321711778640747, 0.14516031742095947, 0.13437360525131226, 0.31747984886169434, 0.07095515727996826, 0.15729224681854248]\n",
            "step : 2169  ---  loss train : [0.1250389814376831, 0.6659603714942932, 0.4254533648490906, 0.11554330587387085, 0.2937316298484802, 0.35413050651550293, 0.1162077784538269, 0.08529400825500488, 0.100796639919281, 0.05505406856536865]\n",
            "step : 2179  ---  loss train : [0.0744277834892273, 0.36691439151763916, 0.1251813769340515, 0.13202351331710815, 0.3696853518486023, 0.13348698616027832, 0.16444498300552368, 0.21170055866241455, 0.23315948247909546, 0.07520920038223267]\n",
            "step : 2189  ---  loss train : [0.19100135564804077, 0.06633925437927246, 0.15924537181854248, 0.08412301540374756, 0.11135119199752808, 0.25499802827835083, 0.5488731265068054, 0.17654675245285034, 0.32082706689834595, 0.49416428804397583]\n",
            "step : 2199  ---  loss train : [0.36102473735809326, 0.06456518173217773, 0.575674295425415, 0.06161421537399292, 0.2618752717971802, 0.20731759071350098, 0.5856645703315735, 0.037570297718048096, 0.13913357257843018, 0.09781122207641602]\n",
            "step : 2209  ---  loss train : [0.44618338346481323, 0.15988880395889282, 0.12992042303085327, 0.3403712511062622, 0.19196540117263794, 0.1601439118385315, 0.16952818632125854, 0.13152551651000977, 0.2757459282875061, 0.21656692028045654]\n",
            "step : 2219  ---  loss train : [0.5182960033416748, 0.5859473943710327, 0.1633654236793518, 0.04680788516998291, 0.06608933210372925, 0.08134704828262329, 0.14513814449310303, 0.43163949251174927, 0.2367020845413208, 0.11998075246810913]\n",
            "step : 2229  ---  loss train : [0.34777939319610596, 0.19422417879104614, 0.17631536722183228, 0.07592517137527466, 0.14103305339813232, 0.12952768802642822, 0.057834088802337646, 0.10205775499343872, 0.2646438479423523, 0.060916125774383545]\n",
            "step : 2239  ---  loss train : [0.16904014348983765, 0.2006293535232544, 0.12903809547424316, 0.1808701753616333, 0.19736593961715698, 0.11523377895355225, 0.1381981372833252, 0.2345241904258728, 0.18252795934677124, 0.18936944007873535]\n",
            "step : 2249  ---  loss train : [0.07824933528900146, 0.04431962966918945, 0.1403590440750122, 0.06346213817596436, 0.09077060222625732, 0.46811532974243164, 0.17737287282943726, 0.16415053606033325, 0.13241171836853027, 0.11148059368133545]\n",
            "step : 2259  ---  loss train : [0.1813846230506897, 0.17639309167861938, 0.31277334690093994, 0.12191641330718994, 0.21832501888275146, 0.6150376200675964, 0.15857058763504028, 0.46071767807006836, 0.3357091546058655, 0.34055596590042114]\n",
            "step : 2269  ---  loss train : [0.11855518817901611, 0.14848291873931885, 0.24959087371826172, 0.1637846827507019, 0.055207669734954834, 0.16910910606384277, 0.35795408487319946, 0.07686084508895874, 0.08718705177307129, 0.19741320610046387]\n",
            "step : 2279  ---  loss train : [0.05955207347869873, 0.07359451055526733, 0.12442749738693237, 0.26897281408309937, 0.13692301511764526, 0.12389546632766724, 0.12666505575180054, 0.11238300800323486, 0.12034428119659424, 0.13608133792877197]\n",
            "step : 2289  ---  loss train : [0.18543225526809692, 0.29413360357284546, 0.15366899967193604, 0.09520381689071655, 0.19842779636383057, 0.1718021035194397, 0.13092434406280518, 0.11542689800262451, 0.18979156017303467, 0.060980260372161865]\n",
            "step : 2299  ---  loss train : [0.6226644515991211, 0.09117692708969116, 0.13716793060302734, 0.13440346717834473, 0.3141580820083618, 0.2390516996383667, 0.05604463815689087, 0.18379873037338257, 0.20892459154129028, 0.2752261161804199]\n",
            "step : 2309  ---  loss train : [0.08028137683868408, 0.2114781141281128, 0.092934250831604, 0.11371415853500366, 0.3031713366508484, 0.13374751806259155, 0.3446587324142456, 0.3740438222885132, 0.11681884527206421, 0.059713006019592285]\n",
            "step : 2319  ---  loss train : [0.11974924802780151, 0.12671661376953125, 0.18250298500061035, 0.10528451204299927, 0.1246071457862854, 0.11665993928909302, 0.12605416774749756, 0.08859443664550781, 0.09870189428329468, 0.3900678753852844]\n",
            "step : 2329  ---  loss train : [0.13187140226364136, 0.4469943046569824, 0.49634623527526855, 0.12031412124633789, 0.16297119855880737, 0.178239643573761, 0.17978453636169434, 0.11609184741973877, 0.212394118309021, 0.16201335191726685]\n",
            "step : 2339  ---  loss train : [0.05283540487289429, 0.1334870457649231, 0.6310238838195801, 0.13334286212921143, 0.11892467737197876, 0.12852954864501953, 0.17368465662002563, 0.10224801301956177, 0.21305304765701294, 0.1882513165473938]\n",
            "step : 2345  ---  loss train : [0.9997329115867615, 0.9993554949760437, 0.9986032843589783, 0.4671436548233032, 0.9459551572799683, 0.9978518486022949, 0.6077206134796143, 0.6185343265533447, 0.5899127125740051, 0.567743182182312, 0.8786761164665222, 0.9777528643608093, 0.9978100657463074, 0.2884531617164612, 0.04611414670944214, 0.06656438112258911, 0.2283598780632019, 0.5838357210159302, 0.5552947521209717, 0.9836161136627197, 0.9905700087547302, 0.9798222780227661, 0.9983453750610352, 0.9990384578704834, 0.32646191120147705, 0.18907707929611206, 0.16088229417800903, 0.13931035995483398, 0.08816784620285034, 0.14321762323379517, 0.9836287498474121, 0.5638562440872192, 0.5880488157272339, 0.9115872383117676, 0.9642901420593262, 0.5824190378189087, 0.9995785355567932, 0.6121070384979248, 0.5889661908149719, 0.7675216197967529, 0.3695055842399597, 0.1437031626701355, 0.14921629428863525, 0.22850292921066284, 0.29738563299179077, 0.2572357654571533, 0.9998409152030945, 0.9999132752418518, 0.9999168515205383, 0.99990314245224, 0.9998876452445984, 0.9998202323913574, 0.9998298287391663, 0.9998716115951538, 0.9998675584793091, 0.9998369216918945, 0.9998513460159302, 0.9999074339866638, 0.9999290704727173, 0.9998288154602051, 0.9984455108642578, 0.16716188192367554, 0.08818882703781128, 0.14737272262573242, 0.28917938470840454, 0.9997824430465698, 0.9986575841903687, 0.6291816234588623, 0.9971777200698853, 0.999236524105072, 0.9994499087333679, 0.9996433258056641, 0.999570369720459, 0.9986782073974609, 0.9976903200149536, 0.9949971437454224, 0.9724313020706177, 0.8585243225097656, 0.986909031867981, 0.8478050827980042, 0.9713925123214722, 0.9862024784088135, 0.6968435049057007, 0.9981398582458496, 0.4490078091621399, 0.9374173879623413, 0.7545508146286011, 0.6153292655944824, 0.6088449954986572, 0.5748261213302612, 0.4728121757507324, 0.3515254259109497, 0.11237913370132446, 0.13836246728897095, 0.61374431848526, 0.6529273986816406, 0.760278582572937, 0.8154405355453491, 0.7358457446098328, 0.601615309715271, 0.5207172632217407, 0.5202839374542236, 0.9982452392578125, 0.7644829154014587, 0.2845245599746704, 0.10214215517044067, 0.10093885660171509, 0.19202005863189697, 0.36058270931243896, 0.6324465274810791, 0.6478110551834106, 0.6985834836959839, 0.9986440539360046, 0.9968372583389282, 0.9336623549461365, 0.9020819664001465, 0.589240312576294, 0.13997554779052734, 0.05572175979614258, 0.07459527254104614, 0.09589731693267822, 0.22757959365844727, 0.9751591086387634, 0.9779608249664307, 0.9950600266456604, 0.9981237053871155, 0.9993208050727844, 0.9983764290809631, 0.9917589426040649, 0.8990606069564819, 0.9792394042015076, 0.9797002077102661, 0.9599458575248718, 0.9713658690452576, 0.6512224674224854, 0.6546100378036499, 0.6238188743591309, 0.6083753108978271, 0.6045413017272949, 0.998365581035614, 0.6047937273979187, 0.6088317632675171, 0.6262604594230652, 0.997908890247345, 0.9993748664855957, 0.9995120167732239, 0.9992562532424927, 0.9960578083992004, 0.5429360866546631, 0.5286341905593872, 0.5237677693367004, 0.34945452213287354]\n",
            "step : 2345  ---  mean_dsc : 0.5582342639047815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [03:06<59:50, 37.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 2349  ---  loss train : [0.37432295083999634, 0.16788744926452637, 0.08816182613372803, 0.12436985969543457, 0.07084107398986816, 0.06392055749893188, 0.13656115531921387, 0.13922357559204102, 0.5406440496444702, 0.12330174446105957]\n",
            "step : 2359  ---  loss train : [0.28198814392089844, 0.0743861198425293, 0.09588617086410522, 0.1520981788635254, 0.4290447235107422, 0.1458139419555664, 0.11640065908432007, 0.09244745969772339, 0.24549990892410278, 0.2515227198600769]\n",
            "step : 2369  ---  loss train : [0.13910621404647827, 0.06418824195861816, 0.11062633991241455, 0.2060578465461731, 0.08152395486831665, 0.06282168626785278, 0.13680720329284668, 0.38129526376724243, 0.15328449010849, 0.09525960683822632]\n",
            "step : 2379  ---  loss train : [0.1258854866027832, 0.14702361822128296, 0.06057697534561157, 0.40678703784942627, 0.07052665948867798, 0.10222840309143066, 0.08566164970397949, 0.12700015306472778, 0.0690719485282898, 0.21364527940750122]\n",
            "step : 2389  ---  loss train : [0.05005675554275513, 0.10564708709716797, 0.10425293445587158, 0.28298288583755493, 0.12238657474517822, 0.1417684555053711, 0.06873929500579834, 0.6015836000442505, 0.07519984245300293, 0.10201913118362427]\n",
            "step : 2399  ---  loss train : [0.34126704931259155, 0.18414658308029175, 0.10533756017684937, 0.5141351222991943, 0.3948966860771179, 0.6809008717536926, 0.3545383810997009, 0.1288233995437622, 0.05299311876296997, 0.09377366304397583]\n",
            "step : 2409  ---  loss train : [0.09244096279144287, 0.07998001575469971, 0.09754705429077148, 0.1269717812538147, 0.5992631912231445, 0.27503758668899536, 0.09349679946899414, 0.2220504879951477, 0.06256687641143799, 0.22321099042892456]\n",
            "step : 2419  ---  loss train : [0.08303385972976685, 0.04894787073135376, 0.28741955757141113, 0.2514148950576782, 0.1722354292869568, 0.06752520799636841, 0.08254021406173706, 0.4156843423843384, 0.41020745038986206, 0.15305083990097046]\n",
            "step : 2429  ---  loss train : [0.12117153406143188, 0.321089506149292, 0.060577213764190674, 0.3403192162513733, 0.24619948863983154, 0.15666800737380981, 0.14568930864334106, 0.052981555461883545, 0.0592074990272522, 0.41483670473098755]\n",
            "step : 2439  ---  loss train : [0.39497268199920654, 0.3463354706764221, 0.22531837224960327, 0.10026723146438599, 0.07882273197174072, 0.20500081777572632, 0.18430787324905396, 0.07009172439575195, 0.19949710369110107, 0.34964466094970703]\n",
            "step : 2449  ---  loss train : [0.9925917983055115, 0.12877607345581055, 0.1139611005783081, 0.08809733390808105, 0.20263546705245972, 0.2193082571029663, 0.2803535461425781, 0.15781235694885254, 0.25783008337020874, 0.13065725564956665]\n",
            "step : 2459  ---  loss train : [0.135342538356781, 0.05755418539047241, 0.3037063479423523, 0.33917033672332764, 0.2141575813293457, 0.17304271459579468, 0.0701436996459961, 0.14827054738998413, 0.08368796110153198, 0.2891579866409302]\n",
            "step : 2469  ---  loss train : [0.18850594758987427, 0.38743072748184204, 0.2966659665107727, 0.14365226030349731, 0.24463289976119995, 0.10744118690490723, 0.22725415229797363, 0.3818979859352112, 0.12887567281723022, 0.10170209407806396]\n",
            "step : 2479  ---  loss train : [0.3980885148048401, 0.08133548498153687, 0.45026105642318726, 0.10756635665893555, 0.346413254737854, 0.09688043594360352, 0.2847468852996826, 0.22132492065429688, 0.3359289765357971, 0.9994937777519226]\n",
            "step : 2489  ---  loss train : [0.08694922924041748, 0.09187912940979004, 0.1311793327331543, 0.07299834489822388, 0.07483041286468506, 0.08321118354797363, 0.2751501202583313, 0.0908961296081543, 0.1694076657295227, 0.05805319547653198]\n",
            "step : 2499  ---  loss train : [0.07888573408126831, 0.11863654851913452, 0.06385624408721924, 0.08246177434921265, 0.1493901014328003, 0.35723042488098145, 0.13597947359085083, 0.23735743761062622, 0.2125788927078247, 0.9997383952140808]\n",
            "step : 2509  ---  loss train : [0.08607625961303711, 0.07793450355529785, 0.16330009698867798, 0.10987204313278198, 0.15039527416229248, 0.2847486138343811, 0.07621133327484131, 0.11115288734436035, 0.11520344018936157, 0.27336812019348145]\n",
            "step : 2519  ---  loss train : [0.1566973328590393, 0.25223541259765625, 0.07285946607589722, 0.09728401899337769, 0.34494978189468384, 0.14179128408432007, 0.08785831928253174, 0.07753431797027588, 0.15082460641860962, 0.16962367296218872]\n",
            "step : 2529  ---  loss train : [0.21101856231689453, 0.3324925899505615, 0.12409353256225586, 0.09490901231765747, 0.2272435426712036, 0.07175332307815552, 0.07125985622406006, 0.19333738088607788, 0.1825513243675232, 0.4980587959289551]\n",
            "step : 2539  ---  loss train : [0.060017943382263184, 0.16237854957580566, 0.24262934923171997, 0.14657658338546753, 0.20884168148040771, 0.15297365188598633, 0.18041515350341797, 0.18991827964782715, 0.21279412508010864, 0.23778659105300903]\n",
            "step : 2549  ---  loss train : [0.10076534748077393, 0.3014141321182251, 0.18950802087783813, 0.4523286819458008, 0.3496134281158447, 0.5786941051483154, 0.1643638014793396, 0.0688321590423584, 0.09649497270584106, 0.2673467993736267]\n",
            "step : 2559  ---  loss train : [0.10068488121032715, 0.2806819677352905, 0.1286817193031311, 0.07290387153625488, 0.07297152280807495, 0.2571437358856201, 0.1091618537902832, 0.2431301474571228, 0.06953203678131104, 0.06790423393249512]\n",
            "step : 2569  ---  loss train : [0.23604410886764526, 0.12076830863952637, 0.2577524781227112, 0.9997056126594543, 0.13651925325393677, 0.1766771674156189, 0.08254754543304443, 0.21591496467590332, 0.10761743783950806, 0.22276777029037476]\n",
            "step : 2579  ---  loss train : [0.3162277340888977, 0.3585449457168579, 0.3542417287826538, 0.2061651349067688, 0.42165881395339966, 0.12838369607925415, 0.3060597777366638, 0.19212830066680908, 0.15657711029052734, 0.19356942176818848]\n",
            "step : 2589  ---  loss train : [0.17603284120559692, 0.25689226388931274, 0.13803333044052124, 0.11507523059844971, 0.30503928661346436, 0.25680863857269287, 0.1732303500175476, 0.21518921852111816, 0.20112186670303345, 0.13525331020355225]\n",
            "step : 2599  ---  loss train : [0.11205166578292847, 0.23473304510116577, 0.0686306357383728, 0.13987267017364502, 0.12202239036560059, 0.3057931661605835, 0.0932081937789917, 0.0720788836479187, 0.17340224981307983, 0.071757972240448]\n",
            "step : 2609  ---  loss train : [0.7541700005531311, 0.09375995397567749, 0.09227991104125977, 0.20997071266174316, 0.17802172899246216, 0.08277732133865356, 0.10525429248809814, 0.10968416929244995, 0.10292577743530273, 0.27119261026382446]\n",
            "step : 2619  ---  loss train : [0.07151073217391968, 0.061946988105773926, 0.26709121465682983, 0.10407531261444092, 0.34729450941085815, 0.649175763130188, 0.19140779972076416, 0.46015167236328125, 0.08848458528518677, 0.35144513845443726]\n",
            "step : 2629  ---  loss train : [0.2591667175292969, 0.18510597944259644, 0.15093481540679932, 0.1428707242012024, 0.1278710961341858, 0.1322399377822876, 0.34449440240859985, 0.07158643007278442, 0.210557222366333, 0.2570362091064453]\n",
            "step : 2639  ---  loss train : [0.4924585819244385, 0.40256476402282715, 0.13101041316986084, 0.2327117919921875, 0.35569900274276733, 0.36918604373931885, 0.22389262914657593, 0.0977904200553894, 0.06058180332183838, 0.174513041973114]\n",
            "step : 2649  ---  loss train : [0.35373455286026, 0.1615796685218811, 0.2652992010116577, 0.6227582693099976, 0.2522088885307312, 0.23874258995056152, 0.23936724662780762, 0.2102392315864563, 0.08578497171401978, 0.19869500398635864]\n",
            "step : 2659  ---  loss train : [0.09252208471298218, 0.16024500131607056, 0.08171278238296509, 0.18771231174468994, 0.32314205169677734, 0.5668852925300598, 0.17023688554763794, 0.4088374376296997, 0.48308736085891724, 0.3611539602279663]\n",
            "step : 2669  ---  loss train : [0.10673689842224121, 0.5185341835021973, 0.09697502851486206, 0.2467254400253296, 0.2519451379776001, 0.48791933059692383, 0.04263812303543091, 0.12937086820602417, 0.11329776048660278, 0.3905298709869385]\n",
            "step : 2679  ---  loss train : [0.1915827989578247, 0.13399064540863037, 0.31858497858047485, 0.2179388403892517, 0.15711891651153564, 0.16988515853881836, 0.13425540924072266, 0.3030250668525696, 0.2159087061882019, 0.47075724601745605]\n",
            "step : 2689  ---  loss train : [0.5494721531867981, 0.15902113914489746, 0.05113774538040161, 0.060785651206970215, 0.09111899137496948, 0.1305943727493286, 0.44625264406204224, 0.296738862991333, 0.11868810653686523, 0.4083438515663147]\n",
            "step : 2699  ---  loss train : [0.17916148900985718, 0.16721391677856445, 0.11338269710540771, 0.13090944290161133, 0.12970715761184692, 0.05497008562088013, 0.10172230005264282, 0.23488658666610718, 0.05981963872909546, 0.17774468660354614]\n",
            "step : 2709  ---  loss train : [0.18401753902435303, 0.1299164891242981, 0.16169112920761108, 0.22640526294708252, 0.11017924547195435, 0.15582382678985596, 0.23689508438110352, 0.17431753873825073, 0.19386506080627441, 0.07410937547683716]\n",
            "step : 2719  ---  loss train : [0.06676822900772095, 0.13783931732177734, 0.06878900527954102, 0.09321272373199463, 0.4835207462310791, 0.18131178617477417, 0.17071574926376343, 0.1379779577255249, 0.10884356498718262, 0.18129843473434448]\n",
            "step : 2729  ---  loss train : [0.17075908184051514, 0.32556992769241333, 0.12212222814559937, 0.19762659072875977, 0.6326388120651245, 0.1599690318107605, 0.40024012327194214, 0.3603009581565857, 0.3211069703102112, 0.12128037214279175]\n",
            "step : 2739  ---  loss train : [0.14845657348632812, 0.23638856410980225, 0.1622638702392578, 0.053881049156188965, 0.167780339717865, 0.3464806079864502, 0.06921136379241943, 0.08037561178207397, 0.20359617471694946, 0.05124145746231079]\n",
            "step : 2749  ---  loss train : [0.0722920298576355, 0.12617164850234985, 0.19546282291412354, 0.12838512659072876, 0.12260633707046509, 0.11885613203048706, 0.1133987307548523, 0.12602627277374268, 0.13696837425231934, 0.17908775806427002]\n",
            "step : 2759  ---  loss train : [0.2999645471572876, 0.15117615461349487, 0.09538191556930542, 0.1906854510307312, 0.16561830043792725, 0.12638044357299805, 0.11680734157562256, 0.18598240613937378, 0.06246119737625122, 0.6295392513275146]\n",
            "step : 2769  ---  loss train : [0.09123796224594116, 0.13686883449554443, 0.09960722923278809, 0.2707783579826355, 0.21317988634109497, 0.05917394161224365, 0.17934948205947876, 0.18960446119308472, 0.28758877515792847, 0.07939237356185913]\n",
            "step : 2779  ---  loss train : [0.1608123779296875, 0.08503341674804688, 0.1097821593284607, 0.27296823263168335, 0.14820361137390137, 0.31924766302108765, 0.3444529175758362, 0.11357676982879639, 0.09956681728363037, 0.13293153047561646]\n",
            "step : 2789  ---  loss train : [0.11791747808456421, 0.20293879508972168, 0.09549897909164429, 0.12281674146652222, 0.1303984522819519, 0.12775355577468872, 0.08733725547790527, 0.10468071699142456, 0.4007266163825989, 0.12851232290267944]\n",
            "step : 2799  ---  loss train : [0.5087316036224365, 0.527967095375061, 0.11894339323043823, 0.23701471090316772, 0.1838666796684265, 0.1381436586380005, 0.09754639863967896, 0.18341606855392456, 0.10634702444076538, 0.06329154968261719]\n",
            "step : 2809  ---  loss train : [0.12124955654144287, 0.5808095335960388, 0.13629204034805298, 0.11631405353546143, 0.11717325448989868, 0.16492915153503418, 0.08091050386428833, 0.20303809642791748, 0.10751181840896606, 0.31916189193725586]\n",
            "step : 2814  ---  loss train : [0.9996490478515625, 0.9980083703994751, 0.9995813369750977, 0.36611491441726685, 0.5846667289733887, 0.8580923080444336, 0.6139364242553711, 0.6187426447868347, 0.5783920288085938, 0.5163943767547607, 0.6253583431243896, 0.860214114189148, 0.9983575940132141, 0.22710102796554565, 0.05180925130844116, 0.06992441415786743, 0.19576901197433472, 0.5331676006317139, 0.48488324880599976, 0.9153448343276978, 0.9307152032852173, 0.7244589328765869, 0.9978911280632019, 0.9965110421180725, 0.2628944516181946, 0.17650538682937622, 0.16921430826187134, 0.17190009355545044, 0.1126750111579895, 0.16735750436782837, 0.9836891293525696, 0.6052232384681702, 0.6200107336044312, 0.6423128843307495, 0.6009916067123413, 0.9985449314117432, 0.9996515512466431, 0.6209856867790222, 0.5791245102882385, 0.664108157157898, 0.33981406688690186, 0.12091171741485596, 0.11330670118331909, 0.16464835405349731, 0.251356840133667, 0.27338457107543945, 0.9998201727867126, 0.9999057054519653, 0.9999071359634399, 0.9998664259910583, 0.9998429417610168, 0.9996984601020813, 0.9996366500854492, 0.8606395721435547, 0.9997918009757996, 0.9996358156204224, 0.999699056148529, 0.9998493194580078, 0.9999175667762756, 0.9998182058334351, 0.9832375049591064, 0.1539057493209839, 0.1044774055480957, 0.18511027097702026, 0.32371842861175537, 0.9998096823692322, 0.9990743398666382, 0.5606366991996765, 0.9984077215194702, 0.9970069527626038, 0.9970380067825317, 0.9995406866073608, 0.9980429410934448, 0.986195981502533, 0.9518411755561829, 0.9755253791809082, 0.8237748146057129, 0.675869345664978, 0.9346373081207275, 0.6509188413619995, 0.6714363098144531, 0.9109206199645996, 0.6425189971923828, 0.9980745315551758, 0.4621375799179077, 0.96806401014328, 0.7715767621994019, 0.6165324449539185, 0.6215237379074097, 0.5877581834793091, 0.4782639741897583, 0.3474978804588318, 0.0790020227432251, 0.13750922679901123, 0.578416109085083, 0.555892825126648, 0.5528045892715454, 0.5449051260948181, 0.5341297388076782, 0.5340628623962402, 0.5211312770843506, 0.5236190557479858, 0.9744670391082764, 0.6020394563674927, 0.3167375922203064, 0.11451578140258789, 0.09569674730300903, 0.20587146282196045, 0.2329539656639099, 0.5608100891113281, 0.5359609127044678, 0.9936699271202087, 0.7073867321014404, 0.9030346274375916, 0.9018389582633972, 0.6771199703216553, 0.5865795016288757, 0.15261709690093994, 0.05246090888977051, 0.08089697360992432, 0.11102724075317383, 0.24661654233932495, 0.974936842918396, 0.836092472076416, 0.9774815440177917, 0.9862030744552612, 0.9963045120239258, 0.983424961566925, 0.9823582768440247, 0.7568211555480957, 0.8703504800796509, 0.7774063944816589, 0.7174760103225708, 0.73560631275177, 0.590523362159729, 0.5882455706596375, 0.5691949129104614, 0.5743804574012756, 0.5755336284637451, 0.5606091022491455, 0.5693784356117249, 0.577039361000061, 0.5983325242996216, 0.9978852868080139, 0.9993555545806885, 0.9994943737983704, 0.9992403388023376, 0.9960345029830933, 0.5024869441986084, 0.48713719844818115, 0.48430925607681274, 0.31442737579345703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [03:48<1:01:53, 39.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 2814  ---  mean_dsc : 0.5549297840745765\n",
            "step : 2819  ---  loss train : [0.10339128971099854, 0.08336585760116577, 0.13323944807052612, 0.07374203205108643, 0.05740422010421753, 0.13908404111862183, 0.13651984930038452, 0.5836295485496521, 0.08863246440887451, 0.30761009454727173]\n",
            "step : 2829  ---  loss train : [0.0717352032661438, 0.08408445119857788, 0.15303468704223633, 0.43266475200653076, 0.13958752155303955, 0.10488557815551758, 0.06065303087234497, 0.18294429779052734, 0.24728387594223022, 0.1343216896057129]\n",
            "step : 2839  ---  loss train : [0.08484882116317749, 0.10261893272399902, 0.2007259726524353, 0.06267130374908447, 0.062330782413482666, 0.1337066888809204, 0.2804245352745056, 0.14563775062561035, 0.09409838914871216, 0.10895061492919922]\n",
            "step : 2849  ---  loss train : [0.15752428770065308, 0.058249056339263916, 0.564915657043457, 0.06580054759979248, 0.09437501430511475, 0.07947772741317749, 0.13700872659683228, 0.06125360727310181, 0.24649441242218018, 0.06153273582458496]\n",
            "step : 2859  ---  loss train : [0.15083682537078857, 0.10505378246307373, 0.2603967785835266, 0.18274271488189697, 0.13531601428985596, 0.05262422561645508, 0.4830830693244934, 0.0693168044090271, 0.1022193431854248, 0.3667683005332947]\n",
            "step : 2869  ---  loss train : [0.17503446340560913, 0.0995669960975647, 0.46089452505111694, 0.28361135721206665, 0.6400914192199707, 0.3085711598396301, 0.2663804888725281, 0.04351526498794556, 0.07169854640960693, 0.09760576486587524]\n",
            "step : 2879  ---  loss train : [0.08395308256149292, 0.20274275541305542, 0.1434565782546997, 0.45518070459365845, 0.26426082849502563, 0.16133427619934082, 0.24066340923309326, 0.05234163999557495, 0.21366500854492188, 0.0729634165763855]\n",
            "step : 2889  ---  loss train : [0.0509909987449646, 0.240922749042511, 0.2293161153793335, 0.15167641639709473, 0.08432668447494507, 0.1061086654663086, 0.24481511116027832, 0.4075874090194702, 0.16083818674087524, 0.08644533157348633]\n",
            "step : 2899  ---  loss train : [0.3897033929824829, 0.05183225870132446, 0.42709028720855713, 0.21212923526763916, 0.12716257572174072, 0.14953923225402832, 0.05914252996444702, 0.06281661987304688, 0.2904118299484253, 0.33679038286209106]\n",
            "step : 2909  ---  loss train : [0.3144610524177551, 0.243014395236969, 0.10756844282150269, 0.07632958889007568, 0.2028294801712036, 0.2136557698249817, 0.06382018327713013, 0.16523289680480957, 0.2743256092071533, 0.9996698498725891]\n",
            "step : 2919  ---  loss train : [0.12606823444366455, 0.11822068691253662, 0.09391999244689941, 0.2137686014175415, 0.2226485013961792, 0.27945876121520996, 0.15684223175048828, 0.2584373354911804, 0.14644396305084229, 0.12013876438140869]\n",
            "step : 2929  ---  loss train : [0.058332085609436035, 0.27950555086135864, 0.3414984941482544, 0.20921272039413452, 0.16943484544754028, 0.06550097465515137, 0.12943631410598755, 0.06476914882659912, 0.18195480108261108, 0.17987382411956787]\n",
            "step : 2939  ---  loss train : [0.35600894689559937, 0.30055946111679077, 0.12051445245742798, 0.22103774547576904, 0.10191720724105835, 0.23503750562667847, 0.3902444839477539, 0.10160684585571289, 0.08736079931259155, 0.2819492816925049]\n",
            "step : 2949  ---  loss train : [0.14492791891098022, 0.4136078357696533, 0.07760751247406006, 0.30515044927597046, 0.10853880643844604, 0.2071378231048584, 0.1675804853439331, 0.30363035202026367, 0.999474048614502, 0.08752179145812988]\n",
            "step : 2959  ---  loss train : [0.07857054471969604, 0.12229877710342407, 0.06973439455032349, 0.06419432163238525, 0.08734124898910522, 0.25863319635391235, 0.05964583158493042, 0.168005108833313, 0.058585166931152344, 0.07117897272109985]\n",
            "step : 2969  ---  loss train : [0.1068151593208313, 0.06511962413787842, 0.0846591591835022, 0.14321112632751465, 0.36247384548187256, 0.13404196500778198, 0.2178363800048828, 0.1792406439781189, 0.9996660947799683, 0.08701425790786743]\n",
            "step : 2979  ---  loss train : [0.0724145770072937, 0.14408409595489502, 0.10523843765258789, 0.1427357792854309, 0.3376210927963257, 0.07865273952484131, 0.10907226800918579, 0.11263471841812134, 0.2466496229171753, 0.0686413049697876]\n",
            "step : 2989  ---  loss train : [0.1758384108543396, 0.05794477462768555, 0.09157347679138184, 0.1474229097366333, 0.14289557933807373, 0.11414903402328491, 0.08163970708847046, 0.16671377420425415, 0.17114406824111938, 0.2055727243423462]\n",
            "step : 2999  ---  loss train : [0.30379587411880493, 0.1429898738861084, 0.08236640691757202, 0.20396405458450317, 0.069430410861969, 0.06991243362426758, 0.22561520338058472, 0.19299620389938354, 0.5066612958908081, 0.07245022058486938]\n",
            "step : 3009  ---  loss train : [0.18436157703399658, 0.1875448226928711, 0.13390278816223145, 0.20597171783447266, 0.15081381797790527, 0.16473346948623657, 0.1975582242012024, 0.1992897391319275, 0.20098835229873657, 0.12183225154876709]\n",
            "step : 3019  ---  loss train : [0.27298563718795776, 0.18193566799163818, 0.4204687476158142, 0.3450576663017273, 0.49413633346557617, 0.1518707275390625, 0.07143455743789673, 0.09762948751449585, 0.21944361925125122, 0.1020728349685669]\n",
            "step : 3029  ---  loss train : [0.23536700010299683, 0.13115078210830688, 0.07701492309570312, 0.08533257246017456, 0.19889366626739502, 0.11779463291168213, 0.20685482025146484, 0.0692548155784607, 0.06778943538665771, 0.2057827115058899]\n",
            "step : 3039  ---  loss train : [0.11153149604797363, 0.2107493281364441, 0.9996797442436218, 0.10247683525085449, 0.18233180046081543, 0.06678736209869385, 0.2259526252746582, 0.14539194107055664, 0.21900057792663574, 0.29267579317092896]\n",
            "step : 3049  ---  loss train : [0.42379796504974365, 0.3149687647819519, 0.23799175024032593, 0.37457579374313354, 0.1152869462966919, 0.48581838607788086, 0.1594332456588745, 0.17693299055099487, 0.18579500913619995, 0.1956610083580017]\n",
            "step : 3059  ---  loss train : [0.197331964969635, 0.15130150318145752, 0.08948439359664917, 0.2961997389793396, 0.22924602031707764, 0.16531336307525635, 0.18102526664733887, 0.19461417198181152, 0.12946993112564087, 0.10049688816070557]\n",
            "step : 3069  ---  loss train : [0.23081707954406738, 0.0743148922920227, 0.13037055730819702, 0.13862377405166626, 0.2788962125778198, 0.09060585498809814, 0.07418763637542725, 0.15973901748657227, 0.06523549556732178, 0.7611393928527832]\n",
            "step : 3079  ---  loss train : [0.10278594493865967, 0.08535027503967285, 0.15324324369430542, 0.17910277843475342, 0.08841222524642944, 0.14002716541290283, 0.11433058977127075, 0.10592973232269287, 0.09857833385467529, 0.06520289182662964]\n",
            "step : 3089  ---  loss train : [0.06966310739517212, 0.30161184072494507, 0.09664261341094971, 0.5145936012268066, 0.6998531818389893, 0.16906142234802246, 0.4488012194633484, 0.08806931972503662, 0.4239742159843445, 0.2926597595214844]\n",
            "step : 3099  ---  loss train : [0.18689841032028198, 0.16239959001541138, 0.17596817016601562, 0.1642879843711853, 0.16140246391296387, 0.45188844203948975, 0.10079652070999146, 0.23585855960845947, 0.28204989433288574, 0.5041729211807251]\n",
            "step : 3109  ---  loss train : [0.3859597444534302, 0.08976197242736816, 0.4408406615257263, 0.35269224643707275, 0.09465187788009644, 0.09029507637023926, 0.20986002683639526, 0.05537766218185425, 0.06211841106414795, 0.3786357045173645]\n",
            "step : 3119  ---  loss train : [0.3305284380912781, 0.15421444177627563, 0.2845458388328552, 0.1435878872871399, 0.1733044981956482, 0.19423353672027588, 0.21916282176971436, 0.09052479267120361, 0.22006875276565552, 0.0682634711265564]\n",
            "step : 3129  ---  loss train : [0.10926026105880737, 0.0898904800415039, 0.08002394437789917, 0.2672901749610901, 0.5488020181655884, 0.2099509835243225, 0.31871122121810913, 0.4737103581428528, 0.3559728264808655, 0.06074041128158569]\n",
            "step : 3139  ---  loss train : [0.4369851350784302, 0.05910944938659668, 0.2872224450111389, 0.13934385776519775, 0.5318958759307861, 0.03836715221405029, 0.11989861726760864, 0.0992078185081482, 0.40915799140930176, 0.16197901964187622]\n",
            "step : 3149  ---  loss train : [0.131150484085083, 0.26003092527389526, 0.19799268245697021, 0.1567521095275879, 0.1663455367088318, 0.11791437864303589, 0.2963036298751831, 0.17376947402954102, 0.21511709690093994, 0.4802241921424866]\n",
            "step : 3159  ---  loss train : [0.17483019828796387, 0.0522884726524353, 0.06680196523666382, 0.07740777730941772, 0.1385815143585205, 0.4104241132736206, 0.2179902195930481, 0.11414963006973267, 0.39943957328796387, 0.2064458727836609]\n",
            "step : 3169  ---  loss train : [0.1729670763015747, 0.14771968126296997, 0.1287255883216858, 0.16931617259979248, 0.06446939706802368, 0.10814589262008667, 0.18641018867492676, 0.06606823205947876, 0.18618685007095337, 0.18929040431976318]\n",
            "step : 3179  ---  loss train : [0.13727611303329468, 0.1494101881980896, 0.1953144073486328, 0.10547250509262085, 0.11874312162399292, 0.22432583570480347, 0.2286263108253479, 0.18838071823120117, 0.07460039854049683, 0.04244154691696167]\n",
            "step : 3189  ---  loss train : [0.14919066429138184, 0.08058017492294312, 0.06664520502090454, 0.4754427671432495, 0.18678820133209229, 0.16426771879196167, 0.13219547271728516, 0.08887273073196411, 0.15118014812469482, 0.148606538772583]\n",
            "step : 3199  ---  loss train : [0.2626253366470337, 0.1207696795463562, 0.18740534782409668, 0.5982989072799683, 0.13047468662261963, 0.4218924045562744, 0.3560318350791931, 0.30037105083465576, 0.12772828340530396, 0.15490305423736572]\n",
            "step : 3209  ---  loss train : [0.23745501041412354, 0.15005970001220703, 0.06912672519683838, 0.17241007089614868, 0.3416571021080017, 0.06953221559524536, 0.07268589735031128, 0.2163185477256775, 0.04964369535446167, 0.07536137104034424]\n",
            "step : 3219  ---  loss train : [0.12507843971252441, 0.1742001175880432, 0.13162356615066528, 0.12272834777832031, 0.11115723848342896, 0.13652265071868896, 0.11741113662719727, 0.13868683576583862, 0.16830778121948242, 0.2814462184906006]\n",
            "step : 3229  ---  loss train : [0.14634180068969727, 0.09559839963912964, 0.19099962711334229, 0.15532255172729492, 0.12116926908493042, 0.11446928977966309, 0.18417274951934814, 0.06329494714736938, 0.6144368648529053, 0.09198039770126343]\n",
            "step : 3239  ---  loss train : [0.1402372121810913, 0.08585375547409058, 0.2682568430900574, 0.19561141729354858, 0.06498396396636963, 0.1721632480621338, 0.2054857611656189, 0.23522478342056274, 0.09109216928482056, 0.18852317333221436]\n",
            "step : 3249  ---  loss train : [0.09211874008178711, 0.11923444271087646, 0.24964547157287598, 0.1207926869392395, 0.29897135496139526, 0.28787851333618164, 0.10477149486541748, 0.06061697006225586, 0.12622052431106567, 0.10427099466323853]\n",
            "step : 3259  ---  loss train : [0.19675713777542114, 0.10215872526168823, 0.1192062497138977, 0.13471263647079468, 0.13655012845993042, 0.09819912910461426, 0.11174172163009644, 0.23556303977966309, 0.12143278121948242, 0.38905709981918335]\n",
            "step : 3269  ---  loss train : [0.44107353687286377, 0.09165120124816895, 0.18029922246932983, 0.17390459775924683, 0.16631460189819336, 0.12394344806671143, 0.23245275020599365, 0.15107697248458862, 0.0730900764465332, 0.15219485759735107]\n",
            "step : 3279  ---  loss train : [0.8377482891082764, 0.13460534811019897, 0.11753988265991211, 0.10863423347473145, 0.15958553552627563, 0.0885058045387268, 0.21157234907150269, 0.13167113065719604, 0.3455490469932556, 0.10665613412857056]\n",
            "step : 3283  ---  loss train : [0.9997759461402893, 0.998845636844635, 0.9994412660598755, 0.4832586646080017, 0.5921361446380615, 0.9977515935897827, 0.32589972019195557, 0.32870763540267944, 0.2918018102645874, 0.98768150806427, 0.38224899768829346, 0.9993825554847717, 0.9993441700935364, 0.25234729051589966, 0.05354553461074829, 0.06564480066299438, 0.19593942165374756, 0.25852352380752563, 0.27357763051986694, 0.6153303384780884, 0.46102380752563477, 0.4051302671432495, 0.9982563257217407, 0.9986804723739624, 0.36840617656707764, 0.23046857118606567, 0.2163550853729248, 0.19508486986160278, 0.12708145380020142, 0.18432188034057617, 0.9834108948707581, 0.27738088369369507, 0.3035529851913452, 0.405229389667511, 0.3017401099205017, 0.9988704323768616, 0.9997506737709045, 0.5002795457839966, 0.4353026747703552, 0.40943896770477295, 0.3316049575805664, 0.12406295537948608, 0.13033878803253174, 0.17930680513381958, 0.2589244246482849, 0.3117002248764038, 0.9998914003372192, 0.9999414086341858, 0.9999407529830933, 0.9999129772186279, 0.9999072551727295, 0.999814510345459, 0.9998286962509155, 0.9785785675048828, 0.9998899102210999, 0.9998739361763, 0.9999211430549622, 0.9999492764472961, 0.9999503493309021, 0.9998485445976257, 0.42213690280914307, 0.13158607482910156, 0.11163502931594849, 0.18588906526565552, 0.32543933391571045, 0.9998290538787842, 0.9989656805992126, 0.2530324459075928, 0.9991914629936218, 0.9994332790374756, 0.9995793700218201, 0.999687910079956, 0.9996143579483032, 0.9988293647766113, 0.9975922107696533, 0.5421457886695862, 0.4386112689971924, 0.34616708755493164, 0.6379697918891907, 0.3596402406692505, 0.3817465901374817, 0.8802182674407959, 0.35134273767471313, 0.9980779886245728, 0.33530157804489136, 0.8396505117416382, 0.6462218761444092, 0.48498475551605225, 0.49120938777923584, 0.44659554958343506, 0.3619071841239929, 0.25956612825393677, 0.055658817291259766, 0.14327645301818848, 0.26136714220046997, 0.251675009727478, 0.24742603302001953, 0.26466333866119385, 0.24209332466125488, 0.2365126609802246, 0.263851523399353, 0.2185184359550476, 0.9988635182380676, 0.993084192276001, 0.370555579662323, 0.12289613485336304, 0.10630553960800171, 0.21423417329788208, 0.23012042045593262, 0.2772101163864136, 0.27031517028808594, 0.9956347346305847, 0.9379716515541077, 0.9971922039985657, 0.3169625997543335, 0.3378446102142334, 0.6043477058410645, 0.15578001737594604, 0.06175565719604492, 0.08723807334899902, 0.11214816570281982, 0.23778730630874634, 0.9745492935180664, 0.4064764976501465, 0.44753873348236084, 0.4387955665588379, 0.9958241581916809, 0.788878858089447, 0.3881682753562927, 0.3378903269767761, 0.368249773979187, 0.378623366355896, 0.9927327632904053, 0.36253100633621216, 0.32219380140304565, 0.321474552154541, 0.30904191732406616, 0.31571274995803833, 0.3129832148551941, 0.9981105923652649, 0.30102384090423584, 0.30172646045684814, 0.31068891286849976, 0.997893214225769, 0.9993780851364136, 0.999522864818573, 0.999275803565979, 0.99608314037323, 0.24799233675003052, 0.2475171685218811, 0.22668182849884033, 0.12567347288131714]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [04:27<1:00:54, 39.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 3283  ---  mean_dsc : 0.5579723003746169\n",
            "step : 3289  ---  loss train : [0.08328330516815186, 0.12288773059844971, 0.07489556074142456, 0.0598490834236145, 0.161699116230011, 0.1440960168838501, 0.5830332636833191, 0.10137432813644409, 0.29362720251083374, 0.07315021753311157]\n",
            "step : 3299  ---  loss train : [0.08041960000991821, 0.1307564377784729, 0.4192010164260864, 0.13212847709655762, 0.116005539894104, 0.059983789920806885, 0.2044568657875061, 0.2479567527770996, 0.14766591787338257, 0.07090926170349121]\n",
            "step : 3309  ---  loss train : [0.10431689023971558, 0.19175034761428833, 0.0722191333770752, 0.06041234731674194, 0.1348288655281067, 0.27004969120025635, 0.1443820595741272, 0.09612452983856201, 0.11440324783325195, 0.14789646863937378]\n",
            "step : 3319  ---  loss train : [0.05797767639160156, 0.3290168046951294, 0.06845170259475708, 0.09898632764816284, 0.07964944839477539, 0.11327362060546875, 0.06090545654296875, 0.21162623167037964, 0.049372315406799316, 0.09510636329650879]\n",
            "step : 3329  ---  loss train : [0.09654790163040161, 0.21657007932662964, 0.09681123495101929, 0.13140130043029785, 0.05877351760864258, 0.5712920427322388, 0.07102560997009277, 0.07586568593978882, 0.36620426177978516, 0.16267049312591553]\n",
            "step : 3339  ---  loss train : [0.10028088092803955, 0.4845271706581116, 0.31839466094970703, 0.6482061147689819, 0.14456409215927124, 0.227075457572937, 0.044056475162506104, 0.07274925708770752, 0.0832715630531311, 0.08012145757675171]\n",
            "step : 3349  ---  loss train : [0.19821298122406006, 0.1358625888824463, 0.5957809686660767, 0.2651771903038025, 0.11896425485610962, 0.20953184366226196, 0.052132606506347656, 0.21474790573120117, 0.07913351058959961, 0.04903519153594971]\n",
            "step : 3359  ---  loss train : [0.2220555543899536, 0.22564780712127686, 0.129316508769989, 0.07881766557693481, 0.07532960176467896, 0.28418833017349243, 0.5336902141571045, 0.15189754962921143, 0.09266799688339233, 0.3354957103729248]\n",
            "step : 3369  ---  loss train : [0.04963719844818115, 0.37197744846343994, 0.21342092752456665, 0.12172633409500122, 0.1255265474319458, 0.057758212089538574, 0.0582846999168396, 0.2725653648376465, 0.3006497025489807, 0.3111850619316101]\n",
            "step : 3379  ---  loss train : [0.22458767890930176, 0.10892850160598755, 0.07125610113143921, 0.1789955496788025, 0.19987237453460693, 0.06561952829360962, 0.2113783359527588, 0.3238879442214966, 0.9997913241386414, 0.12122535705566406]\n",
            "step : 3389  ---  loss train : [0.11748021841049194, 0.09411299228668213, 0.19892823696136475, 0.23230379819869995, 0.2830125689506531, 0.15054422616958618, 0.25722140073776245, 0.13029950857162476, 0.1208462119102478, 0.06444597244262695]\n",
            "step : 3399  ---  loss train : [0.2384500503540039, 0.3399425745010376, 0.20998620986938477, 0.1696212887763977, 0.0636281967163086, 0.12271887063980103, 0.07186007499694824, 0.17402929067611694, 0.18103265762329102, 0.34012311697006226]\n",
            "step : 3409  ---  loss train : [0.29589319229125977, 0.11654925346374512, 0.23100322484970093, 0.10296642780303955, 0.21164721250534058, 0.3584616780281067, 0.10664510726928711, 0.09183281660079956, 0.2576993703842163, 0.06350278854370117]\n",
            "step : 3419  ---  loss train : [0.4214116334915161, 0.09202194213867188, 0.32215434312820435, 0.10105264186859131, 0.19377940893173218, 0.16161280870437622, 0.32510584592819214, 0.9995324015617371, 0.0730317234992981, 0.0747479796409607]\n",
            "step : 3429  ---  loss train : [0.1279783844947815, 0.07102298736572266, 0.06352502107620239, 0.07754957675933838, 0.2548478841781616, 0.05817532539367676, 0.16614419221878052, 0.060877978801727295, 0.07219827175140381, 0.11112356185913086]\n",
            "step : 3439  ---  loss train : [0.061476826667785645, 0.0820624828338623, 0.14698714017868042, 0.3350710868835449, 0.12512773275375366, 0.1925458312034607, 0.17490875720977783, 0.9993264675140381, 0.08564412593841553, 0.07393360137939453]\n",
            "step : 3449  ---  loss train : [0.12143957614898682, 0.10249042510986328, 0.13411486148834229, 0.18712246417999268, 0.07712650299072266, 0.1144789457321167, 0.11320078372955322, 0.25245165824890137, 0.0665428638458252, 0.20319581031799316]\n",
            "step : 3459  ---  loss train : [0.05844396352767944, 0.0905771255493164, 0.14151525497436523, 0.1482190489768982, 0.11353421211242676, 0.08061039447784424, 0.16786473989486694, 0.17305481433868408, 0.2192019820213318, 0.2771470546722412]\n",
            "step : 3469  ---  loss train : [0.11995518207550049, 0.09005546569824219, 0.1695643663406372, 0.06655383110046387, 0.07465749979019165, 0.1909961700439453, 0.21807432174682617, 0.5001708269119263, 0.07948547601699829, 0.16641682386398315]\n",
            "step : 3479  ---  loss train : [0.21057021617889404, 0.14833807945251465, 0.19723397493362427, 0.15422147512435913, 0.17128825187683105, 0.21399229764938354, 0.19826841354370117, 0.21091699600219727, 0.11516779661178589, 0.26203489303588867]\n",
            "step : 3489  ---  loss train : [0.1661761999130249, 0.43517279624938965, 0.3431411385536194, 0.46218371391296387, 0.151938796043396, 0.07218468189239502, 0.10501259565353394, 0.1950628161430359, 0.07887822389602661, 0.2874460220336914]\n",
            "step : 3499  ---  loss train : [0.1289137601852417, 0.0728144645690918, 0.06879013776779175, 0.18418878316879272, 0.10377401113510132, 0.17248469591140747, 0.0669020414352417, 0.06652486324310303, 0.12268394231796265, 0.1139441728591919]\n",
            "step : 3509  ---  loss train : [0.1590120792388916, 0.9997338652610779, 0.10187852382659912, 0.1876031756401062, 0.059648215770721436, 0.22930580377578735, 0.12850236892700195, 0.22521430253982544, 0.2277204990386963, 0.31393611431121826]\n",
            "step : 3519  ---  loss train : [0.19183635711669922, 0.2070784568786621, 0.2881004214286804, 0.14493417739868164, 0.414933443069458, 0.14671683311462402, 0.16093206405639648, 0.18677914142608643, 0.20002824068069458, 0.19369304180145264]\n",
            "step : 3529  ---  loss train : [0.15690141916275024, 0.0633002519607544, 0.33251500129699707, 0.19258826971054077, 0.1623680591583252, 0.12504369020462036, 0.18689334392547607, 0.12476778030395508, 0.10185414552688599, 0.15953749418258667]\n",
            "step : 3539  ---  loss train : [0.07406938076019287, 0.13655829429626465, 0.10499370098114014, 0.24261188507080078, 0.08794033527374268, 0.07219493389129639, 0.16239041090011597, 0.06110566854476929, 0.6740314960479736, 0.1063963770866394]\n",
            "step : 3549  ---  loss train : [0.07049715518951416, 0.1802922487258911, 0.17377710342407227, 0.07463604211807251, 0.10800731182098389, 0.1346387267112732, 0.10870891809463501, 0.10825169086456299, 0.06763434410095215, 0.06245923042297363]\n",
            "step : 3559  ---  loss train : [0.2522541880607605, 0.0894390344619751, 0.35302406549453735, 0.6558318734169006, 0.15099501609802246, 0.44703739881515503, 0.08173197507858276, 0.31505054235458374, 0.24522441625595093, 0.1849428415298462]\n",
            "step : 3569  ---  loss train : [0.11788856983184814, 0.1783631443977356, 0.09655153751373291, 0.1376643180847168, 0.28888756036758423, 0.06894350051879883, 0.1253240704536438, 0.12163323163986206, 0.5765836238861084, 0.4244620203971863]\n",
            "step : 3579  ---  loss train : [0.11188852787017822, 0.23601889610290527, 0.3455316424369812, 0.06248748302459717, 0.10486215353012085, 0.15399813652038574, 0.04184985160827637, 0.06143200397491455, 0.3621387481689453, 0.1750398874282837]\n",
            "step : 3589  ---  loss train : [0.22025161981582642, 0.2549031376838684, 0.11119711399078369, 0.17724257707595825, 0.19424021244049072, 0.2203737497329712, 0.06674307584762573, 0.23019325733184814, 0.06075417995452881, 0.11244887113571167]\n",
            "step : 3599  ---  loss train : [0.090293288230896, 0.06153750419616699, 0.22805845737457275, 0.5541146993637085, 0.18914794921875, 0.19960087537765503, 0.38132721185684204, 0.3555256128311157, 0.06103026866912842, 0.5202850103378296]\n",
            "step : 3609  ---  loss train : [0.0658109188079834, 0.2617284059524536, 0.09025013446807861, 0.4708683490753174, 0.03619331121444702, 0.09859728813171387, 0.09507352113723755, 0.40990149974823, 0.16714900732040405, 0.1161646842956543]\n",
            "step : 3619  ---  loss train : [0.3127366304397583, 0.21154654026031494, 0.14330726861953735, 0.16759395599365234, 0.11483502388000488, 0.2145947813987732, 0.14158809185028076, 0.08533912897109985, 0.48813146352767944, 0.16043108701705933]\n",
            "step : 3629  ---  loss train : [0.04455763101577759, 0.065421462059021, 0.07322883605957031, 0.12673842906951904, 0.3877202272415161, 0.1396762728691101, 0.10330897569656372, 0.2902451753616333, 0.165429949760437, 0.14054471254348755]\n",
            "step : 3639  ---  loss train : [0.06791764497756958, 0.11115044355392456, 0.1560184359550476, 0.0542941689491272, 0.1163756251335144, 0.2021995186805725, 0.07259160280227661, 0.1851850152015686, 0.17713004350662231, 0.1342269778251648]\n",
            "step : 3649  ---  loss train : [0.1459466814994812, 0.13483643531799316, 0.1251000165939331, 0.13655555248260498, 0.19346272945404053, 0.18982839584350586, 0.18437981605529785, 0.07145899534225464, 0.046229660511016846, 0.1569688320159912]\n",
            "step : 3659  ---  loss train : [0.11884242296218872, 0.06434160470962524, 0.39374983310699463, 0.24966323375701904, 0.08861404657363892, 0.09628421068191528, 0.08620935678482056, 0.11931389570236206, 0.1407567858695984, 0.44498682022094727]\n",
            "step : 3669  ---  loss train : [0.11875402927398682, 0.18535476922988892, 0.6589771509170532, 0.09755659103393555, 0.4365004897117615, 0.3805640935897827, 0.2558818459510803, 0.14731073379516602, 0.16033732891082764, 0.21955066919326782]\n",
            "step : 3679  ---  loss train : [0.08992570638656616, 0.08946633338928223, 0.1798047423362732, 0.35046863555908203, 0.08903622627258301, 0.0687209963798523, 0.2096896767616272, 0.05301165580749512, 0.07755589485168457, 0.12240421772003174]\n",
            "step : 3689  ---  loss train : [0.23596292734146118, 0.1210753321647644, 0.12604683637619019, 0.09953337907791138, 0.13033634424209595, 0.11502343416213989, 0.14407259225845337, 0.15945440530776978, 0.27997565269470215, 0.15692061185836792]\n",
            "step : 3699  ---  loss train : [0.10892730951309204, 0.2022191882133484, 0.18278032541275024, 0.12460929155349731, 0.09920656681060791, 0.18484067916870117, 0.05623936653137207, 0.6824125051498413, 0.09383237361907959, 0.1302506923675537]\n",
            "step : 3709  ---  loss train : [0.06413668394088745, 0.22943294048309326, 0.17212039232254028, 0.05853325128555298, 0.1370747685432434, 0.22165459394454956, 0.14736723899841309, 0.08459252119064331, 0.23689347505569458, 0.0853312611579895]\n",
            "step : 3719  ---  loss train : [0.10611152648925781, 0.2839584946632385, 0.10781234502792358, 0.34176701307296753, 0.3141711354255676, 0.1028546690940857, 0.05731087923049927, 0.08044564723968506, 0.10834354162216187, 0.16689664125442505]\n",
            "step : 3729  ---  loss train : [0.10107570886611938, 0.10954886674880981, 0.07552671432495117, 0.14234250783920288, 0.09574490785598755, 0.09781485795974731, 0.21580594778060913, 0.11102896928787231, 0.336067259311676, 0.4497458338737488]\n",
            "step : 3739  ---  loss train : [0.07283622026443481, 0.1865902543067932, 0.1231849193572998, 0.1389402151107788, 0.10735267400741577, 0.20966213941574097, 0.14307910203933716, 0.06194186210632324, 0.13997578620910645, 0.7596461772918701]\n",
            "step : 3749  ---  loss train : [0.1342296004295349, 0.1132459044456482, 0.09447365999221802, 0.15063625574111938, 0.08619004487991333, 0.186720609664917, 0.09199613332748413, 0.31500017642974854, 0.12168020009994507, 0.07745778560638428]\n",
            "step : 3752  ---  loss train : [0.9998190402984619, 0.999121904373169, 0.9995049834251404, 0.3351730704307556, 0.5381247997283936, 0.4504861831665039, 0.7832900285720825, 0.1821509599685669, 0.1395229697227478, 0.9962010383605957, 0.15797698497772217, 0.9960489869117737, 0.998932957649231, 0.25160032510757446, 0.04959326982498169, 0.06280452013015747, 0.18951255083084106, 0.1357136368751526, 0.09489655494689941, 0.11743634939193726, 0.1266711950302124, 0.12316560745239258, 0.9964439272880554, 0.996483325958252, 0.3082205057144165, 0.16349279880523682, 0.16474944353103638, 0.1642410159111023, 0.11007481813430786, 0.1601998209953308, 0.9833013415336609, 0.14291852712631226, 0.1332072615623474, 0.12822794914245605, 0.10975128412246704, 0.9974880814552307, 0.9997914433479309, 0.9968954920768738, 0.129497230052948, 0.451998233795166, 0.38614314794540405, 0.13705646991729736, 0.1408039927482605, 0.20490342378616333, 0.2580525875091553, 0.28235936164855957, 0.999873697757721, 0.9999272227287292, 0.999921977519989, 0.9999033808708191, 0.9998931884765625, 0.999790370464325, 0.9998162984848022, 0.9531468152999878, 0.9998825192451477, 0.999862790107727, 0.9999019503593445, 0.9999338388442993, 0.9999384880065918, 0.9998376369476318, 0.9936608076095581, 0.19719749689102173, 0.09124410152435303, 0.16369092464447021, 0.3100394606590271, 0.9998312592506409, 0.999150276184082, 0.12455469369888306, 0.9984986186027527, 0.999308705329895, 0.9995337128639221, 0.9996244311332703, 0.9995226263999939, 0.9979528188705444, 0.9660344123840332, 0.14550405740737915, 0.1507967710494995, 0.14773184061050415, 0.1830517053604126, 0.16606557369232178, 0.186897873878479, 0.2237953543663025, 0.2264285683631897, 0.990362286567688, 0.3039973974227905, 0.7886742353439331, 0.66251140832901, 0.5406660437583923, 0.5404981374740601, 0.49215948581695557, 0.4017946124076843, 0.2940531373023987, 0.048906803131103516, 0.1302204132080078, 0.1454753279685974, 0.12149077653884888, 0.10968649387359619, 0.11403828859329224, 0.09730863571166992, 0.0988038182258606, 0.10032492876052856, 0.10405600070953369, 0.9986125826835632, 0.957395076751709, 0.3609614372253418, 0.12087839841842651, 0.10665786266326904, 0.21415340900421143, 0.33840328454971313, 0.23098623752593994, 0.10145735740661621, 0.9959109425544739, 0.8249176740646362, 0.9803970456123352, 0.9791330099105835, 0.21360039710998535, 0.6171860694885254, 0.1500651240348816, 0.05773895978927612, 0.08193415403366089, 0.10979592800140381, 0.22686874866485596, 0.9744537472724915, 0.1264134645462036, 0.13938719034194946, 0.1300727128982544, 0.15595072507858276, 0.1442592740058899, 0.134016215801239, 0.12741899490356445, 0.14579790830612183, 0.16212153434753418, 0.9878696799278259, 0.1827387809753418, 0.1843506097793579, 0.1732909083366394, 0.15599608421325684, 0.8801389336585999, 0.9965137839317322, 0.9989780187606812, 0.145693838596344, 0.14874839782714844, 0.14681440591812134, 0.9979184865951538, 0.9993943572044373, 0.9995341300964355, 0.9992774724960327, 0.9960890412330627, 0.100350022315979, 0.09480535984039307, 0.0965278148651123, 0.0496024489402771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [05:05<59:43, 38.95s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 3752  ---  mean_dsc : 0.5613618919696173\n",
            "step : 3759  ---  loss train : [0.10713213682174683, 0.07176685333251953, 0.08138704299926758, 0.12791234254837036, 0.16549479961395264, 0.5983749628067017, 0.1192319393157959, 0.2841978073120117, 0.07613807916641235, 0.06643480062484741]\n",
            "step : 3769  ---  loss train : [0.12417727708816528, 0.3988167643547058, 0.1276523470878601, 0.13867700099945068, 0.06101316213607788, 0.2215442657470703, 0.24194860458374023, 0.1275271773338318, 0.06237834692001343, 0.13229155540466309]\n",
            "step : 3779  ---  loss train : [0.25332480669021606, 0.06869077682495117, 0.058402299880981445, 0.12955254316329956, 0.3418431878089905, 0.14629876613616943, 0.09644860029220581, 0.10022860765457153, 0.15146994590759277, 0.05525892972946167]\n",
            "step : 3789  ---  loss train : [0.4263172149658203, 0.07557672262191772, 0.08744120597839355, 0.0777100920677185, 0.13722705841064453, 0.07597017288208008, 0.27036166191101074, 0.06720763444900513, 0.29798221588134766, 0.10445308685302734]\n",
            "step : 3799  ---  loss train : [0.4797895550727844, 0.18878358602523804, 0.14144963026046753, 0.06471765041351318, 0.4734962582588196, 0.11354774236679077, 0.09193229675292969, 0.42523443698883057, 0.1789965033531189, 0.10471934080123901]\n",
            "step : 3809  ---  loss train : [0.46601229906082153, 0.33731675148010254, 0.6467127203941345, 0.19919371604919434, 0.1904645562171936, 0.047496259212493896, 0.08355063199996948, 0.08330607414245605, 0.10596674680709839, 0.20321571826934814]\n",
            "step : 3819  ---  loss train : [0.15494322776794434, 0.453738272190094, 0.2645535469055176, 0.15460175275802612, 0.18639260530471802, 0.05646997690200806, 0.24059009552001953, 0.07969039678573608, 0.050346195697784424, 0.16976630687713623]\n",
            "step : 3829  ---  loss train : [0.21196144819259644, 0.132080078125, 0.0762593150138855, 0.10461264848709106, 0.14631754159927368, 0.4081704020500183, 0.15684038400650024, 0.12733614444732666, 0.3617647886276245, 0.05054265260696411]\n",
            "step : 3839  ---  loss train : [0.33429914712905884, 0.19320762157440186, 0.11631125211715698, 0.12171220779418945, 0.060991525650024414, 0.05715209245681763, 0.2519628405570984, 0.2770487070083618, 0.28097355365753174, 0.20544731616973877]\n",
            "step : 3849  ---  loss train : [0.10770213603973389, 0.06789618730545044, 0.18351984024047852, 0.19392889738082886, 0.06475931406021118, 0.17432951927185059, 0.24257206916809082, 0.9996517300605774, 0.11564987897872925, 0.11988818645477295]\n",
            "step : 3859  ---  loss train : [0.09483587741851807, 0.17553967237472534, 0.23292165994644165, 0.2805253267288208, 0.1496189832687378, 0.25965309143066406, 0.1039465069770813, 0.10831725597381592, 0.059055984020233154, 0.25021010637283325]\n",
            "step : 3869  ---  loss train : [0.34086745977401733, 0.20996177196502686, 0.1763513684272766, 0.05246073007583618, 0.1285189986228943, 0.08603650331497192, 0.2120828628540039, 0.15867799520492554, 0.3966846466064453, 0.3127133846282959]\n",
            "step : 3879  ---  loss train : [0.11746495962142944, 0.22896844148635864, 0.100616455078125, 0.2510415315628052, 0.37564826011657715, 0.09994012117385864, 0.10547256469726562, 0.32241159677505493, 0.08076643943786621, 0.4451568126678467]\n",
            "step : 3889  ---  loss train : [0.10064643621444702, 0.33498048782348633, 0.10179907083511353, 0.20794963836669922, 0.2195163369178772, 0.31754398345947266, 0.9995569586753845, 0.07243496179580688, 0.08399158716201782, 0.12686610221862793]\n",
            "step : 3899  ---  loss train : [0.07809627056121826, 0.06477344036102295, 0.07863563299179077, 0.26085489988327026, 0.05481994152069092, 0.1644383668899536, 0.05904430150985718, 0.064552903175354, 0.13685238361358643, 0.056288063526153564]\n",
            "step : 3909  ---  loss train : [0.07983160018920898, 0.14676231145858765, 0.3246697783470154, 0.11520862579345703, 0.1742728352546692, 0.18239045143127441, 0.9997444152832031, 0.07879996299743652, 0.0659368634223938, 0.11399251222610474]\n",
            "step : 3919  ---  loss train : [0.08965259790420532, 0.1455872654914856, 0.14803588390350342, 0.07047140598297119, 0.12000197172164917, 0.10745066404342651, 0.23754912614822388, 0.06597292423248291, 0.2131119966506958, 0.06142842769622803]\n",
            "step : 3929  ---  loss train : [0.08688020706176758, 0.14622092247009277, 0.14264285564422607, 0.11764460802078247, 0.08286559581756592, 0.16547399759292603, 0.165216326713562, 0.2066516876220703, 0.3048124313354492, 0.10790455341339111]\n",
            "step : 3939  ---  loss train : [0.08669888973236084, 0.1592494249343872, 0.07290381193161011, 0.07248032093048096, 0.18140965700149536, 0.18289196491241455, 0.4852551817893982, 0.0670425295829773, 0.16825485229492188, 0.22863143682479858]\n",
            "step : 3949  ---  loss train : [0.138283371925354, 0.21087747812271118, 0.151528000831604, 0.15515536069869995, 0.20001226663589478, 0.19704687595367432, 0.20461338758468628, 0.10449391603469849, 0.2596689462661743, 0.1765410304069519]\n",
            "step : 3959  ---  loss train : [0.4219468832015991, 0.34860992431640625, 0.4250915050506592, 0.1501392126083374, 0.07214778661727905, 0.11866235733032227, 0.22820299863815308, 0.07197445631027222, 0.23212003707885742, 0.12807142734527588]\n",
            "step : 3969  ---  loss train : [0.07563000917434692, 0.06253618001937866, 0.1835615634918213, 0.11021196842193604, 0.18569868803024292, 0.07098108530044556, 0.0687946081161499, 0.09961366653442383, 0.11531370878219604, 0.11731338500976562]\n",
            "step : 3979  ---  loss train : [0.9997066259384155, 0.1176983118057251, 0.1889338493347168, 0.05787605047225952, 0.1934722661972046, 0.3327525854110718, 0.21465063095092773, 0.22400689125061035, 0.26926523447036743, 0.23031574487686157]\n",
            "step : 3989  ---  loss train : [0.21138936281204224, 0.31154346466064453, 0.11961209774017334, 0.3118501901626587, 0.15567147731781006, 0.146517813205719, 0.1861180067062378, 0.1724345088005066, 0.19476544857025146, 0.15087038278579712]\n",
            "step : 3999  ---  loss train : [0.08499771356582642, 0.2725387215614319, 0.20677638053894043, 0.1572662591934204, 0.14344018697738647, 0.17915278673171997, 0.1155962347984314, 0.10242962837219238, 0.1944805383682251, 0.07378339767456055]\n",
            "step : 4009  ---  loss train : [0.13442683219909668, 0.10361593961715698, 0.19051772356033325, 0.09027087688446045, 0.07404005527496338, 0.16004008054733276, 0.06508499383926392, 0.6692768335342407, 0.10493409633636475, 0.07080191373825073]\n",
            "step : 4019  ---  loss train : [0.1794116497039795, 0.1754104495048523, 0.07554280757904053, 0.13575470447540283, 0.13331234455108643, 0.10772478580474854, 0.12157970666885376, 0.06340861320495605, 0.058535099029541016, 0.2487056851387024]\n",
            "step : 4029  ---  loss train : [0.07988017797470093, 0.30307137966156006, 0.672041654586792, 0.11170375347137451, 0.4560108780860901, 0.09361439943313599, 0.2941582202911377, 0.21071720123291016, 0.17196375131607056, 0.06956678628921509]\n",
            "step : 4039  ---  loss train : [0.19893872737884521, 0.04338806867599487, 0.14444929361343384, 0.22962915897369385, 0.07046008110046387, 0.132543683052063, 0.12088435888290405, 0.6309036612510681, 0.43358099460601807, 0.13608700037002563]\n",
            "step : 4049  ---  loss train : [0.20299583673477173, 0.3307853937149048, 0.06115370988845825, 0.09780853986740112, 0.08583563566207886, 0.039333462715148926, 0.06535112857818604, 0.3335980772972107, 0.12121760845184326, 0.15367931127548218]\n",
            "step : 4059  ---  loss train : [0.4034186005592346, 0.1222953200340271, 0.15759676694869995, 0.2080157995223999, 0.27279627323150635, 0.05409663915634155, 0.23502427339553833, 0.05240488052368164, 0.1302226185798645, 0.08131915330886841]\n",
            "step : 4069  ---  loss train : [0.06608474254608154, 0.16346949338912964, 0.5277733206748962, 0.19296956062316895, 0.28732746839523315, 0.3598073720932007, 0.35569268465042114, 0.07090342044830322, 0.370478093624115, 0.06352192163467407]\n",
            "step : 4079  ---  loss train : [0.26839500665664673, 0.06603741645812988, 0.49128276109695435, 0.03734463453292847, 0.1159820556640625, 0.09590882062911987, 0.42817968130111694, 0.1558070182800293, 0.11639368534088135, 0.23035472631454468]\n",
            "step : 4089  ---  loss train : [0.24015170335769653, 0.15060573816299438, 0.1702151894569397, 0.11332601308822632, 0.22064298391342163, 0.10912013053894043, 0.0943555235862732, 0.48600995540618896, 0.17486560344696045, 0.04100614786148071]\n",
            "step : 4099  ---  loss train : [0.06346559524536133, 0.07838845252990723, 0.12008273601531982, 0.3627590537071228, 0.1868385672569275, 0.11726731061935425, 0.37090784311294556, 0.17079269886016846, 0.13375520706176758, 0.07668954133987427]\n",
            "step : 4109  ---  loss train : [0.10567396879196167, 0.1427350640296936, 0.04938012361526489, 0.10913074016571045, 0.190684974193573, 0.06746315956115723, 0.1704736351966858, 0.16435688734054565, 0.1354292631149292, 0.09082096815109253]\n",
            "step : 4119  ---  loss train : [0.19149845838546753, 0.1079067587852478, 0.13096433877944946, 0.16258031129837036, 0.18779808282852173, 0.1982138752937317, 0.06923556327819824, 0.03738605976104736, 0.13627910614013672, 0.09415566921234131]\n",
            "step : 4129  ---  loss train : [0.08214610815048218, 0.46729129552841187, 0.17207348346710205, 0.08716052770614624, 0.1168861985206604, 0.0948745608329773, 0.13339102268218994, 0.1490533947944641, 0.34537196159362793, 0.1197546124458313]\n",
            "step : 4139  ---  loss train : [0.20673298835754395, 0.6013932228088379, 0.08597660064697266, 0.33679062128067017, 0.35835951566696167, 0.3462490439414978, 0.11865788698196411, 0.15522021055221558, 0.24891036748886108, 0.10123234987258911]\n",
            "step : 4149  ---  loss train : [0.051706790924072266, 0.17441421747207642, 0.32837843894958496, 0.061462223529815674, 0.07221460342407227, 0.1986910104751587, 0.04935789108276367, 0.07143443822860718, 0.17974889278411865, 0.16820365190505981]\n",
            "step : 4159  ---  loss train : [0.11160653829574585, 0.11574852466583252, 0.11469930410385132, 0.11585533618927002, 0.106548011302948, 0.14004403352737427, 0.14639288187026978, 0.2318023443222046, 0.14862769842147827, 0.09206938743591309]\n",
            "step : 4169  ---  loss train : [0.19976818561553955, 0.14237242937088013, 0.12799960374832153, 0.11534160375595093, 0.19695895910263062, 0.06724280118942261, 0.5572590231895447, 0.09147471189498901, 0.13801568746566772, 0.08339178562164307]\n",
            "step : 4179  ---  loss train : [0.274486780166626, 0.2112278938293457, 0.06280869245529175, 0.15838325023651123, 0.2271949052810669, 0.16592055559158325, 0.09119540452957153, 0.13541549444198608, 0.09949588775634766, 0.12048447132110596]\n",
            "step : 4189  ---  loss train : [0.2566852569580078, 0.11542600393295288, 0.3030129075050354, 0.30999118089675903, 0.10513037443161011, 0.058349668979644775, 0.07402008771896362, 0.09538984298706055, 0.12867337465286255, 0.08432215452194214]\n",
            "step : 4199  ---  loss train : [0.11254125833511353, 0.0882648229598999, 0.1282169222831726, 0.09996867179870605, 0.10733044147491455, 0.16825520992279053, 0.07918643951416016, 0.32583099603652954, 0.4402724504470825, 0.10938382148742676]\n",
            "step : 4209  ---  loss train : [0.24424034357070923, 0.13440650701522827, 0.06350821256637573, 0.07464176416397095, 0.1645466685295105, 0.0958244800567627, 0.11530232429504395, 0.14628100395202637, 0.6603326201438904, 0.07923096418380737]\n",
            "step : 4219  ---  loss train : [0.10645300149917603, 0.070712149143219, 0.15260744094848633, 0.07788634300231934, 0.16683554649353027, 0.05875617265701294, 0.26627838611602783, 0.1526716947555542, 0.07545644044876099, 0.12768739461898804]\n",
            "step : 4221  ---  loss train : [0.999857485294342, 0.9987063407897949, 0.9996166229248047, 0.30762922763824463, 0.4593612551689148, 0.605819821357727, 0.9974539875984192, 0.5828476548194885, 0.29154592752456665, 0.9975342154502869, 0.4980437755584717, 0.998835563659668, 0.9990995526313782, 0.2064112424850464, 0.0530925989151001, 0.07167446613311768, 0.14793676137924194, 0.2073851227760315, 0.10982310771942139, 0.9416464567184448, 0.9839328527450562, 0.9719228744506836, 0.9978541731834412, 0.9982630014419556, 0.34177857637405396, 0.22249877452850342, 0.21167761087417603, 0.18698877096176147, 0.12751662731170654, 0.17996633052825928, 0.9833889007568359, 0.21906912326812744, 0.2021806240081787, 0.20216530561447144, 0.995852530002594, 0.9993597269058228, 0.9998741149902344, 0.9986125826835632, 0.9969591498374939, 0.9888004064559937, 0.2685123085975647, 0.12325531244277954, 0.1326637864112854, 0.1626359224319458, 0.27358657121658325, 0.3316500782966614, 0.9998941421508789, 0.9999451041221619, 0.9999469518661499, 0.9999157786369324, 0.9999006390571594, 0.9998087286949158, 0.9998440742492676, 0.9036868810653687, 0.9998842477798462, 0.9998488426208496, 0.999897837638855, 0.999917209148407, 0.9999199509620667, 0.9998164176940918, 0.9978770613670349, 0.31337863206863403, 0.11386734247207642, 0.18653005361557007, 0.3382824659347534, 0.9998508095741272, 0.9993380308151245, 0.2834942936897278, 0.9994190335273743, 0.9994773864746094, 0.9995935559272766, 0.9996472001075745, 0.9995486736297607, 0.9970035552978516, 0.9638801217079163, 0.9860517382621765, 0.9336621165275574, 0.47851693630218506, 0.9674979448318481, 0.37621206045150757, 0.43282753229141235, 0.9813495874404907, 0.3216162919998169, 0.9555171728134155, 0.2219005823135376, 0.6411253213882446, 0.48373210430145264, 0.42282623052597046, 0.4317653179168701, 0.3969078063964844, 0.25663065910339355, 0.09001117944717407, 0.07959455251693726, 0.14519768953323364, 0.21820390224456787, 0.1969422698020935, 0.19570064544677734, 0.21521174907684326, 0.18082350492477417, 0.18430227041244507, 0.9451218247413635, 0.10774421691894531, 0.99739670753479, 0.9306582808494568, 0.42297887802124023, 0.1672607660293579, 0.14422237873077393, 0.2583714723587036, 0.4810430407524109, 0.9785050749778748, 0.22561365365982056, 0.9969033002853394, 0.9987756609916687, 0.9967703819274902, 0.9982181787490845, 0.9914366006851196, 0.6408267021179199, 0.16058820486068726, 0.05764895677566528, 0.08829283714294434, 0.12458211183547974, 0.25896382331848145, 0.9745645523071289, 0.8588343858718872, 0.9924399256706238, 0.9927650690078735, 0.9969406127929688, 0.9962114095687866, 0.975776731967926, 0.6267547607421875, 0.9082974791526794, 0.8810389041900635, 0.9976375699043274, 0.9353216290473938, 0.3007938861846924, 0.37944769859313965, 0.2977689504623413, 0.34643739461898804, 0.3724159002304077, 0.32699131965637207, 0.2430780529975891, 0.24331969022750854, 0.2508584260940552, 0.9975786209106445, 0.9993631839752197, 0.999525249004364, 0.9992229342460632, 0.9960501790046692, 0.1723853349685669, 0.10187035799026489, 0.08308535814285278, 0.0404241681098938]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [05:44<58:52, 38.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 4221  ---  mean_dsc : 0.562417703205021\n",
            "step : 4229  ---  loss train : [0.07980531454086304, 0.07715809345245361, 0.15331357717514038, 0.15619945526123047, 0.6141111850738525, 0.10715579986572266, 0.34865802526474, 0.07143527269363403, 0.06934010982513428, 0.13727563619613647]\n",
            "step : 4239  ---  loss train : [0.3768531084060669, 0.13327956199645996, 0.10989606380462646, 0.05445301532745361, 0.16757291555404663, 0.24261802434921265, 0.12431806325912476, 0.06378263235092163, 0.0997040867805481, 0.2080489993095398]\n",
            "step : 4249  ---  loss train : [0.06059688329696655, 0.05557382106781006, 0.13551479578018188, 0.27878105640411377, 0.14640694856643677, 0.10404950380325317, 0.056066691875457764, 0.14201492071151733, 0.05782032012939453, 0.3919954299926758]\n",
            "step : 4259  ---  loss train : [0.06684446334838867, 0.09147608280181885, 0.07229936122894287, 0.12170076370239258, 0.06471043825149536, 0.211952805519104, 0.05367612838745117, 0.14662379026412964, 0.10268312692642212, 0.288119375705719]\n",
            "step : 4269  ---  loss train : [0.12551569938659668, 0.07926034927368164, 0.035912394523620605, 0.4932088255882263, 0.07545959949493408, 0.05456244945526123, 0.35960477590560913, 0.17623668909072876, 0.10441291332244873, 0.5120654106140137]\n",
            "step : 4279  ---  loss train : [0.3101889491081238, 0.6805018782615662, 0.16173136234283447, 0.15496772527694702, 0.042055368423461914, 0.07918977737426758, 0.0885273814201355, 0.08441019058227539, 0.09441810846328735, 0.13624876737594604]\n",
            "step : 4289  ---  loss train : [0.468201220035553, 0.26721155643463135, 0.1189870834350586, 0.20369410514831543, 0.03959941864013672, 0.194135844707489, 0.062249064445495605, 0.047606587409973145, 0.15078264474868774, 0.18512791395187378]\n",
            "step : 4299  ---  loss train : [0.141335129737854, 0.06253862380981445, 0.10456758737564087, 0.13747835159301758, 0.29912668466567993, 0.15810292959213257, 0.10062646865844727, 0.3220664858818054, 0.0560002326965332, 0.32522374391555786]\n",
            "step : 4309  ---  loss train : [0.1996936798095703, 0.06606274843215942, 0.07200455665588379, 0.058685362339019775, 0.056202828884124756, 0.2360985279083252, 0.23614215850830078, 0.28378117084503174, 0.1592808961868286, 0.10974043607711792]\n",
            "step : 4319  ---  loss train : [0.08058410882949829, 0.17826193571090698, 0.2094486951828003, 0.06140172481536865, 0.17458128929138184, 0.26588648557662964, 0.7844260931015015, 0.12185686826705933, 0.1346684694290161, 0.09740138053894043]\n",
            "step : 4329  ---  loss train : [0.17566460371017456, 0.2336781620979309, 0.23133444786071777, 0.29875534772872925, 0.4001307487487793, 0.17581921815872192, 0.2506181597709656, 0.17783665657043457, 0.31282365322113037, 0.5290634632110596]\n",
            "step : 4339  ---  loss train : [0.22210806608200073, 0.3475933074951172, 0.059781789779663086, 0.14926356077194214, 0.0734521746635437, 0.214275062084198, 0.15941810607910156, 0.4185851812362671, 0.31168198585510254, 0.23469632863998413]\n",
            "step : 4349  ---  loss train : [0.38814258575439453, 0.12385469675064087, 0.21090418100357056, 0.40056151151657104, 0.14060157537460327, 0.11209869384765625, 0.41692429780960083, 0.07440799474716187, 0.45980578660964966, 0.11200779676437378]\n",
            "step : 4359  ---  loss train : [0.2928684949874878, 0.11579567193984985, 0.222881019115448, 0.13441872596740723, 0.5050147771835327, 0.999756395816803, 0.07584255933761597, 0.057810306549072266, 0.14043080806732178, 0.09029781818389893]\n",
            "step : 4369  ---  loss train : [0.06686967611312866, 0.0824621319770813, 0.25106847286224365, 0.07003521919250488, 0.1593131422996521, 0.059036970138549805, 0.05876576900482178, 0.12755948305130005, 0.048280417919158936, 0.07822555303573608]\n",
            "step : 4379  ---  loss train : [0.15653836727142334, 0.35977864265441895, 0.11813634634017944, 0.2191104292869568, 0.2097131609916687, 0.9997327923774719, 0.08041727542877197, 0.07374048233032227, 0.13058286905288696, 0.0912022590637207]\n",
            "step : 4389  ---  loss train : [0.1532188057899475, 0.1832544207572937, 0.07243525981903076, 0.11386281251907349, 0.11370575428009033, 0.2535521984100342, 0.0676223635673523, 0.19565975666046143, 0.052694082260131836, 0.09152591228485107]\n",
            "step : 4399  ---  loss train : [0.18071919679641724, 0.1433577537536621, 0.09677869081497192, 0.08042681217193604, 0.13835686445236206, 0.14290982484817505, 0.2113284468650818, 0.29745322465896606, 0.0940139889717102, 0.07936632633209229]\n",
            "step : 4409  ---  loss train : [0.19865554571151733, 0.07847905158996582, 0.08563929796218872, 0.16990816593170166, 0.25514501333236694, 0.49630773067474365, 0.07400888204574585, 0.17249703407287598, 0.35375165939331055, 0.15039587020874023]\n",
            "step : 4419  ---  loss train : [0.2111935019493103, 0.16498219966888428, 0.13143110275268555, 0.19671356678009033, 0.17442315816879272, 0.2234305739402771, 0.09198814630508423, 0.3201676607131958, 0.17041200399398804, 0.4883068799972534]\n",
            "step : 4429  ---  loss train : [0.34611934423446655, 0.618363618850708, 0.16112715005874634, 0.08771437406539917, 0.12671905755996704, 0.3033584952354431, 0.0611453652381897, 0.241838276386261, 0.13531142473220825, 0.07591098546981812]\n",
            "step : 4439  ---  loss train : [0.06568920612335205, 0.20696336030960083, 0.13387161493301392, 0.20200109481811523, 0.06630140542984009, 0.06807863712310791, 0.2194979190826416, 0.1231798529624939, 0.19997936487197876, 0.9997157454490662]\n",
            "step : 4449  ---  loss train : [0.12958019971847534, 0.18985265493392944, 0.06600844860076904, 0.20970970392227173, 0.12803131341934204, 0.21580904722213745, 0.23449474573135376, 0.31230098009109497, 0.23434656858444214, 0.17976701259613037]\n",
            "step : 4459  ---  loss train : [0.3313724994659424, 0.13688576221466064, 0.243841290473938, 0.1349942684173584, 0.1721179485321045, 0.19937974214553833, 0.18538308143615723, 0.19341331720352173, 0.1580488681793213, 0.046386778354644775]\n",
            "step : 4469  ---  loss train : [0.2981277108192444, 0.23010241985321045, 0.15245920419692993, 0.11557024717330933, 0.2090049386024475, 0.13074082136154175, 0.11836791038513184, 0.20957744121551514, 0.07375192642211914, 0.14820122718811035]\n",
            "step : 4479  ---  loss train : [0.19016486406326294, 0.15110373497009277, 0.09009295701980591, 0.07280033826828003, 0.15718531608581543, 0.061825335025787354, 0.727995753288269, 0.09729570150375366, 0.0660860538482666, 0.22188204526901245]\n",
            "step : 4489  ---  loss train : [0.18157482147216797, 0.06524771451950073, 0.10908961296081543, 0.13888651132583618, 0.11275726556777954, 0.14635097980499268, 0.0647273063659668, 0.06774687767028809, 0.24222087860107422, 0.08572947978973389]\n",
            "step : 4499  ---  loss train : [0.2676060199737549, 0.6530655026435852, 0.1250762939453125, 0.41917532682418823, 0.09241563081741333, 0.22385233640670776, 0.34347331523895264, 0.15927886962890625, 0.09393876791000366, 0.1774357557296753]\n",
            "step : 4509  ---  loss train : [0.05181783437728882, 0.11690467596054077, 0.2863093614578247, 0.0699920654296875, 0.1914881467819214, 0.11278647184371948, 0.694420337677002, 0.4224039912223816, 0.10774195194244385, 0.23225945234298706]\n",
            "step : 4519  ---  loss train : [0.29956966638565063, 0.06622916460037231, 0.10298073291778564, 0.07295507192611694, 0.03885924816131592, 0.07327961921691895, 0.31725114583969116, 0.10285842418670654, 0.13596254587173462, 0.41321104764938354]\n",
            "step : 4529  ---  loss train : [0.11507129669189453, 0.1518874168395996, 0.21635770797729492, 0.30311065912246704, 0.045482635498046875, 0.21068620681762695, 0.050253987312316895, 0.12864649295806885, 0.08061033487319946, 0.06462240219116211]\n",
            "step : 4539  ---  loss train : [0.17599856853485107, 0.5374001264572144, 0.18450117111206055, 0.3555540442466736, 0.4081529974937439, 0.3565884828567505, 0.06669330596923828, 0.41450178623199463, 0.06290781497955322, 0.27070707082748413]\n",
            "step : 4549  ---  loss train : [0.06664794683456421, 0.4910880923271179, 0.04109537601470947, 0.1303713321685791, 0.09715414047241211, 0.4639158248901367, 0.144561767578125, 0.13071823120117188, 0.25985825061798096, 0.2878568768501282]\n",
            "step : 4559  ---  loss train : [0.18790870904922485, 0.2653774619102478, 0.12077546119689941, 0.23577111959457397, 0.09880733489990234, 0.22699564695358276, 0.4774706959724426, 0.18028771877288818, 0.04913830757141113, 0.05632472038269043]\n",
            "step : 4569  ---  loss train : [0.0796087384223938, 0.11362463235855103, 0.34696805477142334, 0.2348971962928772, 0.13169580698013306, 0.40225207805633545, 0.20268946886062622, 0.1425560712814331, 0.05965864658355713, 0.1167067289352417]\n",
            "step : 4579  ---  loss train : [0.13705456256866455, 0.062401413917541504, 0.10663014650344849, 0.20305651426315308, 0.06303119659423828, 0.16167092323303223, 0.17076659202575684, 0.1445065140724182, 0.0971444845199585, 0.17165547609329224]\n",
            "step : 4589  ---  loss train : [0.10339266061782837, 0.12856876850128174, 0.15262997150421143, 0.20845657587051392, 0.17799240350723267, 0.07025498151779175, 0.08299285173416138, 0.14389735460281372, 0.08289259672164917, 0.06946831941604614]\n",
            "step : 4599  ---  loss train : [0.5071496963500977, 0.19170397520065308, 0.08754998445510864, 0.11677825450897217, 0.08716475963592529, 0.127424955368042, 0.12796777486801147, 0.31829822063446045, 0.10909909009933472, 0.19995683431625366]\n",
            "step : 4609  ---  loss train : [0.598292350769043, 0.06780761480331421, 0.33660364151000977, 0.3824114203453064, 0.28354108333587646, 0.12181943655014038, 0.16234147548675537, 0.22525262832641602, 0.0846414566040039, 0.053295135498046875]\n",
            "step : 4619  ---  loss train : [0.17990291118621826, 0.3336769938468933, 0.06241708993911743, 0.07238346338272095, 0.21224963665008545, 0.05050724744796753, 0.08288151025772095, 0.12217676639556885, 0.11043190956115723, 0.10664635896682739]\n",
            "step : 4629  ---  loss train : [0.12386590242385864, 0.10285240411758423, 0.11276382207870483, 0.11893421411514282, 0.1448974609375, 0.152540922164917, 0.25934547185897827, 0.14144033193588257, 0.08840274810791016, 0.18940764665603638]\n",
            "step : 4639  ---  loss train : [0.13186901807785034, 0.115425705909729, 0.11063617467880249, 0.18108636140823364, 0.059760987758636475, 0.6005821228027344, 0.09477031230926514, 0.13638347387313843, 0.07838588953018188, 0.21803581714630127]\n",
            "step : 4649  ---  loss train : [0.18189483880996704, 0.06075388193130493, 0.1368507742881775, 0.21075016260147095, 0.158935546875, 0.0919426679611206, 0.13038748502731323, 0.0829818844795227, 0.09624338150024414, 0.2537722587585449]\n",
            "step : 4659  ---  loss train : [0.11222422122955322, 0.29668915271759033, 0.2779364585876465, 0.10211193561553955, 0.06139791011810303, 0.07116615772247314, 0.10082626342773438, 0.12238514423370361, 0.08641654253005981, 0.10903340578079224]\n",
            "step : 4669  ---  loss train : [0.10549545288085938, 0.11662137508392334, 0.11418992280960083, 0.08838403224945068, 0.2056165337562561, 0.10051822662353516, 0.31112152338027954, 0.41568857431411743, 0.08777570724487305, 0.2142329216003418]\n",
            "step : 4679  ---  loss train : [0.1211664080619812, 0.0810440182685852, 0.07479733228683472, 0.17720717191696167, 0.10959523916244507, 0.058025896549224854, 0.12240016460418701, 0.6749299168586731, 0.07480406761169434, 0.10523843765258789]\n",
            "step : 4689  ---  loss train : [0.07423442602157593, 0.14694267511367798, 0.08129960298538208, 0.18617033958435059, 0.07034796476364136, 0.27592527866363525, 0.10052895545959473, 0.07553261518478394, 0.1164810061454773, 0.07779812812805176]\n",
            "step : 4690  ---  loss train : [0.999830961227417, 0.9954718351364136, 0.9990280270576477, 0.5054621696472168, 0.6112208366394043, 0.6931056976318359, 0.9928662776947021, 0.24085259437561035, 0.21589183807373047, 0.9893493056297302, 0.238195538520813, 0.9933944940567017, 0.9991222023963928, 0.3079487085342407, 0.047061026096343994, 0.06554365158081055, 0.1792258620262146, 0.24902713298797607, 0.0953570008277893, 0.2924113869667053, 0.27035772800445557, 0.20478761196136475, 0.9975379109382629, 0.9973899722099304, 0.24923503398895264, 0.14511322975158691, 0.14267468452453613, 0.15033382177352905, 0.10288470983505249, 0.16144126653671265, 0.9833098649978638, 0.14193123579025269, 0.13463473320007324, 0.13011932373046875, 0.07560533285140991, 0.9967169165611267, 0.9996784329414368, 0.3126147985458374, 0.3029429316520691, 0.19003057479858398, 0.48170387744903564, 0.15939253568649292, 0.13860970735549927, 0.23011106252670288, 0.29730135202407837, 0.3293082118034363, 0.9998651146888733, 0.9999232292175293, 0.9999253749847412, 0.9998788833618164, 0.9998777508735657, 0.9996928572654724, 0.9998104572296143, 0.9604640603065491, 0.9998997449874878, 0.9998953938484192, 0.999916672706604, 0.9999382495880127, 0.9999398589134216, 0.9998286962509155, 0.9957619905471802, 0.2514575719833374, 0.08671528100967407, 0.13965201377868652, 0.29479968547821045, 0.9998229742050171, 0.9990003108978271, 0.09488165378570557, 0.23613744974136353, 0.999332845211029, 0.9994530081748962, 0.9995965957641602, 0.9994866847991943, 0.9964970350265503, 0.7542628049850464, 0.27122563123703003, 0.5756781101226807, 0.38501983880996704, 0.6140797138214111, 0.7249975204467773, 0.26656126976013184, 0.2794291377067566, 0.3001132011413574, 0.843544602394104, 0.22941970825195312, 0.7826458811759949, 0.5381466150283813, 0.4243438243865967, 0.4263954162597656, 0.40692025423049927, 0.2860459089279175, 0.10021460056304932, 0.04943728446960449, 0.13452154397964478, 0.13984167575836182, 0.09262359142303467, 0.08975547552108765, 0.08134543895721436, 0.06882226467132568, 0.07628881931304932, 0.06721770763397217, 0.06588393449783325, 0.9977351427078247, 0.36869704723358154, 0.3654422163963318, 0.12347930669784546, 0.0859326720237732, 0.1865231990814209, 0.1953892707824707, 0.10910665988922119, 0.08027583360671997, 0.9956384301185608, 0.4606791138648987, 0.6101823449134827, 0.20983433723449707, 0.2358304262161255, 0.5788990259170532, 0.1358494758605957, 0.050062716007232666, 0.0720948576927185, 0.08719754219055176, 0.20571178197860718, 0.9744636416435242, 0.29650717973709106, 0.25814318656921387, 0.3404901623725891, 0.6363505125045776, 0.9876465797424316, 0.26926809549331665, 0.26283591985702515, 0.4010710120201111, 0.38625746965408325, 0.9910478591918945, 0.24031728506088257, 0.23265939950942993, 0.21903282403945923, 0.2929004430770874, 0.9913438558578491, 0.8142746686935425, 0.9985963702201843, 0.12268024682998657, 0.12860530614852905, 0.1770113706588745, 0.9978362321853638, 0.9993832111358643, 0.999528706073761, 0.999275803565979, 0.996077299118042, 0.06893831491470337, 0.058054566383361816, 0.05801069736480713, 0.027780115604400635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [06:24<59:01, 39.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 4690  ---  mean_dsc : 0.5732860460261155\n",
            "step : 4699  ---  loss train : [0.057138264179229736, 0.13883233070373535, 0.14814382791519165, 0.5835796594619751, 0.07413637638092041, 0.1759207844734192, 0.06371301412582397, 0.06426197290420532, 0.14431065320968628, 0.28346365690231323]\n",
            "step : 4709  ---  loss train : [0.13282567262649536, 0.11199367046356201, 0.049914419651031494, 0.1473812460899353, 0.24744796752929688, 0.1338617205619812, 0.07062000036239624, 0.08912450075149536, 0.2031305432319641, 0.06095653772354126]\n",
            "step : 4719  ---  loss train : [0.057652413845062256, 0.12596040964126587, 0.24019157886505127, 0.14140725135803223, 0.11049807071685791, 0.038349449634552, 0.14649361371994019, 0.06521892547607422, 0.34913575649261475, 0.07917147874832153]\n",
            "step : 4729  ---  loss train : [0.08938765525817871, 0.06918621063232422, 0.10041064023971558, 0.05216032266616821, 0.20518255233764648, 0.04510796070098877, 0.1246904730796814, 0.1040802001953125, 0.21520918607711792, 0.07009643316268921]\n",
            "step : 4739  ---  loss train : [0.048951685428619385, 0.03509843349456787, 0.48298126459121704, 0.07143253087997437, 0.06683474779129028, 0.3386971354484558, 0.1671464443206787, 0.09952664375305176, 0.4804159998893738, 0.32192927598953247]\n",
            "step : 4749  ---  loss train : [0.6462435722351074, 0.20762759447097778, 0.1606101393699646, 0.04258674383163452, 0.08247852325439453, 0.10462278127670288, 0.0761559009552002, 0.09097689390182495, 0.13914185762405396, 0.44714832305908203]\n",
            "step : 4759  ---  loss train : [0.26845890283584595, 0.11865383386611938, 0.17219144105911255, 0.04504287242889404, 0.21317678689956665, 0.056859612464904785, 0.04810464382171631, 0.12466037273406982, 0.16683334112167358, 0.12627196311950684]\n",
            "step : 4769  ---  loss train : [0.062230706214904785, 0.08314114809036255, 0.19056415557861328, 0.42114484310150146, 0.15413105487823486, 0.1228373646736145, 0.31658482551574707, 0.07061457633972168, 0.36407238245010376, 0.4238514304161072]\n",
            "step : 4779  ---  loss train : [0.0751420259475708, 0.06210911273956299, 0.06454592943191528, 0.0648459792137146, 0.20448285341262817, 0.27519840002059937, 0.30413883924484253, 0.14061862230300903, 0.12094223499298096, 0.07052147388458252]\n",
            "step : 4789  ---  loss train : [0.15728914737701416, 0.1451573371887207, 0.07724964618682861, 0.18065494298934937, 0.40743356943130493, 0.9925796985626221, 0.12881213426589966, 0.12376439571380615, 0.10943686962127686, 0.1245986819267273]\n",
            "step : 4799  ---  loss train : [0.22244423627853394, 0.2817809581756592, 0.16966742277145386, 0.2545434236526489, 0.09396505355834961, 0.12205016613006592, 0.06284236907958984, 0.2343716025352478, 0.27135133743286133, 0.20414501428604126]\n",
            "step : 4809  ---  loss train : [0.1599329113960266, 0.04274547100067139, 0.10412651300430298, 0.05577421188354492, 0.18781208992004395, 0.1497858166694641, 0.44619840383529663, 0.29794466495513916, 0.10761880874633789, 0.3816615343093872]\n",
            "step : 4819  ---  loss train : [0.09549844264984131, 0.24331778287887573, 0.35929274559020996, 0.0823289155960083, 0.09664666652679443, 0.3622729778289795, 0.07189035415649414, 0.44331902265548706, 0.09028744697570801, 0.3055118918418884]\n",
            "step : 4829  ---  loss train : [0.11281925439834595, 0.1285967230796814, 0.16712772846221924, 0.3081582188606262, 0.9997281432151794, 0.07558149099349976, 0.05191183090209961, 0.11759459972381592, 0.07660490274429321, 0.07109010219573975]\n",
            "step : 4839  ---  loss train : [0.07581174373626709, 0.1252155303955078, 0.05837273597717285, 0.1675102710723877, 0.056333303451538086, 0.06776028871536255, 0.1264614462852478, 0.046769797801971436, 0.08541679382324219, 0.1691056489944458]\n",
            "step : 4849  ---  loss train : [0.3374773859977722, 0.06803852319717407, 0.16375648975372314, 0.1963842511177063, 0.999732494354248, 0.08253192901611328, 0.06981217861175537, 0.10215479135513306, 0.0741647481918335, 0.13542938232421875]\n",
            "step : 4859  ---  loss train : [0.10656172037124634, 0.06817018985748291, 0.10596752166748047, 0.10471785068511963, 0.2547333240509033, 0.06311744451522827, 0.17132508754730225, 0.061295926570892334, 0.08347815275192261, 0.14707648754119873]\n",
            "step : 4869  ---  loss train : [0.13120198249816895, 0.09469974040985107, 0.08082520961761475, 0.1479889154434204, 0.17249071598052979, 0.2163257598876953, 0.2682344317436218, 0.10823434591293335, 0.0733187198638916, 0.15872567892074585]\n",
            "step : 4879  ---  loss train : [0.06837218999862671, 0.07508277893066406, 0.1688995361328125, 0.20144492387771606, 0.4934263229370117, 0.061602652072906494, 0.19838261604309082, 0.2223690152168274, 0.14124363660812378, 0.17118531465530396]\n",
            "step : 4889  ---  loss train : [0.15255498886108398, 0.12736821174621582, 0.20065730810165405, 0.17784744501113892, 0.19479691982269287, 0.08310627937316895, 0.29490602016448975, 0.16568905115127563, 0.40641677379608154, 0.34953951835632324]\n",
            "step : 4899  ---  loss train : [0.40641671419143677, 0.14151006937026978, 0.07115858793258667, 0.11707490682601929, 0.19136780500411987, 0.06472259759902954, 0.26889336109161377, 0.14785116910934448, 0.07258367538452148, 0.05963259935379028]\n",
            "step : 4909  ---  loss train : [0.20001071691513062, 0.12615424394607544, 0.2135278582572937, 0.06875628232955933, 0.06671327352523804, 0.09859859943389893, 0.10362875461578369, 0.0895262360572815, 0.9997486472129822, 0.13716274499893188]\n",
            "step : 4919  ---  loss train : [0.19190680980682373, 0.05591452121734619, 0.17866134643554688, 0.16134697198867798, 0.21522146463394165, 0.15023541450500488, 0.3241228461265564, 0.12834680080413818, 0.16717082262039185, 0.18075531721115112]\n",
            "step : 4929  ---  loss train : [0.11955571174621582, 0.21314465999603271, 0.07813233137130737, 0.15613096952438354, 0.191666841506958, 0.1824457049369812, 0.19275397062301636, 0.1552027463912964, 0.05501073598861694, 0.1909065842628479]\n",
            "step : 4939  ---  loss train : [0.16952013969421387, 0.1860293745994568, 0.0697055459022522, 0.18514764308929443, 0.10869443416595459, 0.10123205184936523, 0.1581631898880005, 0.08303016424179077, 0.16311591863632202, 0.09813541173934937]\n",
            "step : 4949  ---  loss train : [0.09657657146453857, 0.08569592237472534, 0.07082980871200562, 0.14053058624267578, 0.05580180883407593, 0.5307472348213196, 0.07990223169326782, 0.07627236843109131, 0.10817807912826538, 0.1871827244758606]\n",
            "step : 4959  ---  loss train : [0.08178055286407471, 0.15570515394210815, 0.15678668022155762, 0.12219232320785522, 0.1270667314529419, 0.06786823272705078, 0.05914723873138428, 0.25374215841293335, 0.08012628555297852, 0.2500566244125366]\n",
            "step : 4969  ---  loss train : [0.6504487991333008, 0.10267847776412964, 0.41713547706604004, 0.09040981531143188, 0.16651278734207153, 0.23673546314239502, 0.15543818473815918, 0.04462331533432007, 0.19888901710510254, 0.04473692178726196]\n",
            "step : 4979  ---  loss train : [0.11289697885513306, 0.24348467588424683, 0.06787800788879395, 0.16000741720199585, 0.11902672052383423, 0.6206966042518616, 0.4073307514190674, 0.10044592618942261, 0.15407466888427734, 0.23567205667495728]\n",
            "step : 4989  ---  loss train : [0.06017798185348511, 0.09268498420715332, 0.07163530588150024, 0.03755301237106323, 0.06196296215057373, 0.27224624156951904, 0.09958118200302124, 0.1552308201789856, 0.3321228623390198, 0.11499351263046265]\n",
            "step : 4999  ---  loss train : [0.1318444013595581, 0.22774595022201538, 0.33363890647888184, 0.03521078824996948, 0.22430723905563354, 0.04375624656677246, 0.12454688549041748, 0.08367300033569336, 0.06183379888534546, 0.12135028839111328]\n",
            "step : 5009  ---  loss train : [0.5836180448532104, 0.1630082130432129, 0.31012797355651855, 0.3345860242843628, 0.35519349575042725, 0.0711679458618164, 0.36840754747390747, 0.06137341260910034, 0.2727581858634949, 0.04780799150466919]\n",
            "step : 5019  ---  loss train : [0.381015419960022, 0.04515045881271362, 0.11838662624359131, 0.10453170537948608, 0.4366220235824585, 0.1445818543434143, 0.10303092002868652, 0.2209101915359497, 0.2427951693534851, 0.1345781683921814]\n",
            "step : 5029  ---  loss train : [0.14795130491256714, 0.11088502407073975, 0.13409876823425293, 0.10188251733779907, 0.05676078796386719, 0.514521598815918, 0.15130382776260376, 0.04711991548538208, 0.05705273151397705, 0.0760655403137207]\n",
            "step : 5039  ---  loss train : [0.09737449884414673, 0.39087873697280884, 0.11920619010925293, 0.09777027368545532, 0.28718847036361694, 0.1587463617324829, 0.1328449845314026, 0.06876522302627563, 0.08083778619766235, 0.13616520166397095]\n",
            "step : 5049  ---  loss train : [0.052507102489471436, 0.11785870790481567, 0.13293969631195068, 0.07284736633300781, 0.16082149744033813, 0.16542887687683105, 0.14562082290649414, 0.07847589254379272, 0.13955408334732056, 0.11392784118652344]\n",
            "step : 5059  ---  loss train : [0.12952721118927002, 0.12524008750915527, 0.1929672360420227, 0.1827080249786377, 0.0753815770149231, 0.12772291898727417, 0.15500128269195557, 0.08762776851654053, 0.07923746109008789, 0.49803996086120605]\n",
            "step : 5069  ---  loss train : [0.1800045371055603, 0.06112527847290039, 0.09542006254196167, 0.08173197507858276, 0.0947536826133728, 0.07867312431335449, 0.2898290753364563, 0.15995192527770996, 0.1528184413909912, 0.6252726316452026]\n",
            "step : 5079  ---  loss train : [0.06905454397201538, 0.2853192090988159, 0.4115476608276367, 0.26064586639404297, 0.1114690899848938, 0.16575288772583008, 0.22604775428771973, 0.04419827461242676, 0.07271701097488403, 0.17673033475875854]\n",
            "step : 5089  ---  loss train : [0.3402599096298218, 0.06574428081512451, 0.06880563497543335, 0.20717835426330566, 0.04218775033950806, 0.07826381921768188, 0.10903698205947876, 0.15508294105529785, 0.10122668743133545, 0.12161391973495483]\n",
            "step : 5099  ---  loss train : [0.09633803367614746, 0.11381852626800537, 0.11168760061264038, 0.15994888544082642, 0.12483954429626465, 0.26995283365249634, 0.14755374193191528, 0.09798258543014526, 0.21026843786239624, 0.16279006004333496]\n",
            "step : 5109  ---  loss train : [0.12211912870407104, 0.10774296522140503, 0.1973201036453247, 0.059561312198638916, 0.5741370320320129, 0.09075850248336792, 0.14087504148483276, 0.06715518236160278, 0.23387271165847778, 0.17823398113250732]\n",
            "step : 5119  ---  loss train : [0.059570133686065674, 0.13365846872329712, 0.24028921127319336, 0.14474362134933472, 0.08812874555587769, 0.11800187826156616, 0.07530874013900757, 0.08925467729568481, 0.1613510251045227, 0.11796343326568604]\n",
            "step : 5129  ---  loss train : [0.3032544255256653, 0.2740122079849243, 0.11079686880111694, 0.0589028000831604, 0.073844313621521, 0.08982932567596436, 0.10952651500701904, 0.08865445852279663, 0.11875218152999878, 0.07843542098999023]\n",
            "step : 5139  ---  loss train : [0.11459076404571533, 0.12645947933197021, 0.1103169322013855, 0.2488487958908081, 0.08997851610183716, 0.38366037607192993, 0.4216047525405884, 0.09955781698226929, 0.23720258474349976, 0.13119536638259888]\n",
            "step : 5149  ---  loss train : [0.06315875053405762, 0.07397592067718506, 0.1798725724220276, 0.0923953652381897, 0.06965166330337524, 0.127555251121521, 0.6686465740203857, 0.06665217876434326, 0.10907667875289917, 0.08422273397445679]\n",
            "step : 5159  ---  loss train : [0.1495143175125122, 0.072787344455719, 0.20145392417907715, 0.11715596914291382, 0.3667547106742859, 0.13235288858413696, 0.07690197229385376, 0.13359469175338745, 0.06675803661346436, 0.049363017082214355]\n",
            "step : 5159  ---  loss train : [0.999848484992981, 0.9942530393600464, 0.9993757009506226, 0.37469345331192017, 0.5637999773025513, 0.6943109035491943, 0.9962887167930603, 0.10253018140792847, 0.09118205308914185, 0.9954808354377747, 0.12495583295822144, 0.12032592296600342, 0.9988826513290405, 0.2474246621131897, 0.05319434404373169, 0.07353037595748901, 0.10078990459442139, 0.0837509036064148, 0.037980496883392334, 0.1572445034980774, 0.1982308030128479, 0.14466780424118042, 0.997653603553772, 0.9350197315216064, 0.295391321182251, 0.17393231391906738, 0.1598244309425354, 0.17392534017562866, 0.13109993934631348, 0.1999661922454834, 0.9833610653877258, 0.06589454412460327, 0.06253713369369507, 0.05560481548309326, 0.017629623413085938, 0.9986752271652222, 0.9997913241386414, 0.2626040577888489, 0.19084155559539795, 0.37397336959838867, 0.33651018142700195, 0.12487185001373291, 0.15234053134918213, 0.20892119407653809, 0.3158208727836609, 0.37689048051834106, 0.9998723864555359, 0.9999297261238098, 0.9999334216117859, 0.9998951554298401, 0.9998852610588074, 0.9997214078903198, 0.9998132586479187, 0.913086473941803, 0.9798800349235535, 0.9907305240631104, 0.9999169111251831, 0.9999347925186157, 0.9999340176582336, 0.9998301863670349, 0.9961050152778625, 0.29284006357192993, 0.100338876247406, 0.15498310327529907, 0.3186227083206177, 0.9998517036437988, 0.9991558790206909, 0.02647620439529419, 0.9950659275054932, 0.9993555545806885, 0.9993886351585388, 0.9995927810668945, 0.9994943141937256, 0.9949005246162415, 0.8445464372634888, 0.20475852489471436, 0.3186227083206177, 0.13027936220169067, 0.19045978784561157, 0.10779613256454468, 0.13899236917495728, 0.14542120695114136, 0.1302410364151001, 0.7945486307144165, 0.17564016580581665, 0.5479525923728943, 0.3621377944946289, 0.3482886552810669, 0.36926358938217163, 0.3436933755874634, 0.14795124530792236, 0.05634075403213501, 0.06862938404083252, 0.12193888425827026, 0.056347429752349854, 0.03757166862487793, 0.0366971492767334, 0.034105777740478516, 0.024926841259002686, 0.028834760189056396, 0.023056745529174805, 0.024334728717803955, 0.9981060028076172, 0.8084491491317749, 0.4570577144622803, 0.14252090454101562, 0.10611051321029663, 0.21592897176742554, 0.3027884364128113, 0.09629309177398682, 0.03316831588745117, 0.9958186745643616, 0.31278717517852783, 0.7111511826515198, 0.1525995135307312, 0.12674498558044434, 0.6242471933364868, 0.14537227153778076, 0.05299782752990723, 0.07813185453414917, 0.09497612714767456, 0.22370392084121704, 0.9744082093238831, 0.1928730010986328, 0.18049466609954834, 0.35113364458084106, 0.8639252185821533, 0.7729278206825256, 0.20716643333435059, 0.17175942659378052, 0.213029146194458, 0.18974775075912476, 0.9816539287567139, 0.11765319108963013, 0.09965771436691284, 0.10534226894378662, 0.09073299169540405, 0.14656692743301392, 0.15163499116897583, 0.9983313679695129, 0.0637822151184082, 0.07103073596954346, 0.10269045829772949, 0.9979275465011597, 0.999415397644043, 0.999544084072113, 0.9992872476577759, 0.9960901141166687, 0.027767598628997803, 0.022887468338012695, 0.02470463514328003, 0.010336577892303467]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [07:04<58:37, 39.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 5159  ---  mean_dsc : 0.5742954408299654\n",
            "step : 5169  ---  loss train : [0.10757732391357422, 0.14606207609176636, 0.5789338946342468, 0.07628530263900757, 0.186958909034729, 0.06637442111968994, 0.05649852752685547, 0.14110714197158813, 0.26305872201919556, 0.1260574460029602]\n",
            "step : 5179  ---  loss train : [0.11778688430786133, 0.045152366161346436, 0.1319330334663391, 0.24389898777008057, 0.14732277393341064, 0.14635884761810303, 0.09110403060913086, 0.20083755254745483, 0.0554468035697937, 0.05617731809616089]\n",
            "step : 5189  ---  loss train : [0.12511318922042847, 0.20200109481811523, 0.14088457822799683, 0.10675626993179321, 0.03585857152938843, 0.14063912630081177, 0.05928647518157959, 0.2918434143066406, 0.06286966800689697, 0.08942419290542603]\n",
            "step : 5199  ---  loss train : [0.06247758865356445, 0.10099422931671143, 0.049897193908691406, 0.20177316665649414, 0.04259073734283447, 0.1102718710899353, 0.10550820827484131, 0.18589907884597778, 0.059158265590667725, 0.048529863357543945]\n",
            "step : 5209  ---  loss train : [0.03694123029708862, 0.4831349849700928, 0.06854164600372314, 0.054934144020080566, 0.35369396209716797, 0.17067140340805054, 0.09914237260818481, 0.5094635486602783, 0.3109663724899292, 0.6451166272163391]\n",
            "step : 5219  ---  loss train : [0.2670738101005554, 0.13518142700195312, 0.03950643539428711, 0.07920849323272705, 0.1065438985824585, 0.06692439317703247, 0.10556983947753906, 0.12496280670166016, 0.40077418088912964, 0.26668137311935425]\n",
            "step : 5229  ---  loss train : [0.11431986093521118, 0.19519102573394775, 0.04209786653518677, 0.17406797409057617, 0.0531122088432312, 0.05117297172546387, 0.1295422911643982, 0.17468500137329102, 0.12819409370422363, 0.06307071447372437]\n",
            "step : 5239  ---  loss train : [0.08056873083114624, 0.15864348411560059, 0.3588449954986572, 0.15494763851165771, 0.13455772399902344, 0.33769237995147705, 0.06666415929794312, 0.46657419204711914, 0.2655893564224243, 0.0663488507270813]\n",
            "step : 5249  ---  loss train : [0.05907779932022095, 0.05693250894546509, 0.06394296884536743, 0.21347838640213013, 0.26481157541275024, 0.33227378129959106, 0.1541624665260315, 0.12027090787887573, 0.06996279954910278, 0.16218805313110352]\n",
            "step : 5259  ---  loss train : [0.14595776796340942, 0.0741279125213623, 0.18142110109329224, 0.3861595392227173, 0.3863711357116699, 0.10696405172348022, 0.11570662260055542, 0.10844635963439941, 0.14418667554855347, 0.22402304410934448]\n",
            "step : 5269  ---  loss train : [0.2810053825378418, 0.15660202503204346, 0.2582305073738098, 0.11116617918014526, 0.10394233465194702, 0.06367605924606323, 0.19840151071548462, 0.25558018684387207, 0.204334557056427, 0.1587761640548706]\n",
            "step : 5279  ---  loss train : [0.04173469543457031, 0.10246306657791138, 0.05997258424758911, 0.17952805757522583, 0.15077143907546997, 0.39073872566223145, 0.2935410737991333, 0.10976541042327881, 0.2708096504211426, 0.09940111637115479]\n",
            "step : 5289  ---  loss train : [0.2274978756904602, 0.3383371829986572, 0.10308235883712769, 0.112484872341156, 0.3249565362930298, 0.10268741846084595, 0.45159053802490234, 0.07632690668106079, 0.27280116081237793, 0.1268831491470337]\n",
            "step : 5299  ---  loss train : [0.1893402338027954, 0.20244020223617554, 0.36325985193252563, 0.9997313022613525, 0.07512301206588745, 0.056257545948028564, 0.10106062889099121, 0.07528585195541382, 0.06500005722045898, 0.07551449537277222]\n",
            "step : 5309  ---  loss train : [0.14921307563781738, 0.05691879987716675, 0.16086608171463013, 0.060130774974823, 0.06032317876815796, 0.19328075647354126, 0.04629772901535034, 0.07926177978515625, 0.16006618738174438, 0.3082648515701294]\n",
            "step : 5319  ---  loss train : [0.0700000524520874, 0.17836791276931763, 0.19028931856155396, 0.9996742010116577, 0.07916015386581421, 0.07255852222442627, 0.10020041465759277, 0.0775141716003418, 0.1391274333000183, 0.1420084834098816]\n",
            "step : 5329  ---  loss train : [0.06463378667831421, 0.10895603895187378, 0.10811614990234375, 0.24268919229507446, 0.062138259410858154, 0.1889873743057251, 0.05909168720245361, 0.08197671175003052, 0.1387990117073059, 0.1434171199798584]\n",
            "step : 5339  ---  loss train : [0.10915130376815796, 0.08090472221374512, 0.14630579948425293, 0.16520804166793823, 0.2201346755027771, 0.2562374472618103, 0.10069310665130615, 0.06998509168624878, 0.1531570553779602, 0.07534873485565186]\n",
            "step : 5349  ---  loss train : [0.07690834999084473, 0.1386038064956665, 0.19827783107757568, 0.506442666053772, 0.046116530895233154, 0.17991822957992554, 0.2931140065193176, 0.13993293046951294, 0.18276363611221313, 0.14716917276382446]\n",
            "step : 5359  ---  loss train : [0.14839613437652588, 0.19894355535507202, 0.19043570756912231, 0.17340922355651855, 0.08006173372268677, 0.3022334575653076, 0.171769917011261, 0.40575510263442993, 0.3511275053024292, 0.4841218590736389]\n",
            "step : 5369  ---  loss train : [0.13887816667556763, 0.07158833742141724, 0.12091165781021118, 0.21400153636932373, 0.0588718056678772, 0.2764343023300171, 0.14613425731658936, 0.07219427824020386, 0.05913013219833374, 0.21341657638549805]\n",
            "step : 5379  ---  loss train : [0.12855100631713867, 0.21951735019683838, 0.07622784376144409, 0.06808406114578247, 0.12461793422698975, 0.10871320962905884, 0.11818808317184448, 0.9997431039810181, 0.13435900211334229, 0.1936032772064209]\n",
            "step : 5389  ---  loss train : [0.05549365282058716, 0.18376916646957397, 0.13577944040298462, 0.21678811311721802, 0.1448526382446289, 0.3331855535507202, 0.12044471502304077, 0.12783706188201904, 0.19213628768920898, 0.12329167127609253]\n",
            "step : 5399  ---  loss train : [0.18609094619750977, 0.08227252960205078, 0.15473759174346924, 0.18821263313293457, 0.2072121500968933, 0.19467496871948242, 0.15003734827041626, 0.05556732416152954, 0.14652198553085327, 0.1695895791053772]\n",
            "step : 5409  ---  loss train : [0.18927007913589478, 0.08428609371185303, 0.1705443263053894, 0.12134057283401489, 0.09509342908859253, 0.17425328493118286, 0.07331931591033936, 0.13451671600341797, 0.0822632908821106, 0.0900006890296936]\n",
            "step : 5419  ---  loss train : [0.07907283306121826, 0.07111680507659912, 0.13399648666381836, 0.05202126502990723, 0.4233049154281616, 0.0773158073425293, 0.06837499141693115, 0.10147780179977417, 0.2053992748260498, 0.08062553405761719]\n",
            "step : 5429  ---  loss train : [0.16226518154144287, 0.17650336027145386, 0.12492334842681885, 0.10188710689544678, 0.054067790508270264, 0.05273312330245972, 0.2195693850517273, 0.09087693691253662, 0.3324916958808899, 0.6875787973403931]\n",
            "step : 5439  ---  loss train : [0.10368311405181885, 0.39416801929473877, 0.1006476879119873, 0.1715206503868103, 0.24993020296096802, 0.16461116075515747, 0.06996405124664307, 0.17658287286758423, 0.040539443492889404, 0.10471236705780029]\n",
            "step : 5449  ---  loss train : [0.22146272659301758, 0.07009446620941162, 0.13356751203536987, 0.11393338441848755, 0.6044331789016724, 0.39724892377853394, 0.1046898365020752, 0.14677143096923828, 0.24360674619674683, 0.05906510353088379]\n",
            "step : 5459  ---  loss train : [0.07370811700820923, 0.10658997297286987, 0.041077375411987305, 0.06658816337585449, 0.2934730648994446, 0.18069028854370117, 0.15186643600463867, 0.2619660496711731, 0.11653214693069458, 0.142608642578125]\n",
            "step : 5469  ---  loss train : [0.21328520774841309, 0.31174635887145996, 0.06144636869430542, 0.2546548843383789, 0.07586896419525146, 0.116549551486969, 0.10618823766708374, 0.06485992670059204, 0.10170984268188477, 0.5400525331497192]\n",
            "step : 5479  ---  loss train : [0.1714370846748352, 0.17804330587387085, 0.30625826120376587, 0.34956657886505127, 0.0683591365814209, 0.43502354621887207, 0.06001502275466919, 0.26412975788116455, 0.06366837024688721, 0.3483312726020813]\n",
            "step : 5489  ---  loss train : [0.035659730434417725, 0.09381133317947388, 0.09517371654510498, 0.37102001905441284, 0.17808091640472412, 0.0884968638420105, 0.1942998766899109, 0.23231136798858643, 0.10501492023468018, 0.13634133338928223]\n",
            "step : 5499  ---  loss train : [0.10495835542678833, 0.11165732145309448, 0.0855608582496643, 0.09408688545227051, 0.5158316493034363, 0.14952653646469116, 0.042844295501708984, 0.05699187517166138, 0.06018400192260742, 0.10602432489395142]\n",
            "step : 5509  ---  loss train : [0.3542706370353699, 0.12164956331253052, 0.09424865245819092, 0.2313973307609558, 0.1557023525238037, 0.12805408239364624, 0.06226658821105957, 0.08546644449234009, 0.13293063640594482, 0.05221438407897949]\n",
            "step : 5519  ---  loss train : [0.12569302320480347, 0.1288127303123474, 0.06653982400894165, 0.1688898801803589, 0.17175835371017456, 0.14338082075119019, 0.07412296533584595, 0.10927116870880127, 0.1117396354675293, 0.12320679426193237]\n",
            "step : 5529  ---  loss train : [0.1286834478378296, 0.18933719396591187, 0.20062309503555298, 0.07370281219482422, 0.04151242971420288, 0.15060514211654663, 0.11135345697402954, 0.059226274490356445, 0.3031159043312073, 0.17192435264587402]\n",
            "step : 5539  ---  loss train : [0.05447036027908325, 0.09083670377731323, 0.09969007968902588, 0.09808439016342163, 0.15301918983459473, 0.6759147644042969, 0.29157555103302, 0.19639641046524048, 0.7071269750595093, 0.06968790292739868]\n",
            "step : 5549  ---  loss train : [0.2394883632659912, 0.4053569436073303, 0.4175274968147278, 0.10296982526779175, 0.16018885374069214, 0.24192285537719727, 0.08025890588760376, 0.05510854721069336, 0.18643033504486084, 0.28951120376586914]\n",
            "step : 5559  ---  loss train : [0.056078433990478516, 0.07732057571411133, 0.2034112811088562, 0.08000415563583374, 0.0700036883354187, 0.13074618577957153, 0.12553882598876953, 0.11155080795288086, 0.11674880981445312, 0.10959833860397339]\n",
            "step : 5569  ---  loss train : [0.10643529891967773, 0.10111445188522339, 0.13654190301895142, 0.15330296754837036, 0.23134344816207886, 0.1570994257926941, 0.08324092626571655, 0.18917548656463623, 0.1753997802734375, 0.11263668537139893]\n",
            "step : 5579  ---  loss train : [0.1048659086227417, 0.17897218465805054, 0.05912768840789795, 0.5143699645996094, 0.09119993448257446, 0.13628220558166504, 0.05968022346496582, 0.2081090211868286, 0.16102230548858643, 0.060275912284851074]\n",
            "step : 5589  ---  loss train : [0.13607347011566162, 0.19039243459701538, 0.16749107837677002, 0.087368905544281, 0.11121821403503418, 0.07921725511550903, 0.08691036701202393, 0.1876872181892395, 0.11253368854522705, 0.29290956258773804]\n",
            "step : 5599  ---  loss train : [0.261283278465271, 0.10738486051559448, 0.05120497941970825, 0.07220369577407837, 0.08382129669189453, 0.10401386022567749, 0.10240113735198975, 0.10822141170501709, 0.0813717246055603, 0.11404907703399658]\n",
            "step : 5609  ---  loss train : [0.12405222654342651, 0.09153521060943604, 0.21354132890701294, 0.0803486704826355, 0.29620420932769775, 0.4249221682548523, 0.09739232063293457, 0.22759854793548584, 0.13493847846984863, 0.05943065881729126]\n",
            "step : 5619  ---  loss train : [0.0709565281867981, 0.1582363247871399, 0.09071910381317139, 0.0606723427772522, 0.1277625560760498, 0.7022849917411804, 0.08965593576431274, 0.10391467809677124, 0.07914572954177856, 0.14796549081802368]\n",
            "step : 5628  ---  loss train : [0.9997297525405884, 0.9327508211135864, 0.9982961416244507, 0.6216596961021423, 0.6985002160072327, 0.9978484511375427, 0.06426364183425903, 0.08440059423446655, 0.0492398738861084, 0.03734385967254639, 0.05908310413360596, 0.04445505142211914, 0.9836655855178833, 0.3550204038619995, 0.05605953931808472, 0.07611101865768433, 0.15742993354797363, 0.039251625537872314, 0.03161054849624634, 0.041953325271606445, 0.04491770267486572, 0.04808211326599121, 0.21679991483688354, 0.9389371871948242, 0.21750831604003906, 0.11953026056289673, 0.12461000680923462, 0.14032971858978271, 0.09484249353408813, 0.14738786220550537, 0.9833363890647888, 0.06192374229431152, 0.05514401197433472, 0.07316011190414429, 0.03549540042877197, 0.01512598991394043, 0.9989707469940186, 0.062285542488098145, 0.05365985631942749, 0.03897517919540405, 0.9975900650024414, 0.17172765731811523, 0.15495073795318604, 0.2696971893310547, 0.34202951192855835, 0.3366965055465698, 0.9998489022254944, 0.9999001622200012, 0.9998829364776611, 0.9997493028640747, 0.999826192855835, 0.998832106590271, 0.9996235966682434, 0.9966010451316833, 0.9998686909675598, 0.9998617172241211, 0.9998879432678223, 0.9999067783355713, 0.9999149441719055, 0.9998218417167664, 0.145923912525177, 0.2480674386024475, 0.07905203104019165, 0.11312484741210938, 0.27109086513519287, 0.9997829794883728, 0.9987732172012329, 0.1011914610862732, 0.06511563062667847, 0.9958357214927673, 0.9910506010055542, 0.9995538592338562, 0.9992021918296814, 0.09389221668243408, 0.8156644701957703, 0.046194255352020264, 0.039093732833862305, 0.0383375883102417, 0.06555187702178955, 0.040603041648864746, 0.045346081256866455, 0.05063176155090332, 0.04573357105255127, 0.9980940222740173, 0.23017513751983643, 0.815227746963501, 0.5874573588371277, 0.49773186445236206, 0.5123669505119324, 0.46004533767700195, 0.28494036197662354, 0.08264404535293579, 0.04425686597824097, 0.14830642938613892, 0.030131399631500244, 0.029590368270874023, 0.025749802589416504, 0.029629111289978027, 0.02395021915435791, 0.01695352792739868, 0.01575338840484619, 0.016561806201934814, 0.9941508173942566, 0.07365763187408447, 0.31457704305648804, 0.11046791076660156, 0.08561211824417114, 0.18296372890472412, 0.17526459693908691, 0.05256396532058716, 0.06562286615371704, 0.16871851682662964, 0.03730195760726929, 0.09486913681030273, 0.04187202453613281, 0.04851406812667847, 0.5459773540496826, 0.12617754936218262, 0.04982924461364746, 0.06795597076416016, 0.06496238708496094, 0.19123190641403198, 0.9743932485580444, 0.04578155279159546, 0.04293692111968994, 0.09583479166030884, 0.2957710027694702, 0.06125843524932861, 0.04119938611984253, 0.040212810039520264, 0.0419580340385437, 0.04480898380279541, 0.040064334869384766, 0.04812443256378174, 0.04703801870346069, 0.047477662563323975, 0.044324517250061035, 0.04349064826965332, 0.0446094274520874, 0.9801594018936157, 0.04247099161148071, 0.049284279346466064, 0.04943317174911499, 0.9979584813117981, 0.9994187951087952, 0.9995450973510742, 0.9992880821228027, 0.9960935115814209, 0.017702221870422363, 0.016369760036468506, 0.018168210983276367, 0.008484125137329102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [07:53<1:02:08, 42.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 5628  ---  mean_dsc : 0.5721120576438673\n",
            "step : 5629  ---  loss train : [0.07112115621566772, 0.1915249228477478, 0.07060152292251587, 0.3080819249153137, 0.10023218393325806, 0.07902508974075317, 0.11954087018966675, 0.06764739751815796, 0.03773808479309082, 0.09993880987167358]\n",
            "step : 5639  ---  loss train : [0.13155776262283325, 0.5213500261306763, 0.057972490787506104, 0.16870951652526855, 0.05825263261795044, 0.05231368541717529, 0.14994657039642334, 0.2433704137802124, 0.13193267583847046, 0.11073136329650879]\n",
            "step : 5649  ---  loss train : [0.04500234127044678, 0.19457650184631348, 0.24162381887435913, 0.13716751337051392, 0.07140451669692993, 0.08669877052307129, 0.20726972818374634, 0.05262953042984009, 0.05582904815673828, 0.12716758251190186]\n",
            "step : 5659  ---  loss train : [0.1916952133178711, 0.14128702878952026, 0.10882282257080078, 0.037650346755981445, 0.13758069276809692, 0.05744737386703491, 0.29026997089385986, 0.06342869997024536, 0.0947188138961792, 0.06676721572875977]\n",
            "step : 5669  ---  loss train : [0.08662444353103638, 0.05272865295410156, 0.18473726511001587, 0.0439181923866272, 0.10964387655258179, 0.10142529010772705, 0.16147637367248535, 0.06016111373901367, 0.044836461544036865, 0.03849470615386963]\n",
            "step : 5679  ---  loss train : [0.47407859563827515, 0.06762462854385376, 0.05543261766433716, 0.3657810688018799, 0.1730431318283081, 0.09564608335494995, 0.6058517694473267, 0.3139886260032654, 0.6719992756843567, 0.3296111226081848]\n",
            "step : 5689  ---  loss train : [0.13175225257873535, 0.03708440065383911, 0.08780503273010254, 0.1531604528427124, 0.059172213077545166, 0.0932428240776062, 0.1108083724975586, 0.3923582434654236, 0.2794482111930847, 0.1259922981262207]\n",
            "step : 5699  ---  loss train : [0.18239742517471313, 0.03947758674621582, 0.19406050443649292, 0.06474947929382324, 0.048001110553741455, 0.14128321409225464, 0.15378683805465698, 0.12471657991409302, 0.06390106678009033, 0.0834229588508606]\n",
            "step : 5709  ---  loss train : [0.2913251519203186, 0.2883772850036621, 0.15531998872756958, 0.07308882474899292, 0.3078644275665283, 0.059147775173187256, 0.35171282291412354, 0.19027823209762573, 0.06635069847106934, 0.061523497104644775]\n",
            "step : 5719  ---  loss train : [0.052981555461883545, 0.05400359630584717, 0.18201887607574463, 0.22242718935012817, 0.27681851387023926, 0.15795278549194336, 0.10667848587036133, 0.07581573724746704, 0.16395562887191772, 0.20144855976104736]\n",
            "step : 5729  ---  loss train : [0.06307607889175415, 0.15270191431045532, 0.24388158321380615, 0.35175156593322754, 0.11991006135940552, 0.11402308940887451, 0.11468714475631714, 0.10762923955917358, 0.22580599784851074, 0.2799941897392273]\n",
            "step : 5739  ---  loss train : [0.14705294370651245, 0.25801366567611694, 0.09141737222671509, 0.07790869474411011, 0.05596625804901123, 0.22801047563552856, 0.26963239908218384, 0.20661067962646484, 0.14645427465438843, 0.041828274726867676]\n",
            "step : 5749  ---  loss train : [0.09900349378585815, 0.05018162727355957, 0.1924748420715332, 0.1480504870414734, 0.33870577812194824, 0.29009783267974854, 0.08281749486923218, 0.2268248200416565, 0.0985269546508789, 0.2524396777153015]\n",
            "step : 5759  ---  loss train : [0.3359825015068054, 0.06373870372772217, 0.09461253881454468, 0.2035471796989441, 0.05755901336669922, 0.4384331703186035, 0.07596516609191895, 0.23349213600158691, 0.1325475573539734, 0.08088433742523193]\n",
            "step : 5769  ---  loss train : [0.11527067422866821, 0.28656816482543945, 0.9997299909591675, 0.07244038581848145, 0.04749131202697754, 0.09669029712677002, 0.059643447399139404, 0.06328952312469482, 0.06984478235244751, 0.09681016206741333]\n",
            "step : 5779  ---  loss train : [0.05137091875076294, 0.16375142335891724, 0.05208486318588257, 0.04615873098373413, 0.08496129512786865, 0.03705441951751709, 0.09665912389755249, 0.1887088418006897, 0.27759236097335815, 0.060288310050964355]\n",
            "step : 5789  ---  loss train : [0.13329100608825684, 0.18609291315078735, 0.9991564750671387, 0.08999323844909668, 0.08981907367706299, 0.1889687180519104, 0.06351745128631592, 0.11286580562591553, 0.12771981954574585, 0.06125742197036743]\n",
            "step : 5799  ---  loss train : [0.09205812215805054, 0.10788625478744507, 0.24093091487884521, 0.060553669929504395, 0.18071657419204712, 0.05165135860443115, 0.08666813373565674, 0.11839652061462402, 0.12512195110321045, 0.05223613977432251]\n",
            "step : 5809  ---  loss train : [0.07930922508239746, 0.1359749436378479, 0.19018828868865967, 0.20071041584014893, 0.2647849917411804, 0.10445785522460938, 0.0565791130065918, 0.1140141487121582, 0.06269294023513794, 0.07329583168029785]\n",
            "step : 5819  ---  loss train : [0.19121700525283813, 0.1916903257369995, 0.4844962954521179, 0.04209339618682861, 0.22032928466796875, 0.15959912538528442, 0.13554316759109497, 0.19118326902389526, 0.14864712953567505, 0.13481533527374268]\n",
            "step : 5829  ---  loss train : [0.1918824315071106, 0.18149781227111816, 0.17885667085647583, 0.07471311092376709, 0.3228180408477783, 0.16083115339279175, 0.42296016216278076, 0.3405326008796692, 0.4424055218696594, 0.16165292263031006]\n",
            "step : 5839  ---  loss train : [0.06696575880050659, 0.11027264595031738, 0.201011061668396, 0.07182753086090088, 0.2773253917694092, 0.14078080654144287, 0.07037097215652466, 0.05578970909118652, 0.20562642812728882, 0.1080772876739502]\n",
            "step : 5849  ---  loss train : [0.1936016082763672, 0.07440876960754395, 0.0664636492729187, 0.08383023738861084, 0.10798770189285278, 0.0757749080657959, 0.9998146295547485, 0.13906121253967285, 0.19494569301605225, 0.05295729637145996]\n",
            "step : 5859  ---  loss train : [0.14960873126983643, 0.12658578157424927, 0.2163824439048767, 0.15089458227157593, 0.26971203088760376, 0.09346675872802734, 0.20623785257339478, 0.14489728212356567, 0.11438190937042236, 0.22774791717529297]\n",
            "step : 5869  ---  loss train : [0.06804531812667847, 0.15821516513824463, 0.18743973970413208, 0.18307989835739136, 0.19256502389907837, 0.15800684690475464, 0.030587196350097656, 0.1137586236000061, 0.18192678689956665, 0.15400779247283936]\n",
            "step : 5879  ---  loss train : [0.05730605125427246, 0.15921902656555176, 0.1149950623512268, 0.0939640998840332, 0.15924227237701416, 0.07747197151184082, 0.14678186178207397, 0.07555711269378662, 0.0764082670211792, 0.0865524411201477]\n",
            "step : 5889  ---  loss train : [0.07262039184570312, 0.13748657703399658, 0.05970841646194458, 0.21071970462799072, 0.07754141092300415, 0.06366413831710815, 0.09273654222488403, 0.17504924535751343, 0.051023900508880615, 0.13764715194702148]\n",
            "step : 5899  ---  loss train : [0.11796444654464722, 0.1080477237701416, 0.040974557399749756, 0.037948668003082275, 0.06558257341384888, 0.23477208614349365, 0.06640416383743286, 0.2699925899505615, 0.6323879957199097, 0.09294193983078003]\n",
            "step : 5909  ---  loss train : [0.40953266620635986, 0.08295470476150513, 0.14298081398010254, 0.2421608567237854, 0.15416806936264038, 0.0376705527305603, 0.17798691987991333, 0.053533971309661865, 0.09911888837814331, 0.2589488625526428]\n",
            "step : 5919  ---  loss train : [0.06595838069915771, 0.18580490350723267, 0.12475788593292236, 0.5441738367080688, 0.37971997261047363, 0.08337801694869995, 0.11054718494415283, 0.22745662927627563, 0.06852060556411743, 0.08604055643081665]\n",
            "step : 5929  ---  loss train : [0.07450193166732788, 0.036173462867736816, 0.060731709003448486, 0.23031532764434814, 0.11191093921661377, 0.15168243646621704, 0.28938889503479004, 0.10964888334274292, 0.12605255842208862, 0.20535272359848022]\n",
            "step : 5939  ---  loss train : [0.2688678503036499, 0.044295549392700195, 0.18822121620178223, 0.04496389627456665, 0.1136023998260498, 0.08838140964508057, 0.049855589866638184, 0.09305083751678467, 0.6008341312408447, 0.15554320812225342]\n",
            "step : 5949  ---  loss train : [0.14906692504882812, 0.2950142025947571, 0.35565781593322754, 0.06059575080871582, 0.30628079175949097, 0.059378206729888916, 0.26895302534103394, 0.0459669828414917, 0.38209229707717896, 0.03962814807891846]\n",
            "step : 5959  ---  loss train : [0.09726780652999878, 0.09299814701080322, 0.3933250904083252, 0.14710652828216553, 0.09499335289001465, 0.18110966682434082, 0.19846248626708984, 0.10403692722320557, 0.12189549207687378, 0.10819882154464722]\n",
            "step : 5969  ---  loss train : [0.11227405071258545, 0.08480751514434814, 0.05803704261779785, 0.5109367370605469, 0.11554330587387085, 0.043046534061431885, 0.05603480339050293, 0.07218438386917114, 0.09551286697387695, 0.35127007961273193]\n",
            "step : 5979  ---  loss train : [0.10215532779693604, 0.08902692794799805, 0.18452084064483643, 0.1374616026878357, 0.10613447427749634, 0.06977766752243042, 0.07160890102386475, 0.14318084716796875, 0.05480539798736572, 0.12421035766601562]\n",
            "step : 5989  ---  loss train : [0.10540652275085449, 0.0897323489189148, 0.1619640588760376, 0.1728661060333252, 0.14910227060317993, 0.06287658214569092, 0.14326804876327515, 0.10213303565979004, 0.12313222885131836, 0.10200721025466919]\n",
            "step : 5999  ---  loss train : [0.17488700151443481, 0.18853223323822021, 0.09469836950302124, 0.04297506809234619, 0.14710140228271484, 0.14770156145095825, 0.05915665626525879, 0.3979368805885315, 0.16575008630752563, 0.055786848068237305]\n",
            "step : 6009  ---  loss train : [0.06479674577713013, 0.09347057342529297, 0.08422660827636719, 0.07552671432495117, 0.36544907093048096, 0.12306082248687744, 0.13861459493637085, 0.6279975175857544, 0.07665163278579712, 0.2770063877105713]\n",
            "step : 6019  ---  loss train : [0.40183550119400024, 0.25088727474212646, 0.11330413818359375, 0.15842759609222412, 0.22587233781814575, 0.05410808324813843, 0.05342674255371094, 0.1818116307258606, 0.3441915512084961, 0.0649448037147522]\n",
            "step : 6029  ---  loss train : [0.0725775957107544, 0.20544248819351196, 0.050157785415649414, 0.08129096031188965, 0.11658579111099243, 0.11303788423538208, 0.10782384872436523, 0.12000328302383423, 0.10341089963912964, 0.09429383277893066]\n",
            "step : 6039  ---  loss train : [0.10057032108306885, 0.14638173580169678, 0.14353477954864502, 0.250140905380249, 0.14292287826538086, 0.07564228773117065, 0.18623602390289307, 0.14760112762451172, 0.10872519016265869, 0.1013420820236206]\n",
            "step : 6049  ---  loss train : [0.1703883409500122, 0.05638545751571655, 0.5709166526794434, 0.09259408712387085, 0.13643008470535278, 0.06641775369644165, 0.2076752781867981, 0.1654353141784668, 0.05846273899078369, 0.13152790069580078]\n",
            "step : 6059  ---  loss train : [0.19770199060440063, 0.1475813388824463, 0.08255511522293091, 0.11723053455352783, 0.06928551197052002, 0.07896703481674194, 0.24605512619018555, 0.11507481336593628, 0.29863405227661133, 0.24239760637283325]\n",
            "step : 6069  ---  loss train : [0.11024504899978638, 0.05439305305480957, 0.06705331802368164, 0.08091521263122559, 0.0852431058883667, 0.08289378881454468, 0.11980700492858887, 0.07506346702575684, 0.110634446144104, 0.10760378837585449]\n",
            "step : 6079  ---  loss train : [0.10425347089767456, 0.1741529107093811, 0.05976426601409912, 0.37451720237731934, 0.49403703212738037, 0.10129362344741821, 0.24790602922439575, 0.13415426015853882, 0.057012200355529785, 0.08320659399032593]\n",
            "step : 6089  ---  loss train : [0.16866636276245117, 0.08405053615570068, 0.044023215770721436, 0.08874630928039551, 0.6410276889801025, 0.05767279863357544, 0.0940808653831482, 0.07826900482177734, 0.14114534854888916, 0.06581777334213257]\n",
            "step : 6097  ---  loss train : [0.9997349977493286, 0.2707936763763428, 0.9985286593437195, 0.5597374439239502, 0.706092119216919, 0.9978501796722412, 0.0929495096206665, 0.139867901802063, 0.08294767141342163, 0.021603405475616455, 0.02165532112121582, 0.027828752994537354, 0.9981131553649902, 0.28136420249938965, 0.04581528902053833, 0.06816571950912476, 0.20033419132232666, 0.07505297660827637, 0.026896357536315918, 0.015324950218200684, 0.01683253049850464, 0.022360682487487793, 0.19551116228103638, 0.6372741460800171, 0.22203242778778076, 0.11821657419204712, 0.11957412958145142, 0.13993430137634277, 0.08505994081497192, 0.16045981645584106, 0.9833943843841553, 0.1424351930618286, 0.1405532956123352, 0.14959150552749634, 0.011991620063781738, 0.0021593570709228516, 0.9992092847824097, 0.04402804374694824, 0.026698291301727295, 0.01720261573791504, 0.8610648512840271, 0.1698036789894104, 0.13379627466201782, 0.24166101217269897, 0.31392431259155273, 0.2967105507850647, 0.9998508095741272, 0.9999082088470459, 0.9999012351036072, 0.999756395816803, 0.9998095035552979, 0.9983401298522949, 0.9994947910308838, 0.9998502135276794, 0.9998639822006226, 0.9998633861541748, 0.9999009370803833, 0.9999325275421143, 0.9999344348907471, 0.9998310208320618, 0.07794362306594849, 0.22880399227142334, 0.07958084344863892, 0.12134647369384766, 0.27772432565689087, 0.999793291091919, 0.9985296130180359, 0.11781007051467896, 0.04720276594161987, 0.9991819262504578, 0.9984804391860962, 0.9992925524711609, 0.9985769987106323, 0.02812117338180542, 0.6213104724884033, 0.03501307964324951, 0.03901636600494385, 0.04807919263839722, 0.054592132568359375, 0.0608331561088562, 0.07454788684844971, 0.08794784545898438, 0.09797781705856323, 0.9980945587158203, 0.20992958545684814, 0.7209330201148987, 0.4897106885910034, 0.4058733582496643, 0.41802656650543213, 0.3817054033279419, 0.22130441665649414, 0.08123517036437988, 0.04622805118560791, 0.1534419059753418, 0.04254794120788574, 0.033620476722717285, 0.01932358741760254, 0.012918651103973389, 0.0033446550369262695, 0.0023460984230041504, 0.001542210578918457, 0.0032761693000793457, 0.9971701502799988, 0.03470194339752197, 0.40886420011520386, 0.12051600217819214, 0.0707172155380249, 0.17754054069519043, 0.21358394622802734, 0.049472153186798096, 0.019016921520233154, 0.8276664614677429, 0.02091437578201294, 0.032373011112213135, 0.03970247507095337, 0.062460243701934814, 0.5693577527999878, 0.13457781076431274, 0.04913794994354248, 0.0697736144065857, 0.07698863744735718, 0.21392518281936646, 0.9743934273719788, 0.021264731884002686, 0.029365599155426025, 0.01844090223312378, 0.035501956939697266, 0.030141234397888184, 0.02803701162338257, 0.03851950168609619, 0.039187729358673096, 0.06407862901687622, 0.09742981195449829, 0.14158660173416138, 0.14602398872375488, 0.16008436679840088, 0.15866535902023315, 0.13388675451278687, 0.12716394662857056, 0.9956191778182983, 0.12246537208557129, 0.11708128452301025, 0.09103846549987793, 0.9979451298713684, 0.9994168281555176, 0.999544620513916, 0.9992874264717102, 0.9960931539535522, 0.004861354827880859, 0.002035975456237793, 0.00372922420501709, 0.0010493993759155273]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [08:30<58:44, 40.51s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 6097  ---  mean_dsc : 0.5788947575074548\n",
            "step : 6099  ---  loss train : [0.1816157102584839, 0.05243009328842163, 0.29209959506988525, 0.12151885032653809, 0.07401430606842041, 0.13218480348587036, 0.06967836618423462, 0.0356939435005188, 0.10251933336257935, 0.1326255202293396]\n",
            "step : 6109  ---  loss train : [0.47625917196273804, 0.05247765779495239, 0.17647546529769897, 0.05567556619644165, 0.05713391304016113, 0.15315508842468262, 0.25270354747772217, 0.1292310357093811, 0.1039419174194336, 0.07369327545166016]\n",
            "step : 6119  ---  loss train : [0.20475202798843384, 0.25012069940567017, 0.14254224300384521, 0.06204402446746826, 0.08315032720565796, 0.20712530612945557, 0.05909550189971924, 0.05827301740646362, 0.12821674346923828, 0.20234155654907227]\n",
            "step : 6129  ---  loss train : [0.14098197221755981, 0.10802340507507324, 0.06279915571212769, 0.13764792680740356, 0.06652671098709106, 0.28819817304611206, 0.06611400842666626, 0.08495676517486572, 0.06849569082260132, 0.08719158172607422]\n",
            "step : 6139  ---  loss train : [0.0563197135925293, 0.18288743495941162, 0.04193520545959473, 0.09621292352676392, 0.10192948579788208, 0.1386244297027588, 0.05539989471435547, 0.04415184259414673, 0.0338137149810791, 0.4825729727745056]\n",
            "step : 6149  ---  loss train : [0.0686960220336914, 0.05377614498138428, 0.34160470962524414, 0.1669902205467224, 0.09506887197494507, 0.5829024314880371, 0.3222581744194031, 0.6661680936813354, 0.31402093172073364, 0.13194942474365234]\n",
            "step : 6159  ---  loss train : [0.03846651315689087, 0.10273945331573486, 0.13097751140594482, 0.06323730945587158, 0.09107530117034912, 0.10882174968719482, 0.35946935415267944, 0.278153657913208, 0.10585111379623413, 0.18866300582885742]\n",
            "step : 6169  ---  loss train : [0.03651493787765503, 0.2239384651184082, 0.05696529150009155, 0.046199023723602295, 0.15486061573028564, 0.15762382745742798, 0.13518518209457397, 0.06225496530532837, 0.07681107521057129, 0.1862054467201233]\n",
            "step : 6179  ---  loss train : [0.30560189485549927, 0.15574908256530762, 0.08380264043807983, 0.3089340329170227, 0.05873763561248779, 0.33654534816741943, 0.19680893421173096, 0.05208230018615723, 0.047827839851379395, 0.056551575660705566]\n",
            "step : 6189  ---  loss train : [0.05968785285949707, 0.162514328956604, 0.15509098768234253, 0.2732914686203003, 0.15159296989440918, 0.11200553178787231, 0.08139723539352417, 0.1607016921043396, 0.1375943422317505, 0.05743575096130371]\n",
            "step : 6199  ---  loss train : [0.15698248147964478, 0.21089333295822144, 0.37275803089141846, 0.11022329330444336, 0.12814193964004517, 0.10806715488433838, 0.09795546531677246, 0.22790557146072388, 0.2804650664329529, 0.14034825563430786]\n",
            "step : 6209  ---  loss train : [0.25404489040374756, 0.09206187725067139, 0.07998424768447876, 0.05707085132598877, 0.18121248483657837, 0.23273885250091553, 0.20478588342666626, 0.1448041796684265, 0.042617857456207275, 0.09635007381439209]\n",
            "step : 6219  ---  loss train : [0.051278769969940186, 0.19540280103683472, 0.14801758527755737, 0.33844757080078125, 0.29009145498275757, 0.07870060205459595, 0.2081349492073059, 0.09738826751708984, 0.1816021203994751, 0.33495032787323]\n",
            "step : 6229  ---  loss train : [0.06518363952636719, 0.0888371467590332, 0.2031306028366089, 0.050943076610565186, 0.40450984239578247, 0.07391351461410522, 0.19366395473480225, 0.12412017583847046, 0.13154006004333496, 0.10714066028594971]\n",
            "step : 6239  ---  loss train : [0.32179391384124756, 0.9997692704200745, 0.07563942670822144, 0.05251437425613403, 0.11261051893234253, 0.1393781304359436, 0.06301766633987427, 0.07383352518081665, 0.08715683221817017, 0.050129055976867676]\n",
            "step : 6249  ---  loss train : [0.1718030571937561, 0.05370914936065674, 0.0639723539352417, 0.09155875444412231, 0.04145616292953491, 0.08908277750015259, 0.14776098728179932, 0.40700018405914307, 0.06358838081359863, 0.21690744161605835]\n",
            "step : 6259  ---  loss train : [0.17825543880462646, 0.9997318387031555, 0.08609938621520996, 0.06938666105270386, 0.12482643127441406, 0.06976139545440674, 0.1439085602760315, 0.1531130075454712, 0.06663918495178223, 0.0862506628036499]\n",
            "step : 6269  ---  loss train : [0.10782098770141602, 0.24529922008514404, 0.06361168622970581, 0.18774932622909546, 0.04862034320831299, 0.08452671766281128, 0.10663443803787231, 0.12999069690704346, 0.060964107513427734, 0.07548576593399048]\n",
            "step : 6279  ---  loss train : [0.14385831356048584, 0.15683656930923462, 0.2041056752204895, 0.29133129119873047, 0.10773545503616333, 0.06670904159545898, 0.11274147033691406, 0.06158334016799927, 0.07538467645645142, 0.17115777730941772]\n",
            "step : 6289  ---  loss train : [0.18849825859069824, 0.5011543035507202, 0.050083696842193604, 0.19265902042388916, 0.15229910612106323, 0.12119787931442261, 0.19521719217300415, 0.1478363275527954, 0.15762388706207275, 0.20594894886016846]\n",
            "step : 6299  ---  loss train : [0.18980681896209717, 0.17778021097183228, 0.08414965867996216, 0.29701095819473267, 0.16816574335098267, 0.40911126136779785, 0.34267163276672363, 0.4680996537208557, 0.14246106147766113, 0.06780827045440674]\n",
            "step : 6309  ---  loss train : [0.1132538914680481, 0.1974450945854187, 0.06609928607940674, 0.2582954168319702, 0.15091371536254883, 0.06985163688659668, 0.05215024948120117, 0.2005104422569275, 0.12098199129104614, 0.15985268354415894]\n",
            "step : 6319  ---  loss train : [0.08337479829788208, 0.06648659706115723, 0.07662910223007202, 0.11142635345458984, 0.09032624959945679, 0.9997960329055786, 0.10668790340423584, 0.19042015075683594, 0.0526885986328125, 0.14777123928070068]\n",
            "step : 6329  ---  loss train : [0.13027822971343994, 0.21428251266479492, 0.1300135850906372, 0.34677135944366455, 0.08563148975372314, 0.17601078748703003, 0.14139771461486816, 0.11898428201675415, 0.2039940357208252, 0.05857288837432861]\n",
            "step : 6339  ---  loss train : [0.14460492134094238, 0.1821689009666443, 0.21169137954711914, 0.19231384992599487, 0.15636742115020752, 0.033592939376831055, 0.11317890882492065, 0.1550191044807434, 0.20309966802597046, 0.0632593035697937]\n",
            "step : 6349  ---  loss train : [0.15243065357208252, 0.10966527462005615, 0.09196221828460693, 0.14551430940628052, 0.08376187086105347, 0.14823412895202637, 0.07270795106887817, 0.07506388425827026, 0.08214569091796875, 0.07272285223007202]\n",
            "step : 6359  ---  loss train : [0.13012921810150146, 0.05219215154647827, 0.5107914805412292, 0.07057863473892212, 0.07155954837799072, 0.10125666856765747, 0.1947353482246399, 0.09525543451309204, 0.18865108489990234, 0.2037508487701416]\n",
            "step : 6369  ---  loss train : [0.12475693225860596, 0.1058618426322937, 0.05970573425292969, 0.04752814769744873, 0.22633564472198486, 0.06787800788879395, 0.328429639339447, 0.6572785377502441, 0.08907806873321533, 0.4056386351585388]\n",
            "step : 6379  ---  loss train : [0.0903504490852356, 0.13831675052642822, 0.20980113744735718, 0.14395201206207275, 0.04479408264160156, 0.17613011598587036, 0.03791999816894531, 0.10183507204055786, 0.19853824377059937, 0.06887972354888916]\n",
            "step : 6389  ---  loss train : [0.1314220428466797, 0.11537688970565796, 0.5562028884887695, 0.38685667514801025, 0.10074067115783691, 0.09471005201339722, 0.23500001430511475, 0.06029772758483887, 0.08239579200744629, 0.08317667245864868]\n",
            "step : 6399  ---  loss train : [0.04023510217666626, 0.06149226427078247, 0.21334773302078247, 0.12128943204879761, 0.14708125591278076, 0.25620216131210327, 0.10651236772537231, 0.13433533906936646, 0.19993799924850464, 0.25614476203918457]\n",
            "step : 6409  ---  loss train : [0.04687631130218506, 0.2090623378753662, 0.05259501934051514, 0.10321789979934692, 0.09892594814300537, 0.05001598596572876, 0.10608911514282227, 0.42443692684173584, 0.168839693069458, 0.1461254358291626]\n",
            "step : 6419  ---  loss train : [0.2588767409324646, 0.3517587184906006, 0.06387430429458618, 0.3023853302001953, 0.060054004192352295, 0.28576475381851196, 0.04111182689666748, 0.39137059450149536, 0.03834736347198486, 0.10063475370407104]\n",
            "step : 6429  ---  loss train : [0.09200829267501831, 0.3892557621002197, 0.14866715669631958, 0.09159845113754272, 0.1437596082687378, 0.21519362926483154, 0.10612386465072632, 0.12334257364273071, 0.1110120415687561, 0.11949151754379272]\n",
            "step : 6439  ---  loss train : [0.08812177181243896, 0.056223511695861816, 0.5066899061203003, 0.10909408330917358, 0.039195120334625244, 0.06100773811340332, 0.06309515237808228, 0.10335409641265869, 0.3160477876663208, 0.10789448022842407]\n",
            "step : 6449  ---  loss train : [0.0920829176902771, 0.21882086992263794, 0.1411377191543579, 0.08082056045532227, 0.05930221080780029, 0.0799456238746643, 0.14387214183807373, 0.05658787488937378, 0.12815594673156738, 0.11741513013839722]\n",
            "step : 6459  ---  loss train : [0.07865923643112183, 0.17011308670043945, 0.165646493434906, 0.14321041107177734, 0.06674021482467651, 0.1132211685180664, 0.0942656397819519, 0.12232851982116699, 0.08842974901199341, 0.17092984914779663]\n",
            "step : 6469  ---  loss train : [0.17844343185424805, 0.07299536466598511, 0.04936009645462036, 0.1499219536781311, 0.1315276026725769, 0.05791926383972168, 0.3577251434326172, 0.16749423742294312, 0.04934406280517578, 0.09196656942367554]\n",
            "step : 6479  ---  loss train : [0.08601737022399902, 0.09148341417312622, 0.08447390794754028, 0.6458867788314819, 0.2841094136238098, 0.1389980912208557, 0.92182856798172, 0.07419270277023315, 0.24649333953857422, 0.4265555739402771]\n",
            "step : 6489  ---  loss train : [0.3908120393753052, 0.10058921575546265, 0.15920758247375488, 0.21752172708511353, 0.0442085862159729, 0.053051531314849854, 0.15310072898864746, 0.20645004510879517, 0.05849742889404297, 0.07296037673950195]\n",
            "step : 6499  ---  loss train : [0.19834601879119873, 0.0518527626991272, 0.0744858980178833, 0.12003612518310547, 0.0878719687461853, 0.12594997882843018, 0.12337851524353027, 0.107169508934021, 0.09175604581832886, 0.09103745222091675]\n",
            "step : 6509  ---  loss train : [0.14894568920135498, 0.14395290613174438, 0.23049867153167725, 0.14092475175857544, 0.06933367252349854, 0.19149696826934814, 0.1698969602584839, 0.1083478331565857, 0.09883952140808105, 0.17539173364639282]\n",
            "step : 6519  ---  loss train : [0.05647379159927368, 0.4167640209197998, 0.0886182188987732, 0.13747775554656982, 0.06778460741043091, 0.20736896991729736, 0.14577633142471313, 0.055436909198760986, 0.1437954306602478, 0.1874333620071411]\n",
            "step : 6529  ---  loss train : [0.1166691780090332, 0.0880616307258606, 0.11177194118499756, 0.06943070888519287, 0.07356381416320801, 0.32688748836517334, 0.1132921576499939, 0.27132850885391235, 0.22907036542892456, 0.12505584955215454]\n",
            "step : 6539  ---  loss train : [0.0686417818069458, 0.07477277517318726, 0.08280152082443237, 0.09154784679412842, 0.07611912488937378, 0.11844444274902344, 0.0776321291923523, 0.11067473888397217, 0.10113239288330078, 0.10011953115463257]\n",
            "step : 6549  ---  loss train : [0.1561560034751892, 0.051892995834350586, 0.40499579906463623, 0.48198550939559937, 0.10753202438354492, 0.25820183753967285, 0.12944656610488892, 0.051493823528289795, 0.07494860887527466, 0.17573976516723633]\n",
            "step : 6559  ---  loss train : [0.062287092208862305, 0.05290335416793823, 0.08503061532974243, 0.7221400737762451, 0.06645876169204712, 0.09386473894119263, 0.08176660537719727, 0.14577585458755493, 0.061697423458099365, 0.18335551023483276]\n",
            "step : 6566  ---  loss train : [0.9997985363006592, 0.9965183734893799, 0.9991359114646912, 0.36163532733917236, 0.584058403968811, 0.8255891799926758, 0.981993556022644, 0.05139756202697754, 0.05009198188781738, 0.040634095668792725, 0.0501062273979187, 0.07377499341964722, 0.07918053865432739, 0.26907283067703247, 0.04922676086425781, 0.06861722469329834, 0.19509440660476685, 0.07135009765625, 0.10225319862365723, 0.07026791572570801, 0.1281057596206665, 0.05757659673690796, 0.9811524748802185, 0.9688250422477722, 0.22317004203796387, 0.11636614799499512, 0.12024211883544922, 0.12772899866104126, 0.07747650146484375, 0.13308185338974, 0.9835276007652283, 0.04498863220214844, 0.05231350660324097, 0.043262779712677, 0.02197366952896118, 0.02193516492843628, 0.9994346499443054, 0.06510883569717407, 0.2197769284248352, 0.992527961730957, 0.42690807580947876, 0.15428048372268677, 0.13706755638122559, 0.23356342315673828, 0.2802066206932068, 0.29798126220703125, 0.9998359680175781, 0.9999088644981384, 0.9999089241027832, 0.9998353123664856, 0.9998471736907959, 0.9996371865272522, 0.9997155666351318, 0.9777706265449524, 0.9998751878738403, 0.9998705387115479, 0.9998874664306641, 0.9999042749404907, 0.9999117851257324, 0.9998204112052917, 0.9890750050544739, 0.2901451587677002, 0.07836741209030151, 0.1317865252494812, 0.2840809226036072, 0.9998156428337097, 0.9991517066955566, 0.9624022841453552, 0.9535556435585022, 0.9991447925567627, 0.999177873134613, 0.9994748830795288, 0.999122679233551, 0.2907722592353821, 0.8683077096939087, 0.06629616022109985, 0.12667471170425415, 0.08731019496917725, 0.8043718934059143, 0.046434223651885986, 0.05131638050079346, 0.0580754280090332, 0.03861343860626221, 0.9980958104133606, 0.17703187465667725, 0.5320440530776978, 0.49168217182159424, 0.45229750871658325, 0.4592507481575012, 0.4209238290786743, 0.2526336908340454, 0.08749538660049438, 0.04254269599914551, 0.13325828313827515, 0.026071131229400635, 0.025636732578277588, 0.023343205451965332, 0.01982194185256958, 0.020144999027252197, 0.018578171730041504, 0.020581603050231934, 0.024342477321624756, 0.9976789355278015, 0.07700031995773315, 0.35677045583724976, 0.11807632446289062, 0.09091025590896606, 0.19378149509429932, 0.39165574312210083, 0.039226233959198, 0.05069613456726074, 0.9935469031333923, 0.043597400188446045, 0.09228533506393433, 0.3145444393157959, 0.03891700506210327, 0.58404141664505, 0.12763696908950806, 0.04932093620300293, 0.0682634711265564, 0.07961344718933105, 0.20497345924377441, 0.974391520023346, 0.08577972650527954, 0.1916787028312683, 0.2629771828651428, 0.7426762580871582, 0.1082298755645752, 0.08700531721115112, 0.06416434049606323, 0.082724928855896, 0.075001060962677, 0.08522903919219971, 0.09222668409347534, 0.09107154607772827, 0.09765392541885376, 0.09963703155517578, 0.08340519666671753, 0.08414298295974731, 0.9981998205184937, 0.10024571418762207, 0.07312411069869995, 0.06651866436004639, 0.9979662895202637, 0.9994221329689026, 0.9995468258857727, 0.9992889761924744, 0.9960940480232239, 0.022272706031799316, 0.026619017124176025, 0.02647995948791504, 0.010869264602661133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 14/100 [09:09<57:41, 40.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 6566  ---  mean_dsc : 0.579592790559864\n",
            "step : 6569  ---  loss train : [0.07389646768569946, 0.23246705532073975, 0.1540529727935791, 0.07269388437271118, 0.11370563507080078, 0.06535625457763672, 0.042446672916412354, 0.10072433948516846, 0.1365993618965149, 0.4990541338920593]\n",
            "step : 6579  ---  loss train : [0.07967507839202881, 0.17971831560134888, 0.059004902839660645, 0.043572306632995605, 0.13923346996307373, 0.20630502700805664, 0.12795722484588623, 0.103457510471344, 0.0444449782371521, 0.18737566471099854]\n",
            "step : 6589  ---  loss train : [0.2341090440750122, 0.14066839218139648, 0.0522083044052124, 0.0853877067565918, 0.21444159746170044, 0.04787415266036987, 0.05362296104431152, 0.12428438663482666, 0.1818954348564148, 0.14489775896072388]\n",
            "step : 6599  ---  loss train : [0.11364775896072388, 0.039701998233795166, 0.14844197034835815, 0.055369019508361816, 0.25470399856567383, 0.06160002946853638, 0.10137736797332764, 0.07473975419998169, 0.08594423532485962, 0.05357921123504639]\n",
            "step : 6609  ---  loss train : [0.19920259714126587, 0.04166048765182495, 0.09359866380691528, 0.0970810055732727, 0.14039546251296997, 0.04851740598678589, 0.07682305574417114, 0.032883524894714355, 0.4804723262786865, 0.06483948230743408]\n",
            "step : 6619  ---  loss train : [0.05389028787612915, 0.356892466545105, 0.16525709629058838, 0.09190016984939575, 0.4040703773498535, 0.34160590171813965, 0.6246401071548462, 0.3062968850135803, 0.13303399085998535, 0.04008561372756958]\n",
            "step : 6629  ---  loss train : [0.10476195812225342, 0.1151190996170044, 0.06972122192382812, 0.09776163101196289, 0.11254233121871948, 0.19794684648513794, 0.2424260377883911, 0.1220439076423645, 0.16006213426589966, 0.040555357933044434]\n",
            "step : 6639  ---  loss train : [0.23397618532180786, 0.06243753433227539, 0.0476955771446228, 0.18257957696914673, 0.13275235891342163, 0.13606613874435425, 0.0653611421585083, 0.08762550354003906, 0.1347225308418274, 0.3375057578086853]\n",
            "step : 6649  ---  loss train : [0.1080593466758728, 0.08212417364120483, 0.3225743770599365, 0.133547842502594, 0.3461315631866455, 0.18583089113235474, 0.057432591915130615, 0.05512148141860962, 0.067374587059021, 0.05583125352859497]\n",
            "step : 6659  ---  loss train : [0.14890342950820923, 0.16838139295578003, 0.27419328689575195, 0.08265376091003418, 0.10695761442184448, 0.07030290365219116, 0.14908742904663086, 0.13502132892608643, 0.05962759256362915, 0.1492118239402771]\n",
            "step : 6669  ---  loss train : [0.3582448959350586, 0.3616967797279358, 0.12062859535217285, 0.0986717939376831, 0.08506059646606445, 0.1138831377029419, 0.23383700847625732, 0.28710663318634033, 0.1606479287147522, 0.24936360120773315]\n",
            "step : 6679  ---  loss train : [0.10052251815795898, 0.11013329029083252, 0.09290075302124023, 0.1886858344078064, 0.16731619834899902, 0.20694589614868164, 0.1548725962638855, 0.04451388120651245, 0.09269684553146362, 0.05518949031829834]\n",
            "step : 6689  ---  loss train : [0.2124011516571045, 0.14601314067840576, 0.34293025732040405, 0.28980696201324463, 0.08632594347000122, 0.25103306770324707, 0.10322457551956177, 0.1864091157913208, 0.35199815034866333, 0.06413930654525757]\n",
            "step : 6699  ---  loss train : [0.08346748352050781, 0.21249139308929443, 0.04721343517303467, 0.3593601584434509, 0.07292228937149048, 0.19795942306518555, 0.12253087759017944, 0.07660865783691406, 0.13640427589416504, 0.37476879358291626]\n",
            "step : 6709  ---  loss train : [0.999788224697113, 0.07161134481430054, 0.04375195503234863, 0.08632922172546387, 0.07234042882919312, 0.08238548040390015, 0.08223700523376465, 0.0861891508102417, 0.049969375133514404, 0.1652122139930725]\n",
            "step : 6719  ---  loss train : [0.05391603708267212, 0.046422600746154785, 0.16633719205856323, 0.03573966026306152, 0.08125436305999756, 0.1568213701248169, 0.22360503673553467, 0.060762882232666016, 0.12398844957351685, 0.17524951696395874]\n",
            "step : 6729  ---  loss train : [0.9998149275779724, 0.07783728837966919, 0.06742459535598755, 0.2184680700302124, 0.0540844202041626, 0.11892598867416382, 0.08808845281600952, 0.06417912244796753, 0.09543901681900024, 0.1037185788154602]\n",
            "step : 6739  ---  loss train : [0.22825461626052856, 0.06783592700958252, 0.16199451684951782, 0.06832349300384521, 0.07767432928085327, 0.09457665681838989, 0.13296163082122803, 0.05850344896316528, 0.08017361164093018, 0.13274908065795898]\n",
            "step : 6749  ---  loss train : [0.1048927903175354, 0.1902097463607788, 0.26120835542678833, 0.08566170930862427, 0.05813068151473999, 0.0928804874420166, 0.0571972131729126, 0.07201486825942993, 0.17955350875854492, 0.19509518146514893]\n",
            "step : 6759  ---  loss train : [0.49573034048080444, 0.04244530200958252, 0.2184123396873474, 0.142350435256958, 0.11730492115020752, 0.16904520988464355, 0.14108532667160034, 0.1438387632369995, 0.19972866773605347, 0.1664305329322815]\n",
            "step : 6769  ---  loss train : [0.1671338677406311, 0.07464700937271118, 0.2689521312713623, 0.1653153896331787, 0.3571891188621521, 0.34584707021713257, 0.4341251254081726, 0.1209707260131836, 0.06569027900695801, 0.16349762678146362]\n",
            "step : 6779  ---  loss train : [0.17429542541503906, 0.061488986015319824, 0.25193142890930176, 0.12645292282104492, 0.07160228490829468, 0.046222805976867676, 0.17412304878234863, 0.09138315916061401, 0.1566859483718872, 0.06327944993972778]\n",
            "step : 6789  ---  loss train : [0.07098764181137085, 0.06752407550811768, 0.12031716108322144, 0.08890306949615479, 0.9764695763587952, 0.16854268312454224, 0.218153715133667, 0.06234937906265259, 0.12261289358139038, 0.1373124122619629]\n",
            "step : 6799  ---  loss train : [0.21847951412200928, 0.13478249311447144, 0.3325687646865845, 0.11594295501708984, 0.09365314245223999, 0.1456974744796753, 0.11287909746170044, 0.18343806266784668, 0.05539911985397339, 0.15169131755828857]\n",
            "step : 6809  ---  loss train : [0.16839361190795898, 0.18699002265930176, 0.19108128547668457, 0.146298348903656, 0.03304511308670044, 0.08881640434265137, 0.15909689664840698, 0.1875690221786499, 0.048670172691345215, 0.14798349142074585]\n",
            "step : 6819  ---  loss train : [0.10110503435134888, 0.0851251482963562, 0.1208571195602417, 0.07870620489120483, 0.16639453172683716, 0.09610700607299805, 0.05028200149536133, 0.08352315425872803, 0.07433086633682251, 0.12810885906219482]\n",
            "step : 6829  ---  loss train : [0.05399388074874878, 0.25094902515411377, 0.07742440700531006, 0.0602719783782959, 0.10040926933288574, 0.17572778463363647, 0.04810655117034912, 0.14328521490097046, 0.11445939540863037, 0.11109286546707153]\n",
            "step : 6839  ---  loss train : [0.0469091534614563, 0.03595435619354248, 0.06112748384475708, 0.22669947147369385, 0.06641483306884766, 0.3148254156112671, 0.47867095470428467, 0.10252082347869873, 0.36756962537765503, 0.08241605758666992]\n",
            "step : 6849  ---  loss train : [0.16775095462799072, 0.2047237753868103, 0.09950357675552368, 0.03803044557571411, 0.1858113408088684, 0.052346229553222656, 0.06294548511505127, 0.24351632595062256, 0.06719028949737549, 0.16473722457885742]\n",
            "step : 6859  ---  loss train : [0.1347893476486206, 0.29678618907928467, 0.22705078125, 0.10402625799179077, 0.09940195083618164, 0.22406768798828125, 0.042568981647491455, 0.14445006847381592, 0.06682246923446655, 0.040958404541015625]\n",
            "step : 6869  ---  loss train : [0.08192622661590576, 0.2342665195465088, 0.09850740432739258, 0.14794325828552246, 0.25567907094955444, 0.15054535865783691, 0.15004324913024902, 0.19973242282867432, 0.2315971851348877, 0.0505596399307251]\n",
            "step : 6879  ---  loss train : [0.20446288585662842, 0.05076676607131958, 0.10980343818664551, 0.08906787633895874, 0.050361573696136475, 0.11686015129089355, 0.39521849155426025, 0.18501359224319458, 0.1623033881187439, 0.3620418906211853]\n",
            "step : 6889  ---  loss train : [0.3479383587837219, 0.05416327714920044, 0.3760223984718323, 0.05897706747055054, 0.2540697455406189, 0.04881119728088379, 0.34893107414245605, 0.03654944896697998, 0.12319225072860718, 0.11172884702682495]\n",
            "step : 6899  ---  loss train : [0.342906653881073, 0.1938965916633606, 0.08785605430603027, 0.13207894563674927, 0.2188848853111267, 0.10512244701385498, 0.12744510173797607, 0.1075218915939331, 0.1269049048423767, 0.08246040344238281]\n",
            "step : 6909  ---  loss train : [0.0949820876121521, 0.4933299422264099, 0.11352992057800293, 0.04222309589385986, 0.05119091272354126, 0.05692648887634277, 0.10904282331466675, 0.4240681529045105, 0.13178056478500366, 0.12095290422439575]\n",
            "step : 6919  ---  loss train : [0.19223695993423462, 0.15847790241241455, 0.10107356309890747, 0.0931350588798523, 0.0775918960571289, 0.11384201049804688, 0.05478423833847046, 0.10269290208816528, 0.13098901510238647, 0.061363041400909424]\n",
            "step : 6929  ---  loss train : [0.17050284147262573, 0.1745327115058899, 0.13990283012390137, 0.08197122812271118, 0.11283791065216064, 0.10609006881713867, 0.10954272747039795, 0.07904994487762451, 0.1769300103187561, 0.18262428045272827]\n",
            "step : 6939  ---  loss train : [0.07554769515991211, 0.0908663272857666, 0.15519475936889648, 0.10080486536026001, 0.06370270252227783, 0.4040939211845398, 0.16323703527450562, 0.04746091365814209, 0.05125921964645386, 0.08873319625854492]\n",
            "step : 6949  ---  loss train : [0.08274692296981812, 0.08915084600448608, 0.36331838369369507, 0.10835695266723633, 0.14653462171554565, 0.6984840631484985, 0.06549853086471558, 0.1824873685836792, 0.385722279548645, 0.23032033443450928]\n",
            "step : 6959  ---  loss train : [0.10217809677124023, 0.15198487043380737, 0.22839248180389404, 0.042082250118255615, 0.054190099239349365, 0.18907028436660767, 0.3031076192855835, 0.07283329963684082, 0.0690460205078125, 0.17878419160842896]\n",
            "step : 6969  ---  loss train : [0.04088717699050903, 0.06419920921325684, 0.11301296949386597, 0.11196529865264893, 0.12513482570648193, 0.11676663160324097, 0.08314651250839233, 0.10112595558166504, 0.09855306148529053, 0.1408700942993164]\n",
            "step : 6979  ---  loss train : [0.13157641887664795, 0.2670114040374756, 0.15198546648025513, 0.07211577892303467, 0.1819280982017517, 0.12558883428573608, 0.11474609375, 0.10662573575973511, 0.17029732465744019, 0.05529344081878662]\n",
            "step : 6989  ---  loss train : [0.550435483455658, 0.08775526285171509, 0.1345289945602417, 0.055523812770843506, 0.19114935398101807, 0.12713319063186646, 0.05876654386520386, 0.1331976056098938, 0.27548104524612427, 0.15988796949386597]\n",
            "step : 6999  ---  loss train : [0.07362568378448486, 0.09188985824584961, 0.09235912561416626, 0.11281967163085938, 0.2921599745750427, 0.10858511924743652, 0.29298579692840576, 0.21261382102966309, 0.1225847601890564, 0.05647820234298706]\n",
            "step : 7009  ---  loss train : [0.07331514358520508, 0.07906085252761841, 0.07883083820343018, 0.07673382759094238, 0.1239783763885498, 0.08373099565505981, 0.1167212724685669, 0.10825800895690918, 0.11133599281311035, 0.13912922143936157]\n",
            "step : 7019  ---  loss train : [0.054515540599823, 0.3580749034881592, 0.47603535652160645, 0.08491843938827515, 0.23409324884414673, 0.17806845903396606, 0.084381103515625, 0.0866386890411377, 0.1789264678955078, 0.0754198431968689]\n",
            "step : 7029  ---  loss train : [0.04609107971191406, 0.08570796251296997, 0.7124446034431458, 0.07043051719665527, 0.09102088212966919, 0.07341247797012329, 0.14019817113876343, 0.06355071067810059, 0.20741158723831177, 0.05500918626785278]\n",
            "step : 7035  ---  loss train : [0.999898374080658, 0.998354971408844, 0.9986764788627625, 0.2985554337501526, 0.5594384670257568, 0.8460851311683655, 0.998121976852417, 0.954656720161438, 0.04396474361419678, 0.37052440643310547, 0.25264298915863037, 0.1718655228614807, 0.13944971561431885, 0.25937122106552124, 0.056838810443878174, 0.06746548414230347, 0.12122058868408203, 0.08200842142105103, 0.11754578351974487, 0.6821550130844116, 0.8113579750061035, 0.35220104455947876, 0.971838653087616, 0.990688681602478, 0.2771414518356323, 0.1308145523071289, 0.12855929136276245, 0.14150649309158325, 0.09300500154495239, 0.15058088302612305, 0.9842990040779114, 0.026853561401367188, 0.038934409618377686, 0.08602118492126465, 0.045728445053100586, 0.022409558296203613, 0.9996572732925415, 0.23697221279144287, 0.15751713514328003, 0.9443424940109253, 0.42458122968673706, 0.15323615074157715, 0.1503918170928955, 0.23960691690444946, 0.3052390217781067, 0.3568369150161743, 0.9998742938041687, 0.9999220967292786, 0.9999147057533264, 0.9998108148574829, 0.9998393058776855, 0.9993507862091064, 0.9996992349624634, 0.9787707924842834, 0.9998813271522522, 0.999874472618103, 0.9999058246612549, 0.999929666519165, 0.999933123588562, 0.9998270869255066, 0.9359162449836731, 0.3157837390899658, 0.08918088674545288, 0.13810670375823975, 0.3054468631744385, 0.9998371005058289, 0.9990423321723938, 0.0918811559677124, 0.7559115290641785, 0.9993281364440918, 0.9992693066596985, 0.9994141459465027, 0.998465359210968, 0.9592547416687012, 0.8405178189277649, 0.5381437540054321, 0.5528730154037476, 0.13028556108474731, 0.9528920650482178, 0.06633013486862183, 0.5014572143554688, 0.7379028797149658, 0.06973248720169067, 0.9979221224784851, 0.13110893964767456, 0.3038191795349121, 0.22036176919937134, 0.17986291646957397, 0.21506989002227783, 0.17349523305892944, 0.097575843334198, 0.042914748191833496, 0.048806190490722656, 0.13278329372406006, 0.025692224502563477, 0.05519682168960571, 0.03576844930648804, 0.056483685970306396, 0.05777961015701294, 0.028555691242218018, 0.04934060573577881, 0.020222961902618408, 0.9989397525787354, 0.9979531764984131, 0.504118800163269, 0.1452552080154419, 0.13979637622833252, 0.23073804378509521, 0.36997097730636597, 0.1125643253326416, 0.11161690950393677, 0.781583845615387, 0.14965569972991943, 0.8954320549964905, 0.13723862171173096, 0.4521159529685974, 0.6578971147537231, 0.14079856872558594, 0.05516707897186279, 0.07555723190307617, 0.08319205045700073, 0.20510083436965942, 0.9743780493736267, 0.2869337201118469, 0.7708014249801636, 0.942082941532135, 0.9777520298957825, 0.6384242177009583, 0.2731966972351074, 0.09371936321258545, 0.5774204730987549, 0.3615947365760803, 0.11298269033432007, 0.09541600942611694, 0.06313222646713257, 0.06773167848587036, 0.04504287242889404, 0.058350443840026855, 0.9112178087234497, 0.9989496469497681, 0.03758281469345093, 0.02806723117828369, 0.03803730010986328, 0.9977622032165527, 0.9994131326675415, 0.9995407462120056, 0.9992867112159729, 0.9960933327674866, 0.03477489948272705, 0.0313640832901001, 0.022283732891082764, 0.008053481578826904]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [09:46<55:36, 39.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 7035  ---  mean_dsc : 0.5867145991078058\n",
            "step : 7039  ---  loss train : [0.3936939835548401, 0.08289170265197754, 0.0886772871017456, 0.11810052394866943, 0.06602579355239868, 0.043610572814941406, 0.10956060886383057, 0.14026254415512085, 0.4951654076576233, 0.05733388662338257]\n",
            "step : 7049  ---  loss train : [0.1788729429244995, 0.06163632869720459, 0.04411214590072632, 0.13720488548278809, 0.184170663356781, 0.12564468383789062, 0.1069837212562561, 0.0404699444770813, 0.18828368186950684, 0.23354768753051758]\n",
            "step : 7059  ---  loss train : [0.13987410068511963, 0.052322328090667725, 0.08219164609909058, 0.21526604890823364, 0.04668247699737549, 0.051937758922576904, 0.12018966674804688, 0.16488075256347656, 0.125444233417511, 0.10305815935134888]\n",
            "step : 7069  ---  loss train : [0.03445780277252197, 0.12396562099456787, 0.05853307247161865, 0.2591465711593628, 0.05948007106781006, 0.09689593315124512, 0.06322884559631348, 0.08453381061553955, 0.05184483528137207, 0.19110310077667236]\n",
            "step : 7079  ---  loss train : [0.04146087169647217, 0.10017383098602295, 0.09061068296432495, 0.11270248889923096, 0.04660230875015259, 0.04320937395095825, 0.04423791170120239, 0.468416690826416, 0.060935378074645996, 0.050181031227111816]\n",
            "step : 7089  ---  loss train : [0.3881687521934509, 0.16671735048294067, 0.09388434886932373, 0.4021903872489929, 0.3518185019493103, 0.5599467158317566, 0.27935343980789185, 0.12365198135375977, 0.03923124074935913, 0.10276955366134644]\n",
            "step : 7099  ---  loss train : [0.10006439685821533, 0.06430959701538086, 0.09103095531463623, 0.11573034524917603, 0.1939876675605774, 0.18472504615783691, 0.13665932416915894, 0.16875243186950684, 0.039703428745269775, 0.2341468334197998]\n",
            "step : 7109  ---  loss train : [0.05289006233215332, 0.0465472936630249, 0.16223835945129395, 0.13287395238876343, 0.13258808851242065, 0.07096469402313232, 0.0975084900856018, 0.12528640031814575, 0.2411230206489563, 0.10752123594284058]\n",
            "step : 7119  ---  loss train : [0.08164834976196289, 0.31405699253082275, 0.07053971290588379, 0.34770238399505615, 0.25310516357421875, 0.05870330333709717, 0.057695865631103516, 0.057746708393096924, 0.058725833892822266, 0.1259833574295044]\n",
            "step : 7129  ---  loss train : [0.12658780813217163, 0.26548850536346436, 0.08689051866531372, 0.11183518171310425, 0.0744938850402832, 0.15342217683792114, 0.13051128387451172, 0.058242619037628174, 0.14532750844955444, 0.24927693605422974]\n",
            "step : 7139  ---  loss train : [0.22230005264282227, 0.11100327968597412, 0.10518324375152588, 0.09556573629379272, 0.09923464059829712, 0.23516309261322021, 0.2852358818054199, 0.13226574659347534, 0.25174808502197266, 0.09241503477096558]\n",
            "step : 7149  ---  loss train : [0.0822458267211914, 0.08684539794921875, 0.18672198057174683, 0.18008792400360107, 0.20403814315795898, 0.14920926094055176, 0.03891676664352417, 0.08965152502059937, 0.05456966161727905, 0.17735731601715088]\n",
            "step : 7159  ---  loss train : [0.14481252431869507, 0.3546411395072937, 0.287924587726593, 0.08013415336608887, 0.23778218030929565, 0.10194027423858643, 0.18062150478363037, 0.3193444609642029, 0.060140252113342285, 0.08640116453170776]\n",
            "step : 7169  ---  loss train : [0.19819879531860352, 0.047863662242889404, 0.385505735874176, 0.06540048122406006, 0.18448472023010254, 0.12710130214691162, 0.07347887754440308, 0.10926711559295654, 0.29852229356765747, 0.9996550679206848]\n",
            "step : 7179  ---  loss train : [0.07167351245880127, 0.050698041915893555, 0.08291196823120117, 0.0637403130531311, 0.06319206953048706, 0.07197219133377075, 0.061830878257751465, 0.04917997121810913, 0.16361117362976074, 0.049883902072906494]\n",
            "step : 7189  ---  loss train : [0.04568558931350708, 0.09035360813140869, 0.03497499227523804, 0.09607428312301636, 0.15863323211669922, 0.19731801748275757, 0.050755321979522705, 0.1164395809173584, 0.1656000018119812, 0.9996050000190735]\n",
            "step : 7199  ---  loss train : [0.08091497421264648, 0.07026815414428711, 0.13419026136398315, 0.05340760946273804, 0.12452065944671631, 0.07229894399642944, 0.06329703330993652, 0.09057372808456421, 0.10735583305358887, 0.24132633209228516]\n",
            "step : 7209  ---  loss train : [0.06361722946166992, 0.1657794713973999, 0.061034321784973145, 0.08167678117752075, 0.10286062955856323, 0.12232476472854614, 0.06044042110443115, 0.07512778043746948, 0.13099461793899536, 0.161693274974823]\n",
            "step : 7219  ---  loss train : [0.19604307413101196, 0.259803831577301, 0.07407575845718384, 0.055024564266204834, 0.08415234088897705, 0.05696779489517212, 0.06697195768356323, 0.17942357063293457, 0.1801707148551941, 0.509924054145813]\n",
            "step : 7229  ---  loss train : [0.043050527572631836, 0.2179396152496338, 0.13265252113342285, 0.1189536452293396, 0.15267479419708252, 0.13896650075912476, 0.12321668863296509, 0.19426292181015015, 0.16476744413375854, 0.1635628342628479]\n",
            "step : 7239  ---  loss train : [0.06976252794265747, 0.20995843410491943, 0.16379600763320923, 0.32366031408309937, 0.34363269805908203, 0.43210601806640625, 0.12254738807678223, 0.06857156753540039, 0.1506127119064331, 0.22681540250778198]\n",
            "step : 7249  ---  loss train : [0.06464165449142456, 0.2526194453239441, 0.13002407550811768, 0.06759291887283325, 0.0517769455909729, 0.18366920948028564, 0.10453838109970093, 0.17173421382904053, 0.06820154190063477, 0.06947523355484009]\n",
            "step : 7259  ---  loss train : [0.06517839431762695, 0.1140027642250061, 0.04886442422866821, 0.9996960759162903, 0.11921614408493042, 0.19847387075424194, 0.05175626277923584, 0.16187328100204468, 0.1466401219367981, 0.22260147333145142]\n",
            "step : 7269  ---  loss train : [0.1301552653312683, 0.30880308151245117, 0.07155019044876099, 0.1805436611175537, 0.11791872978210449, 0.11599236726760864, 0.2084643840789795, 0.05304861068725586, 0.15331584215164185, 0.18522536754608154]\n",
            "step : 7279  ---  loss train : [0.1646045446395874, 0.19180220365524292, 0.15837877988815308, 0.02904045581817627, 0.08123928308486938, 0.16190403699874878, 0.14481651782989502, 0.042796313762664795, 0.13342607021331787, 0.10346370935440063]\n",
            "step : 7289  ---  loss train : [0.09181046485900879, 0.13054078817367554, 0.07103943824768066, 0.13686776161193848, 0.08946001529693604, 0.05178523063659668, 0.08999049663543701, 0.07694488763809204, 0.13642048835754395, 0.05419725179672241]\n",
            "step : 7299  ---  loss train : [0.16517210006713867, 0.07120978832244873, 0.05587559938430786, 0.08563411235809326, 0.17904329299926758, 0.049655377864837646, 0.14556366205215454, 0.11655014753341675, 0.10760504007339478, 0.032573819160461426]\n",
            "step : 7309  ---  loss train : [0.03832966089248657, 0.06723058223724365, 0.2205459475517273, 0.051246464252471924, 0.290733277797699, 0.6321631669998169, 0.08103197813034058, 0.41574013233184814, 0.07704895734786987, 0.21707004308700562]\n",
            "step : 7319  ---  loss train : [0.1524181365966797, 0.15233200788497925, 0.0349421501159668, 0.18273121118545532, 0.05586522817611694, 0.10136520862579346, 0.2652314305305481, 0.06947547197341919, 0.19703495502471924, 0.11220502853393555]\n",
            "step : 7329  ---  loss train : [0.49498826265335083, 0.3772314190864563, 0.06767439842224121, 0.09098666906356812, 0.22727364301681519, 0.07194656133651733, 0.06675291061401367, 0.06919795274734497, 0.03491276502609253, 0.061476826667785645]\n",
            "step : 7339  ---  loss train : [0.22742581367492676, 0.08413523435592651, 0.14300251007080078, 0.3014451265335083, 0.10518538951873779, 0.12666720151901245, 0.21699655055999756, 0.32218998670578003, 0.03476768732070923, 0.1658422350883484]\n",
            "step : 7349  ---  loss train : [0.0414699912071228, 0.11174958944320679, 0.08423876762390137, 0.051694273948669434, 0.09484189748764038, 0.551139235496521, 0.1452895998954773, 0.137437641620636, 0.26740193367004395, 0.35444945096969604]\n",
            "step : 7359  ---  loss train : [0.06190133094787598, 0.25071150064468384, 0.05533069372177124, 0.2722622752189636, 0.04798328876495361, 0.3633044362068176, 0.0409395694732666, 0.11909890174865723, 0.09044116735458374, 0.4296303391456604]\n",
            "step : 7369  ---  loss train : [0.1438550353050232, 0.09480559825897217, 0.1352185606956482, 0.19402283430099487, 0.09827947616577148, 0.09060776233673096, 0.10331881046295166, 0.12280356884002686, 0.08732181787490845, 0.05992823839187622]\n",
            "step : 7379  ---  loss train : [0.5061101913452148, 0.10847270488739014, 0.04091578722000122, 0.05667674541473389, 0.05868047475814819, 0.09406596422195435, 0.30371540784835815, 0.11246156692504883, 0.08398634195327759, 0.17442047595977783]\n",
            "step : 7389  ---  loss train : [0.14210885763168335, 0.080699622631073, 0.0751638412475586, 0.06975686550140381, 0.12423884868621826, 0.0504223108291626, 0.11137652397155762, 0.10917985439300537, 0.0634412169456482, 0.169089674949646]\n",
            "step : 7399  ---  loss train : [0.16736018657684326, 0.14437854290008545, 0.060851454734802246, 0.10678446292877197, 0.09873628616333008, 0.1084941029548645, 0.054526686668395996, 0.17083722352981567, 0.18199151754379272, 0.08191084861755371]\n",
            "step : 7409  ---  loss train : [0.07157295942306519, 0.14319556951522827, 0.09638875722885132, 0.05330532789230347, 0.06632077693939209, 0.15939587354660034, 0.053693294525146484, 0.05408740043640137, 0.08704227209091187, 0.07568615674972534]\n",
            "step : 7419  ---  loss train : [0.07053768634796143, 0.27102190256118774, 0.11201274394989014, 0.14233148097991943, 0.6474161148071289, 0.05629682540893555, 0.16805559396743774, 0.3954828977584839, 0.23196852207183838, 0.09883958101272583]\n",
            "step : 7429  ---  loss train : [0.15343308448791504, 0.23922300338745117, 0.04200631380081177, 0.05533808469772339, 0.1990087628364563, 0.3211650848388672, 0.08150053024291992, 0.06774526834487915, 0.19167661666870117, 0.04306185245513916]\n",
            "step : 7439  ---  loss train : [0.0649605393409729, 0.10860127210617065, 0.09964066743850708, 0.11385011672973633, 0.11843657493591309, 0.08529633283615112, 0.09546554088592529, 0.09165215492248535, 0.14249145984649658, 0.13384699821472168]\n",
            "step : 7449  ---  loss train : [0.25139129161834717, 0.14517152309417725, 0.07291966676712036, 0.1790667176246643, 0.12239903211593628, 0.10348188877105713, 0.09590893983840942, 0.1695442795753479, 0.05602794885635376, 0.506203293800354]\n",
            "step : 7459  ---  loss train : [0.09067779779434204, 0.12578928470611572, 0.054558396339416504, 0.1576901078224182, 0.10760068893432617, 0.05713528394699097, 0.1325780153274536, 0.19076836109161377, 0.08675771951675415, 0.08577895164489746]\n",
            "step : 7469  ---  loss train : [0.10318499803543091, 0.07015573978424072, 0.06685316562652588, 0.15920835733413696, 0.11080420017242432, 0.25442272424697876, 0.19531726837158203, 0.11641114950180054, 0.06322956085205078, 0.079032301902771]\n",
            "step : 7479  ---  loss train : [0.07881772518157959, 0.0845189094543457, 0.07605791091918945, 0.11430734395980835, 0.06441479921340942, 0.07605904340744019, 0.1040087342262268, 0.0688408613204956, 0.16771811246871948, 0.05808150768280029]\n",
            "step : 7489  ---  loss train : [0.311426043510437, 0.4672936797142029, 0.08619654178619385, 0.18428701162338257, 0.1320090889930725, 0.04399394989013672, 0.07044649124145508, 0.1576130986213684, 0.09207016229629517, 0.0571591854095459]\n",
            "step : 7499  ---  loss train : [0.08197557926177979, 0.550982654094696, 0.06648951768875122, 0.08984643220901489, 0.061778485774993896, 0.13867896795272827, 0.05788099765777588, 0.17109113931655884, 0.04283660650253296, 0.15071219205856323]\n",
            "step : 7504  ---  loss train : [0.9998048543930054, 0.9760591983795166, 0.9989182353019714, 0.5107986330986023, 0.8479859232902527, 0.9978455305099487, 0.9866944551467896, 0.04403841495513916, 0.029900550842285156, 0.03377801179885864, 0.08117657899856567, 0.08642959594726562, 0.08738476037979126, 0.32983094453811646, 0.07963734865188599, 0.08921879529953003, 0.19702094793319702, 0.044681668281555176, 0.08967339992523193, 0.07422840595245361, 0.060850322246551514, 0.049787700176239014, 0.9740793704986572, 0.8408229351043701, 0.2151421308517456, 0.0937318205833435, 0.1042143702507019, 0.11502176523208618, 0.0752519965171814, 0.13975787162780762, 0.9833369255065918, 0.0224301815032959, 0.03435403108596802, 0.11316734552383423, 0.0620877742767334, 0.019842863082885742, 0.9995390772819519, 0.09098654985427856, 0.9771304726600647, 0.9608731269836426, 0.9936555027961731, 0.1876983642578125, 0.14363646507263184, 0.26086652278900146, 0.3456990122795105, 0.33243000507354736, 0.9998264908790588, 0.9998931884765625, 0.9998385906219482, 0.9996098875999451, 0.9996764063835144, 0.9983330368995667, 0.9995859861373901, 0.9912065267562866, 0.999851644039154, 0.9998299479484558, 0.9998670816421509, 0.9998939037322998, 0.9999135732650757, 0.9998196363449097, 0.4850274324417114, 0.20047467947006226, 0.07161659002304077, 0.1098511815071106, 0.2678842544555664, 0.9997864961624146, 0.9981049299240112, 0.13417774438858032, 0.13758927583694458, 0.9986791014671326, 0.99554443359375, 0.9992793202400208, 0.997809112071991, 0.4087127447128296, 0.40568476915359497, 0.07497590780258179, 0.06877130270004272, 0.06079667806625366, 0.8187721967697144, 0.08211570978164673, 0.0662081241607666, 0.08148884773254395, 0.06739038228988647, 0.9980859160423279, 0.18832087516784668, 0.4370924234390259, 0.27800124883651733, 0.23449844121932983, 0.24188411235809326, 0.2077775001525879, 0.11600875854492188, 0.06347596645355225, 0.04467242956161499, 0.16486674547195435, 0.021553635597229004, 0.03456699848175049, 0.02824711799621582, 0.032678306102752686, 0.02792888879776001, 0.017827093601226807, 0.021363496780395508, 0.020776987075805664, 0.998732328414917, 0.97881680727005, 0.44891172647476196, 0.12301802635192871, 0.0731305480003357, 0.18191123008728027, 0.3304324150085449, 0.0828392505645752, 0.0906670093536377, 0.048836469650268555, 0.06028091907501221, 0.09544223546981812, 0.07774907350540161, 0.06667464971542358, 0.505621075630188, 0.11637067794799805, 0.04867386817932129, 0.06391078233718872, 0.05220705270767212, 0.17518872022628784, 0.9743986129760742, 0.06689304113388062, 0.0700802206993103, 0.09832072257995605, 0.08200299739837646, 0.07388049364089966, 0.0726972222328186, 0.07689160108566284, 0.11439275741577148, 0.08151769638061523, 0.07379806041717529, 0.06542164087295532, 0.08358794450759888, 0.06732362508773804, 0.05406421422958374, 0.12470483779907227, 0.04129546880722046, 0.9988342523574829, 0.03996598720550537, 0.03362417221069336, 0.042375922203063965, 0.9978992938995361, 0.9994145035743713, 0.9995427131652832, 0.9992883801460266, 0.9960936903953552, 0.021412432193756104, 0.021331071853637695, 0.018938779830932617, 0.004055917263031006]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [10:22<53:36, 38.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 7504  ---  mean_dsc : 0.5905844200438555\n",
            "step : 7509  ---  loss train : [0.1679854393005371, 0.07146406173706055, 0.10810136795043945, 0.06217771768569946, 0.03508549928665161, 0.08942073583602905, 0.12375879287719727, 0.4861968755722046, 0.04164546728134155, 0.2480926513671875]\n",
            "step : 7519  ---  loss train : [0.05345165729522705, 0.0552668571472168, 0.16577982902526855, 0.16710609197616577, 0.14182132482528687, 0.1094244122505188, 0.047946035861968994, 0.20825517177581787, 0.23248016834259033, 0.1299828290939331]\n",
            "step : 7529  ---  loss train : [0.04533267021179199, 0.08553582429885864, 0.20418059825897217, 0.0447501540184021, 0.05189192295074463, 0.12078464031219482, 0.18986260890960693, 0.11983519792556763, 0.11804342269897461, 0.03768271207809448]\n",
            "step : 7539  ---  loss train : [0.1325097680091858, 0.06215620040893555, 0.27416902780532837, 0.06175577640533447, 0.09019917249679565, 0.07467657327651978, 0.09066617488861084, 0.05323737859725952, 0.1811283826828003, 0.04243779182434082]\n",
            "step : 7549  ---  loss train : [0.09574675559997559, 0.09050357341766357, 0.11405360698699951, 0.05420190095901489, 0.04942166805267334, 0.030586063861846924, 0.4488893151283264, 0.06570494174957275, 0.05720663070678711, 0.3566588759422302]\n",
            "step : 7559  ---  loss train : [0.16448426246643066, 0.0973157286643982, 0.4288535714149475, 0.3102700114250183, 0.607890248298645, 0.2016294002532959, 0.1255844235420227, 0.04324448108673096, 0.0806695818901062, 0.1027640700340271]\n",
            "step : 7569  ---  loss train : [0.0631454586982727, 0.08399838209152222, 0.0998086929321289, 0.20021921396255493, 0.22201865911483765, 0.12961453199386597, 0.22071164846420288, 0.03756570816040039, 0.23659026622772217, 0.051127731800079346]\n",
            "step : 7579  ---  loss train : [0.04445236921310425, 0.11361390352249146, 0.14023882150650024, 0.14913254976272583, 0.06601893901824951, 0.08076977729797363, 0.12141942977905273, 0.33595824241638184, 0.0981789231300354, 0.06985193490982056]\n",
            "step : 7589  ---  loss train : [0.3026992678642273, 0.055007994174957275, 0.3557993769645691, 0.201710045337677, 0.10586667060852051, 0.0775720477104187, 0.05867105722427368, 0.04969435930252075, 0.13240009546279907, 0.10878342390060425]\n",
            "step : 7599  ---  loss train : [0.2837047576904297, 0.1743241548538208, 0.09968167543411255, 0.07177698612213135, 0.15558212995529175, 0.12842780351638794, 0.05438840389251709, 0.14804905652999878, 0.1832001805305481, 0.2521529197692871]\n",
            "step : 7609  ---  loss train : [0.11672192811965942, 0.10738521814346313, 0.09495079517364502, 0.10136479139328003, 0.2168973684310913, 0.2801457643508911, 0.14408624172210693, 0.2519839406013489, 0.0906214714050293, 0.08063322305679321]\n",
            "step : 7619  ---  loss train : [0.05483764410018921, 0.2135518193244934, 0.2369576096534729, 0.20208096504211426, 0.13797956705093384, 0.037997305393218994, 0.09497129917144775, 0.04891401529312134, 0.18582838773727417, 0.146264910697937]\n",
            "step : 7629  ---  loss train : [0.3450479507446289, 0.251165509223938, 0.08291959762573242, 0.22297358512878418, 0.10395967960357666, 0.1758018136024475, 0.26638513803482056, 0.06130009889602661, 0.09210407733917236, 0.23178905248641968]\n",
            "step : 7639  ---  loss train : [0.04831373691558838, 0.3582608103752136, 0.0703202486038208, 0.18963634967803955, 0.11170446872711182, 0.0690995454788208, 0.10767090320587158, 0.36905986070632935, 0.999855101108551, 0.07276743650436401]\n",
            "step : 7649  ---  loss train : [0.042011797428131104, 0.0909585952758789, 0.0694422721862793, 0.06373792886734009, 0.07675498723983765, 0.05621391534805298, 0.0552600622177124, 0.15909552574157715, 0.05698150396347046, 0.04602253437042236]\n",
            "step : 7659  ---  loss train : [0.08780276775360107, 0.036394476890563965, 0.09021401405334473, 0.16180384159088135, 0.1814538836479187, 0.05755835771560669, 0.11438655853271484, 0.18422746658325195, 0.9997124075889587, 0.07493263483047485]\n",
            "step : 7669  ---  loss train : [0.06685978174209595, 0.13510888814926147, 0.05325525999069214, 0.1144983172416687, 0.05943572521209717, 0.061763226985931396, 0.0920264720916748, 0.0983319878578186, 0.24209320545196533, 0.06588476896286011]\n",
            "step : 7679  ---  loss train : [0.168739914894104, 0.06288158893585205, 0.07890647649765015, 0.09957051277160645, 0.11456996202468872, 0.052466511726379395, 0.07463496923446655, 0.10874217748641968, 0.10654699802398682, 0.19551724195480347]\n",
            "step : 7689  ---  loss train : [0.2279757857322693, 0.06659907102584839, 0.054772019386291504, 0.10960817337036133, 0.057290613651275635, 0.07172274589538574, 0.20218199491500854, 0.18126678466796875, 0.2753758430480957, 0.056618571281433105]\n",
            "step : 7699  ---  loss train : [0.27691614627838135, 0.12525290250778198, 0.21661698818206787, 0.15026426315307617, 0.1284143328666687, 0.15212196111679077, 0.23787373304367065, 0.14205855131149292, 0.19573557376861572, 0.08176440000534058]\n",
            "step : 7709  ---  loss train : [0.21642935276031494, 0.17459887266159058, 0.3186802864074707, 0.18258041143417358, 0.41578513383865356, 0.11249607801437378, 0.06897246837615967, 0.13819628953933716, 0.22136974334716797, 0.07025778293609619]\n",
            "step : 7719  ---  loss train : [0.2629498839378357, 0.13617587089538574, 0.06901586055755615, 0.047257065773010254, 0.23489224910736084, 0.09598535299301147, 0.20268911123275757, 0.06402885913848877, 0.06724333763122559, 0.07146751880645752]\n",
            "step : 7729  ---  loss train : [0.11045700311660767, 0.0830608606338501, 0.7210947275161743, 0.10261231660842896, 0.19107341766357422, 0.053549349308013916, 0.1351650357246399, 0.1485733985900879, 0.21700894832611084, 0.12312865257263184]\n",
            "step : 7739  ---  loss train : [0.329590380191803, 0.09732091426849365, 0.0817955732345581, 0.1159200668334961, 0.11675006151199341, 0.23969495296478271, 0.040376484394073486, 0.1451328992843628, 0.1563132405281067, 0.16410940885543823]\n",
            "step : 7749  ---  loss train : [0.18950217962265015, 0.1544284224510193, 0.04765439033508301, 0.06677120923995972, 0.15075576305389404, 0.21309411525726318, 0.0472603440284729, 0.13553661108016968, 0.09049099683761597, 0.08487248420715332]\n",
            "step : 7759  ---  loss train : [0.12594258785247803, 0.0782843828201294, 0.14107650518417358, 0.0858302116394043, 0.043857455253601074, 0.08178669214248657, 0.07862794399261475, 0.126298725605011, 0.05140143632888794, 0.3510481119155884]\n",
            "step : 7769  ---  loss train : [0.07575410604476929, 0.054600298404693604, 0.11598896980285645, 0.17938894033432007, 0.053928494453430176, 0.15228629112243652, 0.11707693338394165, 0.11379724740982056, 0.042230427265167236, 0.03297889232635498]\n",
            "step : 7779  ---  loss train : [0.05786299705505371, 0.2437261939048767, 0.07366293668746948, 0.2808595895767212, 0.17954516410827637, 0.07275146245956421, 0.27712351083755493, 0.07831740379333496, 0.21166646480560303, 0.17892205715179443]\n",
            "step : 7789  ---  loss train : [0.1306334137916565, 0.03600853681564331, 0.1751900315284729, 0.04130423069000244, 0.07981055974960327, 0.17067301273345947, 0.06855857372283936, 0.13091140985488892, 0.10532939434051514, 0.390885591506958]\n",
            "step : 7799  ---  loss train : [0.3925820589065552, 0.0679808259010315, 0.1759403944015503, 0.25298428535461426, 0.06922435760498047, 0.06023496389389038, 0.07997357845306396, 0.038172245025634766, 0.07154953479766846, 0.1929602026939392]\n",
            "step : 7809  ---  loss train : [0.13727009296417236, 0.16333794593811035, 0.2563518285751343, 0.15243792533874512, 0.16847622394561768, 0.21562319993972778, 0.3215551972389221, 0.050784945487976074, 0.1861792802810669, 0.05005013942718506]\n",
            "step : 7819  ---  loss train : [0.1142699122428894, 0.08873534202575684, 0.05056196451187134, 0.0733380913734436, 0.3601890802383423, 0.16177111864089966, 0.1882953643798828, 0.2609342932701111, 0.35112226009368896, 0.06625890731811523]\n",
            "step : 7829  ---  loss train : [0.2802025079727173, 0.05806374549865723, 0.1913197636604309, 0.04338759183883667, 0.34933626651763916, 0.037618160247802734, 0.10817819833755493, 0.09346562623977661, 0.3944206237792969, 0.14526033401489258]\n",
            "step : 7839  ---  loss train : [0.09799468517303467, 0.14724373817443848, 0.205330491065979, 0.09731161594390869, 0.08228212594985962, 0.09983110427856445, 0.15532052516937256, 0.07254683971405029, 0.059594035148620605, 0.48365044593811035]\n",
            "step : 7849  ---  loss train : [0.10404092073440552, 0.04093128442764282, 0.05698072910308838, 0.060059964656829834, 0.08986979722976685, 0.3014923334121704, 0.12911254167556763, 0.08816534280776978, 0.1731281876564026, 0.15860515832901]\n",
            "step : 7859  ---  loss train : [0.07936662435531616, 0.07766187191009521, 0.0669519305229187, 0.12447017431259155, 0.05046796798706055, 0.10684007406234741, 0.11048650741577148, 0.059092164039611816, 0.1646099090576172, 0.17207294702529907]\n",
            "step : 7869  ---  loss train : [0.10434770584106445, 0.06383776664733887, 0.1047595739364624, 0.10383784770965576, 0.11880093812942505, 0.05262923240661621, 0.17500019073486328, 0.18963319063186646, 0.08530688285827637, 0.05480682849884033]\n",
            "step : 7879  ---  loss train : [0.15536677837371826, 0.17793428897857666, 0.058260560035705566, 0.23251450061798096, 0.15625053644180298, 0.0508800745010376, 0.042606890201568604, 0.08752143383026123, 0.09177690744400024, 0.08319157361984253]\n",
            "step : 7889  ---  loss train : [0.600292444229126, 0.28463107347488403, 0.16016709804534912, 0.7305763959884644, 0.08204591274261475, 0.20447760820388794, 0.4127667546272278, 0.5389962196350098, 0.10040473937988281, 0.1518251895904541]\n",
            "step : 7899  ---  loss train : [0.25318241119384766, 0.06975477933883667, 0.055046796798706055, 0.1917549967765808, 0.25711488723754883, 0.0747334361076355, 0.06803256273269653, 0.22141796350479126, 0.041575729846954346, 0.06569111347198486]\n",
            "step : 7909  ---  loss train : [0.14332610368728638, 0.08353477716445923, 0.11936140060424805, 0.13103806972503662, 0.0924411416053772, 0.08511430025100708, 0.07958394289016724, 0.14518052339553833, 0.13922333717346191, 0.21982330083847046]\n",
            "step : 7919  ---  loss train : [0.1489587426185608, 0.06878316402435303, 0.11415624618530273, 0.14744174480438232, 0.11032837629318237, 0.08361101150512695, 0.17108958959579468, 0.056978046894073486, 0.5182054042816162, 0.09387338161468506]\n",
            "step : 7929  ---  loss train : [0.12909209728240967, 0.05662053823471069, 0.17467069625854492, 0.1290939450263977, 0.05617433786392212, 0.13007420301437378, 0.17574822902679443, 0.06150537729263306, 0.08727008104324341, 0.11942416429519653]\n",
            "step : 7939  ---  loss train : [0.0687214732170105, 0.06958895921707153, 0.13011133670806885, 0.0948057770729065, 0.2604091167449951, 0.3165273070335388, 0.10094386339187622, 0.061220765113830566, 0.06782019138336182, 0.08081573247909546]\n",
            "step : 7949  ---  loss train : [0.09092986583709717, 0.07542675733566284, 0.10428488254547119, 0.06357663869857788, 0.09073925018310547, 0.10006773471832275, 0.07160300016403198, 0.2773887515068054, 0.056823134422302246, 0.35049283504486084]\n",
            "step : 7959  ---  loss train : [0.5897675156593323, 0.08500021696090698, 0.20696890354156494, 0.14419007301330566, 0.04358243942260742, 0.07133984565734863, 0.1516147255897522, 0.06940776109695435, 0.05959188938140869, 0.08397489786148071]\n",
            "step : 7969  ---  loss train : [0.4742714762687683, 0.04681164026260376, 0.0934571623802185, 0.07142776250839233, 0.14451658725738525, 0.051800668239593506, 0.18302679061889648, 0.05838370323181152, 0.3109571933746338, 0.16661548614501953]\n",
            "step : 7973  ---  loss train : [0.9997601509094238, 0.9893173575401306, 0.9979206323623657, 0.406363844871521, 0.5880604982376099, 0.9989646673202515, 0.999001681804657, 0.9850701093673706, 0.007453978061676025, 0.7834411859512329, 0.02188277244567871, 0.022098064422607422, 0.01890242099761963, 0.2953527569770813, 0.0673794150352478, 0.07810002565383911, 0.11993300914764404, 0.008468866348266602, 0.010257720947265625, 0.04648500680923462, 0.028930604457855225, 0.01666325330734253, 0.24836111068725586, 0.11330670118331909, 0.19091540575027466, 0.08660930395126343, 0.09728741645812988, 0.1024964451789856, 0.06659156084060669, 0.1805117130279541, 0.9836400151252747, 0.005912065505981445, 0.007082700729370117, 0.028378844261169434, 0.023159503936767578, 0.011060953140258789, 0.9992572665214539, 0.02455073595046997, 0.9961386322975159, 0.033117473125457764, 0.9974790215492249, 0.16965252161026, 0.14129340648651123, 0.2673889398574829, 0.3021789789199829, 0.3323460817337036, 0.9997791647911072, 0.9997925162315369, 0.9997028708457947, 0.9981521368026733, 0.9988642930984497, 0.977696418762207, 0.9992305040359497, 0.9996983408927917, 0.9569066166877747, 0.9366872310638428, 0.9995347261428833, 0.9997014403343201, 0.9998362064361572, 0.9997896552085876, 0.9764378666877747, 0.3440479636192322, 0.07074630260467529, 0.1011231541633606, 0.25985729694366455, 0.9997657537460327, 0.9985978603363037, 0.05505102872848511, 0.11639809608459473, 0.9909370541572571, 0.9077563285827637, 0.9936217069625854, 0.4960368275642395, 0.05570262670516968, 0.20069313049316406, 0.03510487079620361, 0.04235053062438965, 0.023776233196258545, 0.6864229440689087, 0.022444963455200195, 0.022033095359802246, 0.013571321964263916, 0.013052940368652344, 0.9980658292770386, 0.1598261594772339, 0.5092955827713013, 0.2956475019454956, 0.2733665108680725, 0.4702748656272888, 0.36501795053482056, 0.11717915534973145, 0.050440967082977295, 0.039674222469329834, 0.15922093391418457, 0.01086193323135376, 0.01127403974533081, 0.011947154998779297, 0.009788334369659424, 0.014244019985198975, 0.006792247295379639, 0.00486224889755249, 0.004788577556610107, 0.9972195029258728, 0.05292379856109619, 0.3202190399169922, 0.10639756917953491, 0.10620462894439697, 0.22066360712051392, 0.4810313582420349, 0.019692599773406982, 0.03759312629699707, 0.02679044008255005, 0.022893130779266357, 0.014630436897277832, 0.9155930876731873, 0.01656097173690796, 0.5761418342590332, 0.11927711963653564, 0.05273991823196411, 0.0644148588180542, 0.05876278877258301, 0.18071043491363525, 0.9743704199790955, 0.019481539726257324, 0.03046506643295288, 0.03982478380203247, 0.02891308069229126, 0.022026240825653076, 0.023121297359466553, 0.02473682165145874, 0.030816256999969482, 0.023217380046844482, 0.023138344287872314, 0.020268738269805908, 0.018037497997283936, 0.018964827060699463, 0.018933653831481934, 0.015895724296569824, 0.9900239706039429, 0.9989830851554871, 0.013574481010437012, 0.00965118408203125, 0.01284700632095337, 0.9979708194732666, 0.9994229078292847, 0.9995476603507996, 0.9992895722389221, 0.99609375, 0.003535926342010498, 0.0031584501266479492, 0.002372443675994873, 0.00042170286178588867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [10:59<52:14, 37.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 7973  ---  mean_dsc : 0.5890313768453838\n",
            "step : 7979  ---  loss train : [0.07691341638565063, 0.10445475578308105, 0.07321548461914062, 0.04399311542510986, 0.09184938669204712, 0.12895488739013672, 0.6774420738220215, 0.05703097581863403, 0.3280991315841675, 0.05506491661071777]\n",
            "step : 7989  ---  loss train : [0.04813253879547119, 0.16278237104415894, 0.18884408473968506, 0.14694660902023315, 0.11380922794342041, 0.053167641162872314, 0.2193731665611267, 0.23436576128005981, 0.14156121015548706, 0.042520999908447266]\n",
            "step : 7999  ---  loss train : [0.0952216386795044, 0.20910733938217163, 0.04642075300216675, 0.05327242612838745, 0.12426841259002686, 0.1968303918838501, 0.11519908905029297, 0.10934257507324219, 0.03357964754104614, 0.12481170892715454]\n",
            "step : 8009  ---  loss train : [0.05423218011856079, 0.28787195682525635, 0.06240648031234741, 0.0816192626953125, 0.06394165754318237, 0.07156723737716675, 0.05744868516921997, 0.17758262157440186, 0.04225355386734009, 0.10030198097229004]\n",
            "step : 8019  ---  loss train : [0.09521114826202393, 0.13851070404052734, 0.05302387475967407, 0.048901379108428955, 0.04026716947555542, 0.45440107583999634, 0.07243478298187256, 0.05645942687988281, 0.3497679829597473, 0.16487598419189453]\n",
            "step : 8029  ---  loss train : [0.09653705358505249, 0.3411400318145752, 0.2895849943161011, 0.474698543548584, 0.1690477728843689, 0.13948965072631836, 0.03992307186126709, 0.07332664728164673, 0.07143330574035645, 0.08132237195968628]\n",
            "step : 8039  ---  loss train : [0.08989274501800537, 0.12484294176101685, 0.21875882148742676, 0.1642528772354126, 0.13535654544830322, 0.1785522699356079, 0.0404323935508728, 0.24096626043319702, 0.050636231899261475, 0.04495781660079956]\n",
            "step : 8049  ---  loss train : [0.11819368600845337, 0.14204919338226318, 0.12118160724639893, 0.0604134202003479, 0.09925544261932373, 0.1484103798866272, 0.20036077499389648, 0.10126519203186035, 0.07764899730682373, 0.3267325162887573]\n",
            "step : 8059  ---  loss train : [0.06759470701217651, 0.36052536964416504, 0.16567271947860718, 0.05910754203796387, 0.048409879207611084, 0.0635749101638794, 0.05353081226348877, 0.11618584394454956, 0.1414088010787964, 0.26141035556793213]\n",
            "step : 8069  ---  loss train : [0.050338566303253174, 0.10457742214202881, 0.06817507743835449, 0.1510525345802307, 0.12731999158859253, 0.06161332130432129, 0.13813084363937378, 0.17476433515548706, 0.2656291127204895, 0.1353001594543457]\n",
            "step : 8079  ---  loss train : [0.10442668199539185, 0.08860236406326294, 0.08562946319580078, 0.21874815225601196, 0.287212610244751, 0.12155824899673462, 0.24888312816619873, 0.09044915437698364, 0.0876169204711914, 0.07217824459075928]\n",
            "step : 8089  ---  loss train : [0.17281615734100342, 0.19435060024261475, 0.19801509380340576, 0.13870519399642944, 0.03923523426055908, 0.09273052215576172, 0.05242961645126343, 0.18932461738586426, 0.14500433206558228, 0.3419417142868042]\n",
            "step : 8099  ---  loss train : [0.20875614881515503, 0.0716751217842102, 0.22509771585464478, 0.08845490217208862, 0.1805098056793213, 0.23020809888839722, 0.04927968978881836, 0.08439630270004272, 0.21261054277420044, 0.047270119190216064]\n",
            "step : 8109  ---  loss train : [0.2678210139274597, 0.0638437271118164, 0.17210930585861206, 0.1277710199356079, 0.21163350343704224, 0.1217724084854126, 0.40186458826065063, 0.8398590683937073, 0.06996268033981323, 0.04407954216003418]\n",
            "step : 8119  ---  loss train : [0.09226328134536743, 0.04475635290145874, 0.06036776304244995, 0.2560041546821594, 0.24907487630844116, 0.0594441294670105, 0.19010072946548462, 0.05286979675292969, 0.05290400981903076, 0.16510283946990967]\n",
            "step : 8129  ---  loss train : [0.04859215021133423, 0.09765380620956421, 0.15118664503097534, 0.3085508942604065, 0.0581476092338562, 0.16118818521499634, 0.16108065843582153, 0.9996595978736877, 0.0849294662475586, 0.07658320665359497]\n",
            "step : 8139  ---  loss train : [0.11480057239532471, 0.06925785541534424, 0.13088256120681763, 0.07744759321212769, 0.06919372081756592, 0.09331321716308594, 0.10916256904602051, 0.23792874813079834, 0.06660091876983643, 0.142628014087677]\n",
            "step : 8149  ---  loss train : [0.05747067928314209, 0.0857200026512146, 0.09060633182525635, 0.1306055188179016, 0.06629174947738647, 0.07473897933959961, 0.16407626867294312, 0.13639318943023682, 0.1990288496017456, 0.27760249376296997]\n",
            "step : 8159  ---  loss train : [0.08930951356887817, 0.05550879240036011, 0.09437263011932373, 0.06007951498031616, 0.07366663217544556, 0.17172378301620483, 0.18276888132095337, 0.5036112666130066, 0.04228478670120239, 0.18598449230194092]\n",
            "step : 8169  ---  loss train : [0.20964938402175903, 0.13451486825942993, 0.16293871402740479, 0.13578051328659058, 0.11576920747756958, 0.1995532512664795, 0.17022961378097534, 0.16896915435791016, 0.07528674602508545, 0.25979089736938477]\n",
            "step : 8179  ---  loss train : [0.15950840711593628, 0.40547019243240356, 0.339788556098938, 0.48759377002716064, 0.13466984033584595, 0.06464868783950806, 0.12349146604537964, 0.16598182916641235, 0.06209123134613037, 0.26612794399261475]\n",
            "step : 8189  ---  loss train : [0.1315397024154663, 0.06842988729476929, 0.058385491371154785, 0.20964759588241577, 0.11079370975494385, 0.16204839944839478, 0.06876194477081299, 0.06233865022659302, 0.0677747130393982, 0.10467380285263062]\n",
            "step : 8199  ---  loss train : [0.044394075870513916, 0.9997612237930298, 0.10674858093261719, 0.18006300926208496, 0.05211716890335083, 0.15485483407974243, 0.1343473196029663, 0.21024870872497559, 0.12601196765899658, 0.2881031632423401]\n",
            "step : 8209  ---  loss train : [0.10068076848983765, 0.1633526086807251, 0.10470974445343018, 0.11326855421066284, 0.21611112356185913, 0.057246506214141846, 0.14316326379776, 0.1810017228126526, 0.20352637767791748, 0.19293934106826782]\n",
            "step : 8219  ---  loss train : [0.14187651872634888, 0.02800428867340088, 0.09015631675720215, 0.15062659978866577, 0.19401657581329346, 0.04145944118499756, 0.1385459303855896, 0.10726618766784668, 0.07436871528625488, 0.10930633544921875]\n",
            "step : 8229  ---  loss train : [0.07477307319641113, 0.1304139494895935, 0.06060570478439331, 0.06111985445022583, 0.08133202791213989, 0.07421994209289551, 0.12250626087188721, 0.04880648851394653, 0.2892422080039978, 0.0677870512008667]\n",
            "step : 8239  ---  loss train : [0.05400562286376953, 0.13601064682006836, 0.1836654543876648, 0.05661433935165405, 0.16355955600738525, 0.15771698951721191, 0.11965346336364746, 0.04279297590255737, 0.03506594896316528, 0.04791676998138428]\n",
            "step : 8249  ---  loss train : [0.25046539306640625, 0.06697297096252441, 0.30177032947540283, 0.6386640071868896, 0.07653844356536865, 0.44285857677459717, 0.08435428142547607, 0.11895871162414551, 0.13236993551254272, 0.13544660806655884]\n",
            "step : 8259  ---  loss train : [0.03441590070724487, 0.2001274824142456, 0.03828620910644531, 0.08694922924041748, 0.15762358903884888, 0.07693439722061157, 0.1296270489692688, 0.10142570734024048, 0.34938424825668335, 0.4025952219963074]\n",
            "step : 8269  ---  loss train : [0.08128410577774048, 0.12508803606033325, 0.20637190341949463, 0.05650043487548828, 0.06206232309341431, 0.08750909566879272, 0.037690162658691406, 0.06635075807571411, 0.19675511121749878, 0.17908912897109985]\n",
            "step : 8279  ---  loss train : [0.1742146611213684, 0.11996155977249146, 0.15853482484817505, 0.12639731168746948, 0.20965135097503662, 0.33618754148483276, 0.039709627628326416, 0.1670892834663391, 0.05108499526977539, 0.10109686851501465]\n",
            "step : 8289  ---  loss train : [0.10293209552764893, 0.0507928729057312, 0.1005282998085022, 0.3027160167694092, 0.1593223214149475, 0.154826819896698, 0.2435632348060608, 0.31001001596450806, 0.06388586759567261, 0.16706562042236328]\n",
            "step : 8299  ---  loss train : [0.05368894338607788, 0.12488913536071777, 0.05287671089172363, 0.33865249156951904, 0.037094831466674805, 0.15168678760528564, 0.10608017444610596, 0.32268476486206055, 0.15011411905288696, 0.09449601173400879]\n",
            "step : 8309  ---  loss train : [0.06370216608047485, 0.21787405014038086, 0.0961342453956604, 0.09563291072845459, 0.10608458518981934, 0.1459295153617859, 0.07110005617141724, 0.06718730926513672, 0.5199331045150757, 0.11147958040237427]\n",
            "step : 8319  ---  loss train : [0.041593074798583984, 0.05354982614517212, 0.0648297667503357, 0.10109478235244751, 0.22757214307785034, 0.11352258920669556, 0.07872962951660156, 0.15659010410308838, 0.14485901594161987, 0.05163019895553589]\n",
            "step : 8329  ---  loss train : [0.060056209564208984, 0.059995293617248535, 0.12266749143600464, 0.05070316791534424, 0.12836986780166626, 0.09354901313781738, 0.06237834692001343, 0.15551549196243286, 0.16129666566848755, 0.1072126030921936]\n",
            "step : 8339  ---  loss train : [0.05988740921020508, 0.09862232208251953, 0.09654736518859863, 0.1124868392944336, 0.056047260761260986, 0.1785907745361328, 0.19605422019958496, 0.08523714542388916, 0.049062907695770264, 0.13977712392807007]\n",
            "step : 8349  ---  loss train : [0.10042017698287964, 0.04793304204940796, 0.26530921459198, 0.15519309043884277, 0.04524117708206177, 0.06117445230484009, 0.09270095825195312, 0.07784682512283325, 0.058944106101989746, 0.3853733539581299]\n",
            "step : 8359  ---  loss train : [0.15625375509262085, 0.1759854555130005, 0.5487703084945679, 0.04013407230377197, 0.1645050048828125, 0.39760780334472656, 0.2368762493133545, 0.09694105386734009, 0.15423953533172607, 0.23209112882614136]\n",
            "step : 8369  ---  loss train : [0.049317777156829834, 0.05020415782928467, 0.1733141541481018, 0.37017256021499634, 0.05591684579849243, 0.0721847414970398, 0.1927875280380249, 0.050072431564331055, 0.0703432559967041, 0.1065744161605835]\n",
            "step : 8379  ---  loss train : [0.130942702293396, 0.1068657636642456, 0.11580508947372437, 0.0831458568572998, 0.09404277801513672, 0.09373432397842407, 0.13471513986587524, 0.12335926294326782, 0.22830212116241455, 0.1543794870376587]\n",
            "step : 8389  ---  loss train : [0.08191674947738647, 0.18776637315750122, 0.1310054063796997, 0.11025339365005493, 0.11041432619094849, 0.17513513565063477, 0.05672657489776611, 0.39299166202545166, 0.08799123764038086, 0.13174152374267578]\n",
            "step : 8399  ---  loss train : [0.06458598375320435, 0.18852895498275757, 0.12480533123016357, 0.05690646171569824, 0.13714313507080078, 0.2491641640663147, 0.09089130163192749, 0.08534407615661621, 0.10319113731384277, 0.0698137879371643]\n",
            "step : 8409  ---  loss train : [0.07979995012283325, 0.1616506576538086, 0.1045234203338623, 0.29603636264801025, 0.20966458320617676, 0.11329406499862671, 0.06326419115066528, 0.07891613245010376, 0.0818207859992981, 0.08833831548690796]\n",
            "step : 8419  ---  loss train : [0.07024645805358887, 0.10907924175262451, 0.06459677219390869, 0.08305668830871582, 0.1059560775756836, 0.07884234189987183, 0.20151585340499878, 0.05246615409851074, 0.30855607986450195, 0.48498666286468506]\n",
            "step : 8429  ---  loss train : [0.08602654933929443, 0.19225811958312988, 0.1486991047859192, 0.03983783721923828, 0.06911200284957886, 0.14640140533447266, 0.06069570779800415, 0.059184253215789795, 0.08173823356628418, 0.5966959595680237]\n",
            "step : 8439  ---  loss train : [0.05051565170288086, 0.08516788482666016, 0.05985546112060547, 0.14837205410003662, 0.05512511730194092, 0.18010330200195312, 0.04430067539215088, 0.17718523740768433, 0.0920870304107666, 0.07263123989105225]\n",
            "step : 8442  ---  loss train : [0.999259889125824, 0.07095658779144287, 0.9979230165481567, 0.8266932368278503, 0.7255053520202637, 0.8956039547920227, 0.020492851734161377, 0.018457233905792236, 0.01577669382095337, 0.017231523990631104, 0.05057859420776367, 0.046195149421691895, 0.047236502170562744, 0.3275458812713623, 0.07018923759460449, 0.08482187986373901, 0.1805742383003235, 0.018969297409057617, 0.037346601486206055, 0.01341104507446289, 0.010587990283966064, 0.010153234004974365, 0.016546666622161865, 0.01390087604522705, 0.19134771823883057, 0.07481664419174194, 0.08816641569137573, 0.09909766912460327, 0.06531310081481934, 0.10604774951934814, 0.9833346605300903, 0.00414586067199707, 0.004837930202484131, 0.01307988166809082, 0.030802488327026367, 0.011106908321380615, 0.9960760474205017, 0.04962104558944702, 0.05892682075500488, 0.047810256481170654, 0.997663676738739, 0.1911417841911316, 0.15942013263702393, 0.29308217763900757, 0.39176762104034424, 0.489605188369751, 0.9991398453712463, 0.9922109246253967, 0.9945184588432312, 0.25399941205978394, 0.934384822845459, 0.8248142004013062, 0.07177877426147461, 0.9995999932289124, 0.9997175931930542, 0.9995238780975342, 0.9955068826675415, 0.9982946515083313, 0.9997830390930176, 0.9997529983520508, 0.0452265739440918, 0.25778454542160034, 0.07172077894210815, 0.09477108716964722, 0.25141441822052, 0.9997661113739014, 0.9982280731201172, 0.05447810888290405, 0.11096376180648804, 0.9250689148902893, 0.18757539987564087, 0.6007362008094788, 0.027531325817108154, 0.0208742618560791, 0.019331157207489014, 0.018200457096099854, 0.02034848928451538, 0.02180635929107666, 0.033926546573638916, 0.026955008506774902, 0.020546257495880127, 0.018156349658966064, 0.02442222833633423, 0.9980937838554382, 0.1535482406616211, 0.3274475932121277, 0.22945630550384521, 0.2849287986755371, 0.3047184348106384, 0.20435035228729248, 0.09321087598800659, 0.05584454536437988, 0.04785412549972534, 0.1695736050605774, 0.010442197322845459, 0.01186150312423706, 0.009615540504455566, 0.013253867626190186, 0.023946166038513184, 0.014766037464141846, 0.018215537071228027, 0.013377606868743896, 0.4581450819969177, 0.0951462984085083, 0.3428761959075928, 0.10548388957977295, 0.06619924306869507, 0.1790482997894287, 0.42197853326797485, 0.05093318223953247, 0.08650600910186768, 0.020161747932434082, 0.028992652893066406, 0.026606500148773193, 0.02778381109237671, 0.02749347686767578, 0.4674818515777588, 0.11290758848190308, 0.05110931396484375, 0.060713112354278564, 0.054163575172424316, 0.2270936369895935, 0.9743787050247192, 0.016198337078094482, 0.018102705478668213, 0.015474200248718262, 0.014011561870574951, 0.015755891799926758, 0.0176389217376709, 0.016476929187774658, 0.022227346897125244, 0.020516514778137207, 0.017946243286132812, 0.012984991073608398, 0.012799263000488281, 0.015710055828094482, 0.01233142614364624, 0.013346493244171143, 0.014815688133239746, 0.01941549777984619, 0.012070655822753906, 0.011272072792053223, 0.010433316230773926, 0.9979668855667114, 0.9994231462478638, 0.99954754114151, 0.9992890357971191, 0.99609375, 0.01225060224533081, 0.014019906520843506, 0.01472938060760498, 0.0037238597869873047]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [11:35<56:36, 40.92s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2a3779ba15f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mvalidation_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mvalidation_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0mloader_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatient_slice_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 )\n\u001b[1;32m     71\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-11-4a5359d3fc2d>\u001b[0m in \u001b[0;36mdsc_per_volume\u001b[0;34m(validation_pred, validation_true, patient_slice_index)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdsc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdsc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-144481ed737c>\u001b[0m in \u001b[0;36mdsc\u001b[0;34m(y_pred, y_true, lcc)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlargest_connected_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/medpy/filter/binary.py\u001b[0m in \u001b[0;36mlargest_connected_component\u001b[0;34m(img, structure)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mlabeled_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcomponent_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_array\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mlargest_component_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \"\"\"\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imsave\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "\n",
        "phase = \"valid\"\n",
        "with torch.set_grad_enabled(False):\n",
        "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "    state_dict = torch.load(\"unet.pt\", map_location=device)\n",
        "    unet.load_state_dict(state_dict)\n",
        "    unet.eval()\n",
        "    unet.to(device)\n",
        "\n",
        "    input_list = []\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "\n",
        "    for i, data in tqdm(enumerate(loaders[phase])):\n",
        "        x, y_true = data\n",
        "        x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "        y_pred = unet(x)\n",
        "        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "        pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
        "\n",
        "        y_true_np = y_true.detach().cpu().numpy()\n",
        "        true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
        "\n",
        "def postprocess_per_volume(\n",
        "    input_list, pred_list, true_list, patient_slice_index, patients\n",
        "):\n",
        "    volumes = {}\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
        "        volume_pred = np.round(\n",
        "            np.array(pred_list[index : index + num_slices[p]])\n",
        "        ).astype(int)\n",
        "        volume_pred = largest_connected_component(volume_pred)\n",
        "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
        "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
        "        index += num_slices[p]\n",
        "    return volumes\n",
        "\n",
        "volumes = postprocess_per_volume(\n",
        "    input_list,\n",
        "    pred_list,\n",
        "    true_list,\n",
        "    loaders[phase].dataset.patient_slice_index,\n",
        "    loaders[phase].dataset.patients,\n",
        ")\n",
        "\n",
        "def dsc_distribution(volumes):\n",
        "    dsc_dict = {}\n",
        "    for p in volumes:\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        dsc_dict[p] = dsc(y_pred, y_true, lcc=False)\n",
        "    return dsc_dict\n",
        "\n",
        "dsc_dist = dsc_distribution(volumes)\n",
        "\n",
        "\n",
        "\n",
        "def plot_dsc(dsc_dist):\n",
        "    y_positions = np.arange(len(dsc_dist))\n",
        "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
        "    values = [x[1] for x in dsc_dist]\n",
        "    labels = [x[0] for x in dsc_dist]\n",
        "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
        "    plt.yticks(y_positions, labels)\n",
        "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
        "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
        "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
        "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    canvas.draw()\n",
        "    plt.close()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
        "\n",
        "dsc_dist_plot = plot_dsc(dsc_dist)\n",
        "imsave(\"./dsc.png\", dsc_dist_plot)\n",
        "\n",
        "for p in volumes:\n",
        "    x = volumes[p][0]\n",
        "    y_pred = volumes[p][1]\n",
        "    y_true = volumes[p][2]\n",
        "    for s in range(x.shape[0]):\n",
        "        image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
        "        image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
        "        image = outline(image, y_true[s, 0], color=[0, 255, 0])\n",
        "        filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
        "        filepath = os.path.join(\"./predictions\", filename)\n",
        "        imsave(filepath, image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnd1xXJPlxkv",
        "outputId": "0c8aa2bb-12d9-40ff-8183-bff28265706e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "152it [00:03, 44.85it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: ./predictions/TCGA_DU_5851_19950428-34.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: ./predictions/TCGA_DU_5872_19950223-66.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: ./predictions/TCGA_DU_6405_19851005-55.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: ./predictions/TCGA_DU_6405_19851005-56.png is a low contrast image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References during the workshop\n",
        "\n",
        "- [Workshop recording](https://www.youtube.com/watch?v=QDYqYyrdf7M&list=PLVH7T2_su-vkHLGQXJ0gHijbhjLJOCbaq&index=20)\n",
        "\n",
        "- [Dataset from Kaggle used in this workshop: Brain MRI segmentation\n",
        "Brain MRI images together with manual FLAIR abnormality segmentation masks](https://www.kaggle.com/datasets/mateuszbuda/lgg-mri-segmentation)\n",
        "\n",
        "- [Original Unet paper 2015](https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "- [Unet Classification and Loss Evaluation - Softmax and Cross Entropy Loss](https://deepnotes.io/softmax-crossentropy)\n",
        "  - [Softmax classification with cross-entropy](https://peterroelants.github.io/posts/cross-entropy-softmax/)\n",
        "\n",
        "- [Towards a guideline for evaluation metrics in medical image segmentation (reviews of evaluation metric definitions)](https://arxiv.org/ftp/arxiv/papers/2202/2202.05273.pdf)\n",
        "\n",
        "- [Mask DINO: Towards a Unified Transformer-based framework for object detection and segmentation (most state-of-the-art detection algorithm Dr. Mohebbian mentioned)](https://arxiv.org/abs/2206.02777)\n",
        "\n",
        "- Relating back to previous PyTorch Lightning workshop\n",
        "\n",
        "  - [Using Optuna to optimize PyTorch lighting hyperparameters](https://www.pytorchlightning.ai/blog/using-optuna-to-optimize-pytorch-lightning-hyperparameters)\n",
        "\n",
        "  - [Unet segmentation with `rising` and `Pytorch Lightning` Google Colab](https://colab.research.google.com/github/PhoenixDL/rising/blob/master/notebooks/lightning_segmentation.ipynb#scrollTo=9v04VKjPz6-c)\n",
        "\n",
        "  - [Unet-PytochLightning: Kaggle challenge, attention Unet, very neat code!](https://www.kaggle.com/code/abhiswain/unet-pytorchlightning/notebook)\n",
        "    - [Used data from Medical Segmentation Decathlon: generalisable 3D Semantic Segmentation, huge paper and public dataset from multimodalities and multi-institutions](http://medicaldecathlon.com/)\n",
        "    - [paper](https://arxiv.org/pdf/2106.05735.pdf)\n",
        "\n",
        "- [Peak Signal Noise Ratio (PSNR) example: post-processing smoothing method to improve model performance](https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio)\n",
        "\n",
        "- [A tutorial on how you can load and resume training your model in PyTorch (if you want to resume since we stopped early)](https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61)\n",
        "\n",
        "- [Loss functions for image segmentation summary](https://github.com/JunMa11/SegLoss)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tflGy3oEvJs7"
      }
    }
  ]
}